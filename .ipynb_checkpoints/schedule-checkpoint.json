{"schedule": {"version": "1.3", "base_url": "https://pretalx.com/pyconde-pydata-berlin-2023/schedule/", "conference": {"acronym": "pyconde-pydata-berlin-2023", "title": "PyCon DE & PyData Berlin 2023", "start": "2023-04-17", "end": "2023-04-19", "daysCount": 3, "timeslot_duration": "00:05", "time_zone_name": "Europe/Berlin", "rooms": [{"name": "Kuppelsaal", "guid": null, "description": null, "capacity": 950}, {"name": "B09", "guid": null, "description": null, "capacity": 240}, {"name": "B07-B08", "guid": null, "description": null, "capacity": 200}, {"name": "B05-B06", "guid": null, "description": null, "capacity": 300}, {"name": "A1", "guid": null, "description": null, "capacity": 80}, {"name": "A03-A04", "guid": null, "description": null, "capacity": 140}, {"name": "A05-A06", "guid": null, "description": null, "capacity": 140}], "days": [{"index": 1, "date": "2023-04-17", "day_start": "2023-04-17T04:00:00+02:00", "day_end": "2023-04-18T03:59:00+02:00", "rooms": {"Kuppelsaal": [{"id": 26497, "guid": "df93a270-a127-5dfe-8360-3c166e9f94f1", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26497-pandas-2-0-and-beyond", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/DB3KC7/", "title": "Pandas 2.0 and beyond", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk (long)", "language": "en", "abstract": "Pandas has reached a 2.0 milestone in 2023. But what does that mean? And what is coming after 2.0? This talk will give an overview of what happened in the latest releases of pandas and highlight some topics and major new features the pandas project is working on.", "description": "The pandas 2.0 release is targeted for the first quarter of 2023. This is a major milestone for the pandas project, and this talk will start  with an overview of this release. Pandas 2.0 includes some new (experimental) features, but mostly means enforcing deprecations that have been accumulated in the 1.x series, along with some necessary breaking changes.\r\n\r\nBut that doesn\u2019t mean there are no interesting features to talk about! The main part of the presentation will showcase some new features, both already released as opt-in features or to come in future releases. \r\nSupport for non-nanosecond resolution datetimes, allowing time spans ranging over a billion of years. Improved support for nullable data types, including easy opt-in options for I/O functions. Experimental integration with pyarrow to back columns of a DataFrame (beyond the string dtype). \r\nA major change that is under way is a change to the copy and view semantics of operations in pandas (related to the well-known (or hated) SettingWithCopyWarning). This is already available as an experimental opt-in to test and use the new behaviour, and will probably be a highlight of pandas 3.0.", "recording_license": "", "do_not_record": false, "persons": [{"id": 75, "code": "7VUXWM", "public_name": "Joris Van den Bossche", "biography": "I am a core contributor to Pandas and Apache Arrow, and maintainer of GeoPandas. I did a PhD at Ghent University and VITO in air quality research and worked at the Paris-Saclay Center for Data Science. Currently, I work at Voltron Data, contributing to Apache Arrow, and am a freelance teacher of python (pandas) at Ghent University.", "answers": []}, {"id": 25677, "code": "DDZLYH", "public_name": "Patrick Hoefler", "biography": "I am a member of the pandas core team since early 2021. I am a regular contributor of pandas since early 2020. I am currently working at Coiled as a Senior Software Engineer. I hold a Masters degree in Mathematics and I am currently studying towards a Software Engineering degree.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25390, "guid": "31447685-7759-5755-a9cd-184da879c325", "logo": "", "date": "2023-04-17T11:40:00+02:00", "start": "11:40", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-25390-an-unbiased-evaluation-of-environment-management-and-packaging-tools", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/VBP3PE/", "title": "An unbiased evaluation of environment management and packaging tools", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk (long)", "language": "en", "abstract": "Python packaging is quickly evolving and new tools pop up on a regular basis. Lots of talks and posts on packaging exist but none of them give a structured, unbiased overview of the available tools.\r\n\r\nThis talk will shed light on the jungle of packaging and environment management tools, comparing them on a basis of predefined features.", "description": "Python packaging is quickly evolving and new tools pop up on a regular basis. Lots of talks and posts on packaging exist but none of them give a structured, unbiased overview of the available tools.\r\n\r\nThis talk will shed light on the jungle of packaging and environment management tools, comparing them on a basis of predefined features.\r\n\r\nWe will categorize tools using the following categories:\r\n- Python version management\r\n- Environment management\r\n- Package management\r\n- Package building\r\n- Package publishing\r\n\r\nA lot of tools exist, including pyenv, pip, venv, poetry, hatch, and many more. We will categorize all of them and discuss some in more detail, e.g. hatch. Most importantly, we will evaluate the tools on the basis of features that are important for developers like:\r\n- Does the tool manage dependencies?\r\n- Can it manage Python installations?\r\n- Does it have a clean build/publish flow?\r\n- Does it allow for plugins?\r\n- Does it support important PEPs, e.g. PEP 660, PEP 621, PEP 582?\r\n\r\n## Audience\r\nThis talk is intended for developers who\r\n- Have used packaging and want to get to know new tools\r\n- Want to have an overview of existing tools and their capabilities\r\n\r\n## Existing talks on the topic of packaging\r\n- PyCon US 2021 Jeremy Paige / Packaging Python in 2021\r\n- PyCon US 2021 TUTORIAL / Bern\u00e1t Gabor / Python Packaging Demystified\r\n- EuroPython 2022 Packaging in Python in 2022", "recording_license": "", "do_not_record": false, "persons": [{"id": 24562, "code": "9CX9CB", "public_name": "Anna-Lena Popkes", "biography": "I'm Anna-Lena, a machine learning engineer living in Bonn, Germany. I'm very passionate about learning and love to share my knowledge with other people. Besides machine learning I love teaching Python and have been a regular guest on PyCon events and podcasts.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28265, "guid": "0f226c67-f8a6-5c6f-86a8-472a794e0e09", "logo": "", "date": "2023-04-17T13:55:00+02:00", "start": "13:55", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-28265-keynote-a-journey-through-4-industries-with-python-python-s-versatile-problem-solving-toolkit", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/NMMT8M/", "title": "Keynote - A journey through 4 industries with Python: Python's versatile problem-solving toolkit", "subtitle": "", "track": "Plenary", "type": "Keynote", "language": "en", "abstract": "In this keynote, I will share the lessons learned from using Python in 4 industries. Apart from machine learning applications that I build in my day to day as a data scientist and machine learning engineer, I also use Python to develop games for my own gaming company, Quill Game Studios. There is a lot of versatility in Python, and it's been my pleasure to use it to solve many interesting problems. I hope that this talk can give inspiration to various types of applications in your own industry as well.", "description": "In this keynote, I will share the lessons learned from using Python in 4 industries. Apart from machine learning applications that I build in my day to day as a data scientist and machine learning engineer, I also use Python to develop games for my own gaming company, Quill Game Studios. There is a lot of versatility in Python, and it's been my pleasure to use it to solve many interesting problems. I hope that this talk can give inspiration to various types of applications in your own industry as well.", "recording_license": "", "do_not_record": false, "persons": [{"id": 27401, "code": "DCY37X", "public_name": "Susan Shu Chang", "biography": "Susan Shu Chang is the founder, Quill Game Studios and Principal data scientist at Elastic. Previously, she built machine learning at scale in the fintech, social, and telecom industries. At Quill Game Studios, she grew the company to 10+ developers and shipped 2 commercial releases on PC and consoles. The studio is currently developing the game \u201cAutumn with the Shiba Inu\u201d. She\u2019s a five-time speaker at PyCons around the world and keynote speaker at the O\u2019Reilly MLOps Superstream. You can find her machine learning career guides at Susanshu.com.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26247, "guid": "bc2deaec-64f4-53f8-a25c-5dc33ddba69a", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26247-raised-by-pandas-striving-for-more-an-opinionated-introduction-to-polars", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/Z8PESY/", "title": "Raised by Pandas, striving for more: An opinionated introduction to Polars", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "Pandas is the de-facto standard for data manipulation in python, which I personally love for its flexible syntax and interoperability. But Pandas has well-known drawbacks such as memory in-efficiency, inconsistent missing data handling and lacking multicore-support. Multiple open-source projects aim to solve those issues, the most interesting is Polars.\r\n\r\nPolars uses Rust and Apache Arrow to win in all kinds of performance-benchmarks and evolves fast. But is it already stable enough to migrate an existing Pandas' codebase? And does it meet the high-expectations on query language flexibility of long-time Pandas-lovers?\r\n\r\nIn this talk, I will explain, how Polars can be that fast, and present my insights on where Polars shines and in which scenarios I stay with pandas (at least for now!)", "description": "Pandas and Polars are both popular open-source libraries for data manipulation and analysis in Python. While both libraries offer a range of powerful tools for working with data, there are several key differences that users should be aware of when choosing which library to use.\r\n\r\nOne of the main differences between Pandas and Polars is the way that they handle data processing and evaluation. Pandas uses a traditional, eager evaluation model, in which operations are immediately evaluated and the results are returned. In contrast, Polars offers optional lazy evaluation, which allows users to delay the evaluation of certain operations until they are actually needed. This can be especially useful for large or complex datasets, as it can improve performance by reducing the amount of data that needs to be processed at any given time.\r\n\r\nAnother key difference between the two libraries is the way they handle data storage and indexing. Pandas is built around a powerful indexing system that allows users to quickly access and manipulate specific rows or columns of data. However, this indexing system can be complex and can sometimes lead to slower performance. In contrast, Polars does not use indexes, which can simplify the underlying data structure and improve performance.\r\n\r\nIn terms of functionality, Pandas has a number of features that are not currently available in Polars. For example, Pandas offers built-in plotting functionality, which can be useful during explorative data analysis for visualizing and interpreting data. Additionally, Pandas has a much stronger integration in the PyData ecosystem and is more widely used in data analysis and scientific computing. This can make it easier for users to find resources and support when working with Pandas.\r\n\r\nOne notable difference between the two libraries is the syntax and API. Polars is inspired by the popular distributed computing library Apache Spark, but uses a column-based API in contrast to the row-based API within Spark. Generally the polar's syntax will be more familiar to spark users.\r\n\r\nOverall, both Pandas and Polars are powerful libraries with a lot to offer for data manipulation and analysis in Python. Which library is the best choice will depend on the specific needs and goals of the user. By understanding the differences between the two libraries, users can make an informed decision about which one is best suited for their needs.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25797, "code": "HWZVSQ", "public_name": "Nico Kreiling", "biography": "Nico is a Data Scientist at scieneers, co-organizer of PyData cologne meetup and host of the Techtiefen podcast. His passions are quick and simple solutions and the constant expansion of his and the communities' knowledge base.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25254, "guid": "81edd27a-4037-5a6a-9ddf-3f374f1e5552", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-25254-polars-make-the-switch-to-lightning-fast-dataframes", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/S79HEH/", "title": "Polars - make the switch to lightning-fast dataframes", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "In this talk, we will report on our experiences switching from Pandas to Polars in a real-world ML project. Polars is a new high-performance dataframe library for Python based on Apache Arrow and written in Rust. We will compare the performance of polars with the popular pandas library, and show how polars can provide significant speed improvements for data manipulation and analysis tasks. We will also discuss the unique features of polars, such as its ability to handle large datasets that do not fit into memory, and how it feels in practice to make the switch from Pandas. This talk is aimed at data scientists, analysts, and anyone interested in fast and efficient data processing in Python.", "description": "The pandas library is one of the most widely used tools for working with data in the Python ecosystem. However, pandas can be slow for medium and larger datasets, and many users have been looking for faster alternatives. In this talk, we introduce the new polars library, a high-performance dataframe library for Python based on Apache Arrow and written in Rust. We will report on our experiences switching from Pandas to Polars in a real-world ML project.\r\n\r\nWe will compare the performance of polars with pandas using various use-cases, and show how polars can provide significant speed improvements for common data manipulation and analysis tasks. Due to its speed it can even be an alternative for cases where people normally use distributed systems like Spark. For example, we will demonstrate how polars can process large datasets with minimal overhead, and how its massive use of parallelization can provide an additional speed boost.\r\n\r\nWe will also discuss how polars compares to other popular options like DuckDB and cuDF.\r\n\r\nThis talk is aimed at data scientists, analysts, and anyone interested in fast and efficient data processing in Python. Whether you are a pandas user looking for a faster alternative, or a Spark user interested in a simpler alternative, this talk will provide valuable insights and practical examples.", "recording_license": "", "do_not_record": false, "persons": [{"id": 24992, "code": "3GQ8XJ", "public_name": "Thomas Bierhance", "biography": "Thomas passion has been working with data since 25 years: from small databases for SMEs to large distributed systems for international enterprises and intelligent systems using machine learning. He graduated from the KIT in Karlsruhe, Germany and trained his first neural network while studying at UPC, Barcelona, Spain in 2002. Today he leads the Data Science & AI practice of BettercallPaul in Stuttgart and supports his customers and teams on their journey to generate added value from data.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26096, "guid": "18e26a7c-7801-5249-8538-63a90cea68a1", "logo": "", "date": "2023-04-17T16:20:00+02:00", "start": "16:20", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26096-fastapi-and-celery-building-reliable-web-applications-with-tdd", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/8CVQDW/", "title": "FastAPI and Celery: Building Reliable Web Applications with TDD", "subtitle": "", "track": "PyCon: Testing", "type": "Talk", "language": "en", "abstract": "In this talk, we will explore how to use the FastAPI web framework and Celery task queue to build reliable and scalable web applications in a test-driven manner. We will start by setting up a testing environment and writing unit tests for the core functionality of our application. Next, we will use FastAPI to create an api to perform some long-running task. Finally, we will then see how Celery can help us offload long-running tasks and improve the performance of our application.\r\nBy the end of this talk, attendees will have a strong understanding of TDD and how to apply it to your FastAPI and Celery projects, and you will be able to write tests that ensure the reliability and maintainability of your code.", "description": "1. Introduction (1 min)\r\n- Title of the talk and speaker's name: This section introduces the title of the talk and the speaker's name, and current role of the speaker.\r\n- Overview of the topics covered in the talk: This section introduces the main themes and goals of the talk, and gives the audience a sense of what they can expect to learn.\r\n2. What is Test-Driven Development (TDD)? (2 min)\r\n- Definition of TDD and how it fits into the software development process: This section defines TDD and explains how it fits into the software development process. It will highlight the benefits of TDD such as improved quality, reduced debugging time, and faster development.\r\n3. Setting up a dockerized Development Environment for a Math API (5 min)\r\n- Installing the necessary tools and libraries with Docker: This section covers the steps to install the necessary tools and libraries for testing, such as FastAPI, Celery, and a testing framework. \r\n- Setting up a testing database with Docker: This subsection explains how to set up a testing database (PostgreSQL) using Docker. It can include instructions for pulling the Docker image, running the container, and configuring the connection.\r\n- Configuring the application to use the testing database: This sub-section covers the steps to configure the application to use the testing database during testing. It can include instructions for setting up environment variables or config files to switch between different databases.\r\n- Writing a basic test case: This subsection provides an example of a basic test case that verifies the setup of the testing environment. It will include a demonstration for running the test and checking the results.\r\n4. Writing Unit Tests (7 min)\r\n- Identifying the core functionality and behavior of the application: This section discusses how to identify the core functionality and behavior of the application, and how to break it down into smaller pieces that can be tested separately. It can include tips on how to prioritize the tests and focus on the most important or risky areas of the code.\r\n- Writing test cases to cover the different scenarios and edge cases: This sub-section covers the steps to write test cases that cover the different scenarios and edge cases for the core functionality of the application. It can include examples of different types of tests, such as positive, negative, and boundary tests.\r\n- Using mocks and fixtures to isolate the tests: This subsection explains how to use mocks and fixtures to isolate the tests from external dependencies and control the input and output. It can include examples of how to use these techniques to test different parts of the application in isolation.\r\n5. Building the API with FastAPI and Celery (8 min)\r\n- Setting up a FastAPI application: This section introduces FastAPI, and explains its key features and benefits. It will include a demonstration of how to use FastAPI to build a simple API using TDD.\r\n- Setting up a Celery worker and task queue: This subsection explains how to set up a Celery worker and task queue, and how to configure the application to use them. It can include instructions on how to install Celery, create a Celery instance, and define the queue and backend.\r\n- Defining tasks as functions and decorating them with Celery's @task decorator: This sub-section covers the steps to define tasks as functions and decorate them with Celery's @task decorator. It can include examples of how to define tasks and pass arguments and options to them.\r\n- Using the Celery client to trigger tasks and receive the results: This subsection explains how to use the Celery client to trigger tasks and receive the results. It can include instructions on how to send tasks and wait for the results, and how to handle errors and exceptions.\r\n6. Conclusion and Next Steps (2 min)\r\n- Recap of the main points and takeaways from the talk: This section provides a brief summary of the main points and takeaways from the talk, and highlights the key skills and knowledge that the attendees have learned.\r\n- Suggestions for further learning and resources: This subsection provides suggestions for further learning and resources for the attendees to dive deeper into TDD and FastAPI/Celery development. It can include links to tutorials, documentation, and other resources that can help the attendees continue learning and practicing what they have learned in the talk.\r\n- Encouragement for attendees to apply these techniques to their own projects: This subsection encourages the attendees to apply the techniques and skills they have learned in the talk to their own projects, and to share their experiences and feedback with the community.\r\n7. Question/Answer (5 min)", "recording_license": "", "do_not_record": false, "persons": [{"id": 25684, "code": "YC7CYR", "public_name": "Avanindra Kumar Pandeya", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}], "B09": [{"id": 26239, "guid": "c0eb5c06-f14f-55ef-85a2-660f9e2a7353", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "00:45", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26239-apache-streampipes-for-pythonistas-iiot-data-handling-made-easy-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/LXBGZS/", "title": "Apache StreamPipes for Pythonistas: IIoT data handling made easy!", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk (long)", "language": "en", "abstract": "The industrial environment offers a lot of interesting use cases for data enthusiasts. There are myriads of interesting challenges that can be solved by data scientists. \r\nHowever, collecting industrial data in general and industrial IoT (IIoT) data in particular, is cumbersome and not really appealing for anyone who just wants to work with data.\r\nApache StreamPipes addresses this pitfall and allows anyone to extract data from IIoT data sources without messing around with (old-fashioned) protocols. In addition, StreamPipes newly developed Python client now gives Pythonistas the ability to programmatically access and work with them in a Pythonic way.\r\n\r\nThis talk will provide a basic introduction into the functionality of Apache StreamPipes itself, followed by a deeper discussion of the Python client. Finally, a live demo will show how IIoT data can be easily derived in Python and used directly for visualization and ML model training.", "description": "The industrial environment is becoming an increasingly attractive use case for data enthusiasts with challenges ranging from predictive maintenance to robotics to autonomous vehicles.\r\nBuilding a full-fledged IIoT architecture is a big endeavor, especially for small and medium sized companies with limited resources. It requires IIoT specialists with extensive knowledge of industrial protocols, software architects capable of designing an IIoT platform, and cloud specialists able to operate an infrastructure at scale that can handle potentially massive data streams. However, the added value lies not in the technical infrastructure, but in the data itself. Therefore, it should be as easy as possible for data scientists to analyze data to gain new insights without worrying about underlying technical details. But such a project has many pitfalls, which is why many projects are not even initiated because the costs seem too high. These pitfalls are addressed by Apache StreamPipes, an end-to-end toolbox that allows anyone to easily extract, explore and analyze IIoT data. With its new Python client, it targets Python data enthusiasts (e.g., data scientists) who want to work with IIoT data but don't want to get their hands dirty interacting with industrial systems.\r\nVia an easy-to-use python client, it is possible for developers to get streaming or historic data from StreamPipes internal data management layer in a pythonic representation like dictionaries or pandas dataframes. This allows data scientists to work with their familiar tech stack and use the extracted data directly for analytics, visualizations, or even machine learning. StreamPipes handles all the infrastructure such as the message broker or time-series storage and provides many out-of-the-box features that ease data analytics of industrial sources: More than 20 data adapters for quickly getting access to a variety of industrial protocols, built-in pre-processing rules to harmonize sensor and other data on the fly and a pipeline editor featuring over 100 algorithms and a rich user interface to interactively build data processing pipelines.\r\n\r\nApache StreamPipes is a large and mature open source project which started as a research project in 2015 and made its way to an Apache top-level project in November 2022 with a community of currently more than 25 active contributors.\r\n \r\nThe talk will provide a basic introduction to Apache StreamPipes, followed by a deeper discussion of the Python client focusing on the target audience (Python developers). The main part is about data handling with python, and design decisions within the client for common patterns will be discussed in detail.\r\n \r\nAs a conclusion we will show how IIoT data can be extracted via Apache StreamPipes and used for further analytics within the Python world. Attendees will get familiar with Apache StreamPipes in general, its mission, and its core modules. In addition, common IIoT patterns will be presented and illustrated using the Python client of Apache StreamPipes. The presentation includes an extensive demo with many hands-on examples.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25788, "code": "R7NUKX", "public_name": "Tim Bossenmaier", "biography": "Tim Bossenmaier works as a Data Engineer at inovex. There he develops and builds modern data infrastructures in customer projects, from streaming ETL pipelines to data catalogs. He is also a developer and member of the project management committee of Apache StreamPipes, an open source solution for IoT data analysis.", "answers": []}, {"id": 26231, "code": "PPHET7", "public_name": "Sven Oehler", "biography": "I study Applied Artificial Intelligence at the Offenburg University of Applied Sciences and I am very interested in Data Science and AI. During my internship at the startup Bytefabrik.AI in Karlsruhe, I came in touch with the Apache StreamPipes software and became a committer for this project. I work on the python integration to enable easy access to live data streams that can be quickly connected by StreamPipes.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26159, "guid": "6dec904a-926c-56d7-a500-dfad984a925e", "logo": "", "date": "2023-04-17T11:40:00+02:00", "start": "11:40", "duration": "00:45", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26159-hyperparameter-optimization-for-the-impatient", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/DECAHT/", "title": "Hyperparameter optimization for the impatient", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk (long)", "language": "en", "abstract": "In the last years, Hyperparameter Optimization (HPO) became a fundamental step in the training\r\nof Machine Learning (ML) models and in the creation of automatic ML pipelines.\r\nUnfortunately, while HPO improves the predictive performance of the final model, it comes with a significant cost both in terms of computational resources and waiting time.\r\nThis leads many practitioners to try to lower the cost of HPO by employing unreliable heuristics.\r\n\r\nIn this talk we will provide simple and practical algorithms for users that want to train models\r\nwith almost-optimal predictive performance, while incurring in a significantly lower cost and waiting\r\ntime. The presented algorithms are agnostic to the application and the model being trained so they can be useful in a wide range of scenarios.\r\n\r\nWe provide results from an extensive experimental activity on public benchmarks, including comparisons with well-known techniques like Bayesian Optimization (BO), ASHA, Successive Halving.\r\nWe will describe in which scenarios the biggest gains are observed (up to 30x) and provide examples for how to use these algorithms in a real-world environment.\r\n\r\nAll the code used for this talk is available on (GitHub)[https://github.com/awslabs/syne-tune].", "description": "In this talk we will present simple and practical solutions to perform HPO quickly with results on-par with well-know (and costly) techniques. Our claims are supported by empirical evidence obtained on public standardized benchmarks and our work has been accepted in peer-reviewed workshop (currently under submission to a conference).\r\n\r\nSpecifically, [1] has been accepted at the AutoML Conference Workshop Track and [2] has been accepted at the AutoML workshop at ICML 2021.\r\nAll the code regarding the algorithms is available in the Syne-Tune package under license Apache 2.0 (https://github.com/awslabs/syne-tune).\r\n\r\nReferences:\r\n[1] https://arxiv.org/abs/2207.06940\r\n[2] https://arxiv.org/abs/2103.16111", "recording_license": "", "do_not_record": false, "persons": [{"id": 25795, "code": "N7FUTG", "public_name": "Martin Wistuba", "biography": "Martin Wistuba is a researcher at Amazon Web Services where he works on automation of hyperparameter optimization and Neural Architecture Search. Earlier, he was at  IBM Research, where he developed tools to automate deep learning.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 27691, "guid": "94f56d53-33ad-58cd-8d0d-6e45bff510ba", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-27691-exploring-the-power-of-cyclic-boosting-a-pure-python-explainable-and-efficient-ml-method", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MYARJG/", "title": "Exploring the Power of Cyclic Boosting: A Pure-Python, Explainable, and Efficient ML Method", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "We have recently open-sourced a pure-Python implementation of Cyclic Boosting, a family of general-purpose, supervised machine learning algorithms. Its predictions are fully explainable on individual sample level, and yet Cyclic Boosting can deliver highly accurate and robust models. For this, it requires little hyperparameter tuning and minimal data pre-processing (including support for missing information and categorical variables of high cardinality), making it an ideal off-the-shelf method for structured, heterogeneous data sets. Furthermore, it is computationally inexpensive and fast, allowing for rapid improvement iterations. The modeling process, especially the infamous but unavoidable feature engineering, is facilitated by automatic creation of an extensive set of visualizations for data dependencies and training results. In this presentation, we will provide an overview of the inner workings of Cyclic Boosting, along with a few sample use cases, and demonstrate the usage of the new Python library.\r\n\r\n \r\n\r\nYou can find Cyclic Boosting on GitHub: https://github.com/Blue-Yonder-OSS/cyclic-boosting", "description": "We have recently open-sourced a pure-Python implementation of Cyclic Boosting, a family of general-purpose, supervised machine learning algorithms. Its predictions are fully explainable on individual sample level, and yet Cyclic Boosting can deliver highly accurate and robust models. For this, it requires little hyperparameter tuning and minimal data pre-processing (including support for missing information and categorical variables of high cardinality), making it an ideal off-the-shelf method for structured, heterogeneous data sets. Furthermore, it is computationally inexpensive and fast, allowing for rapid improvement iterations. The modeling process, especially the infamous but unavoidable feature engineering, is facilitated by automatic creation of an extensive set of visualizations for data dependencies and training results. In this presentation, we will provide an overview of the inner workings of Cyclic Boosting, along with a few sample use cases, and demonstrate the usage of the new Python library.\r\n\r\n \r\n\r\nYou can find Cyclic Boosting on GitHub: https://github.com/Blue-Yonder-OSS/cyclic-boosting", "recording_license": "", "do_not_record": false, "persons": [{"id": 27381, "code": "XDS3M7", "public_name": "Felix Wick", "biography": "I received my PhD in high energy physics at the Karlsruhe Institute of Technology in 2011. Then I joined Blue Yonder, a provider of cloud-based predictive applications for the retail market, where I led the Machine Learning core team and developed, among other things, new methods for demand forecasting and shaping. In 2018, Blue Yonder got acquired by JDA (subsequently re-branded to Blue Yonder), a leading provider of supply chain management software, and in 2021, the merged company got, in turn, acquired by Panasonic. As Corporate Vice President and Blue Yonder fellow, I am now advising on overall Data Science strategies and driving research toward an autonomous supply chain powered by Artificial Intelligence. Moreover, I am lecturer for Machine Learning at the Karlsruhe Institute of Technology.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28570, "guid": "4c2b0022-f172-5360-b80f-2c2db29bb1dd", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-28570-a-concrete-guide-to-time-series-databases-with-python", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/8WXSR9/", "title": "A concrete guide to time-series databases with Python", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "We evaluated time-series databases and complementary services to stream-process sensor data. In this talk, our evaluation will be presented. The final implementation will be shown, alongside python-tools we\u2019ve built and lessons learned during the process.", "description": "Understanding time-series data is essential to handle automatically generated data, be it from server logs, IoT devices or any other continuous measurement.\r\n\r\nIn order to handle the large amounts of incoming data from concrete mixing trucks, we evaluated a number of time-series databases as well as services to stream-process the data. For all of those decisions a key question was, of course, how well any of these tools integrate with our existing, all-Python backend.\r\n\r\nThe right angle on time-series data will help you move tons of data with little engineering effort. In this talk, you\u2019ll learn from our practical experiences of choosing and implementing a time-series database in a Python context. You\u2019ll go away with a better understanding of how you can efficiently store, analyse and exploit streaming data.", "recording_license": "", "do_not_record": false, "persons": [{"id": 27690, "code": "JYEUEG", "public_name": "Ellen K\u00f6nig", "biography": null, "answers": []}, {"id": 25847, "code": "KDMRKX", "public_name": "Heiner Tholen", "biography": "Heiner leads the truck-IoT effort at alcemy GmbH, where he's responsible for hard- and software. He holds a PhD in Physics and has a knack for building things that open a new dimension for their users.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28572, "guid": "009d2090-2afe-5cd3-8102-a4f7a4ab2d3c", "logo": "", "date": "2023-04-17T16:20:00+02:00", "start": "16:20", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-28572-how-to-build-observability-into-a-ml-platform", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/TGZFSF/", "title": "How to build observability into a ML Platform", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "As machine learning becomes more prevalent across nearly every business and industry, making sure that these technologies are working and delivering quality is critical. In her talk, Alicia will discuss the importance of machine learning observability and why it should be a fundamental tool of modern machine learning architectures. Not only does it ensure models are accurate, but it helps teams iterate and improve models quicker. Alicia will dive into how Shopify has been prototyping building observability into different parts of its machine learning platform. This talk will provide insights on how to track model performance, how to catch any unexpected or erroneous behaviour, what types of behavior to look for in your data (e.g. drift, quality metrics) and in your model/predictions, and how observability could work with large language models and Chat AIs.", "description": "As machine learning becomes more prevalent across nearly every business and industry, making sure that these technologies are working and delivering quality is critical. In her talk, Alicia will discuss the importance of machine learning observability and why it should be a fundamental tool of modern machine learning architectures. Not only does it ensure models are accurate, but it helps teams iterate and improve models quicker. Alicia will dive into how Shopify has been prototyping building observability into different parts of its machine learning platform. This talk will provide insights on how to track model performance, how to catch any unexpected or erroneous behaviour, what types of behavior to look for in your data (e.g. drift, quality metrics) and in your model/predictions, and how observability could work with large language models and Chat AIs.", "recording_license": "", "do_not_record": false, "persons": [{"id": 28945, "code": "JURAZQ", "public_name": "Alicia Bargar", "biography": "Alicia is a coding polyglot with over five years of professional engineering experience applied to R&D platform development and data engineering. As a Senior Data Developer at Shopify, Alicia works on the Machine Learning Platform team, working on designing and implementing data quality monitoring.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B07-B08": [{"id": 28575, "guid": "1b2f69bf-c531-59c7-8aef-710dee5c9af4", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-28575-cooking-up-a-ml-platform-growing-pains-and-lessons-learned", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/KMGYZF/", "title": "Cooking up a ML Platform: Growing pains and lessons learned", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk (long)", "language": "en", "abstract": "What is a ML platform and do you even need one? When should you consider investing in your own ML platform? What challenges can you expect building and maintaining one? Tune in and discover (some) answers to these questions and more! I will share a first-hand account of our ongoing journey towards becoming a ML platform team within Delivery Hero's Logistics department, including how we got here, how we structure our work, and what challenges and tools we are focussing on next.", "description": "What is an ML platform and do you even need one? When should you consider investing in your own ML platform? What challenges can you expect building and maintaining one? Tune in and discover (some) answers to these questions and more! I will share a first-hand account of our ongoing journey towards becoming an ML platform team within Delivery Hero's Logistics department, including how we got here, how we structure our work, and what challenges and tools we are focusing on next.", "recording_license": "", "do_not_record": false, "persons": [{"id": 29030, "code": "E8FWQS", "public_name": "Cole Bailey", "biography": "I am leading a scrappy but growing team of ML engineers at Delivery Hero who aim to bridge the gap between software engineering, DevOps, data engineering, and data science. I hope to make data science easier without restricting the creativity and flexibility that data scientists need to make an impact in their role.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28666, "guid": "1954d766-21b2-5c1a-8091-3ec05a21b6df", "logo": "", "date": "2023-04-17T11:40:00+02:00", "start": "11:40", "duration": "00:45", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-28666-large-scale-feature-engineering-and-datascience-with-python-snowflake", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/3TYND7/", "title": "Large Scale Feature Engineering and Datascience with Python & Snowflake", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk (long)", "language": "en", "abstract": "[Snowflake](https://www.snowflake.com/en/) as a data platform is the core data repository of many large organizations.  \r\nWith the introduction of Snowflake's [Snowpark for Python](https://github.com/snowflakedb/snowpark-python), Python developers can now collaborate and build on one platform with a secure Python sandbox, providing developers with dynamic scalability & elasticity as well as security and compliance.  \r\n\r\nIn this talk I'll explain the core concepts of <b>Snowpark for Python</b> and how they can be used for <b>large scale feature engineering</b> and <b>data science</b>.", "description": "This talk is for technical people that would like to get a deep dive into how Snowflake enables large scale feature engineering and data science via <b>Snowpark for Python</b>.  \r\nDuring this talk we'll explore Snowflake's Python capabilities using a simple machine learning use case.\r\n\r\nAfter this talk you will:\r\n\r\n* know how Snowpark avoids data movement and keeps existing security & governance intact,\r\n* understand the concept of the Snowpark DataFrame-API and how it enables accelerated performance compared to standard Pandas DataFrames,\r\n* know how to distribute Hyper Parameter Tuning and training of multiple models,\r\n* understand the concept of Vectorized User-Defined-Functions and how they can be used to perform large scale model inference.", "recording_license": "", "do_not_record": false, "persons": [{"id": 27787, "code": "D7YJ88", "public_name": "Michael Gorkow", "biography": "Michael is Field CTO for Datascience at Snowflake where he helps organisations to implement state of the art machine learning solutions. As a data science professional, he is passionate about sharing with others how to go beyond standard use cases and implement machine learning techniques for big data.\u00a0\r\nHe is based out of Munich, Germany.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25879, "guid": "27a2e08c-1671-500a-8881-a6eefbd73c81", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-25879-the-cpu-in-your-browser-webassembly-demystified", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/E77G9H/", "title": "The CPU in your browser: WebAssembly demystified", "subtitle": "", "track": "General: Python & PyData Friends", "type": "Talk", "language": "en", "abstract": "In the recent years we saw an explosion of usage of Python in the browser:\r\nPyodide, CPython on WASM, PyScript, etc. All of this is possible thanks to the\r\npowerful functionalities of the underlying platform, WebAssembly, which is essentially a virtual CPU\r\ninside the browser.", "description": "In the recent years we saw an explosion of usage of Python in the browser:\r\nPyodide, CPython on WASM, PyScript, etc. All of this is possible thanks to the\r\npowerful functionalities of the underlying platform, WebAssembly.\r\n\r\nIn this talk we will examine what is exactly WebAssembly, what are the strong\r\nand weak points, what are the limitations and what the future will bring us.\r\nWe will also see why and how WebAssembly is useful and used outside the\r\nbrowser.\r\n\r\nThis talk is targeted to an intermediate/advanced audience: no prior knowledge of\r\nWebAssembly is required, but it is required to have a basic understanding of what is a compiler, an interpreter and the concept of bytecode.\r\n\r\nThe introduction will cover the basics to make sure that the talk is\r\nunderstandable also by people who are completely new to the WebAssembly world,\r\nbut after that we will dive into the low-level technical details, with a\r\nspecial focus on those who are relevant to the Python world, such WASI vs\r\nemscripten, dynamic linking, JIT compilation, interoperability with other\r\nlanguages, etc.", "recording_license": "", "do_not_record": false, "persons": [{"id": 231, "code": "QMAPYZ", "public_name": "Antonio Cuni", "biography": "Dr. Antonio Cuni is a Principal Software Engineer at Anaconda. He is a core\r\ndeveloper of PyScript and PyPy, and one of the founders of the HPy project,\r\nwhich aims to design a better and more modern C API for Python.  He loves to\r\nwrite tools from developers for developers, such as Pdb++, fancycompleter and\r\nvmprof and he is creator/maintainer/contributor of numerous other open source\r\nprojects.  He have also been very active in the Python community for years,\r\ngiving talks at various conferences such as EuroPython, EuroSciPy, PyCon\r\nItalia, and many others. He regularly writes on the PyPy blog and on the HPy\r\nblog.  His main areas of interest are compilers, language implementation, TDD\r\nand performance.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28571, "guid": "632c2020-1b81-545a-b730-9dacc5f90078", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-28571-have-your-cake-and-eat-it-too-rapid-model-development-and-stable-high-performance-deployments", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/ABNXHC/", "title": "Have your cake and eat it too: Rapid model development and stable, high-performance deployments", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "At the boundary of model development and MLOps lies the balance between the speed of deploying new models and ensuring operational constraints. These include factors like low latency prediction, the absence of vulnerabilities in dependencies and the need for the model behavior to stay reproducible for years. The longer the list of constraints, the longer it usually takes to take a model from its development environment into production. In this talk, we present how we seemingly managed to square the circle and have both a rapid, highly dynamic model development and yet also a stable and high-performance deployment.", "description": "At QuantCo, we ship sklearn-based models in a real-time service that guarantees 24/7 uptime with low latency (ms) responses. Simultaneously, we adhere to strict regulatory and security policies, where every model must remain available for 3-5 years, while its dependencies are kept up-to-date. As the basis, we are using ONNX as a technology to transform our dynamic Python pipelines into static, low-overhead model definitions. To ensure the cost of the model transformation does not slow down our Data Scientists, we have developed an open-source library named Spox, to streamline these operations as much as possible. Combined with an apt model serving infrastructure, we can satisfy the needs of our data scientists (fast development and deployment) and those of corporate IT (vulnerability-free, year-long stability) without compromising efficiency.", "recording_license": "", "do_not_record": false, "persons": [{"id": 27864, "code": "KFKWUY", "public_name": "Christian Bourjau", "biography": "With a PhD in experimental particle physics, Christian has a passion for the intersection of\r\ncutting-edge data science and modern software engineering. His work at QuantCo is centered\r\naround creating efficient tools for data scientists with a clean and maintainable path toward\r\nproduction. That pursuit has led him deep into ONNX and its related ecosystem over the last\r\nyear.", "answers": []}, {"id": 27865, "code": "SCMAXY", "public_name": "Jakub Bachurski", "biography": "Currently studying Computer Science at the University of Cambridge, Jakub\u2019s interests are primarily in algorithm design and programming languages. He put those interests to use designing the Spox framework for ONNX as a Software Engineer at QuantCo.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26364, "guid": "2aeb93f3-1e5b-57c3-a6af-1faade8fb4a8", "logo": "", "date": "2023-04-17T16:20:00+02:00", "start": "16:20", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26364-building-a-personal-assistant-with-gpt-and-haystack-how-to-feed-facts-to-large-language-models-and-reduce-hallucination-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/H8KMTT/", "title": "Building a Personal Assistant With GPT and Haystack: How to Feed Facts to Large Language Models and Reduce Hallucination.", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Large Language Models (LLM), like ChatGPT, have shown miraculous performances on various tasks. But there are still unsolved issues with these models: they can be confidently wrong and their knowledge becomes outdated. GPT also does not have any of the information that you have stored in your own data.  In this talk, you'll learn how to use Haystack, an open source framework, to chain LLMs with other models and components to overcome these issues. We will build a practical application using these techniques. And you will walk away with a deeper understanding of how to use LLMs to build NLP products that work.", "description": "You can apply LLMs to solve various NLP and NLU tasks, such as summarization or question answering. These models have billions of parameters they can use to effectively store some of the information they saw during pre-training. This enables them to show deep knowledge of a subject, even if they weren't explicitly trained on it.\r\n\r\nYet, this capability also comes with issues. The information stored in the parameters can\u2019t easily be updated, and the model's knowledge might become stale. The model won\u2019t have any of your custom data, your company\u2019s knowledge base for example. Sometimes, the model makes things up. We call that hallucination.\r\n\r\nCases of hallucination can be hard to spot. The model may be very confident while making up a response. It may even make up fake citations and research papers to support its claims.\r\n\r\nHaystack is an open source NLP framework for pragmatic builders. Developers use it to build NLP applications, such as question answering systems, neural search engines, or summarization services. Haystack provides all the components you need to build an actual NLP application, which differentiates it from other NLP frameworks.\r\n\r\nIt provides document conversion, pre-processing, data storage, vector databases, and model inference. It also wraps all these components in a neat pipeline abstraction. You can use a pipeline to run your application as a reliable and scalable service in production.\r\n\r\nIn this talk, machine learning engineers, data scientists, and NLP developers will learn how Haystack integrates with LLMs, such as GPT-3. We will show how to use the pipeline abstraction and retrieval-augmented generation to address issues like stale knowledge and hallucination. We will also provide a practical example by showing how to create a personal assistant for knowledge workers. Each step will be accompanied with open source code examples. By the end of the talk, you will have seen these concepts applied in practice and you will be able to build an assistant for your own use case.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25879, "code": "NDEHZM", "public_name": "Mathis Lucka", "biography": "Building great products for NLP developers at deepset. Been a PM and NLP Engineer for 5 years. On a mission to make sure that every developer can incorporate NLP features into any product that needs it.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B05-B06": [{"id": 26390, "guid": "fda8e74f-0bd6-5584-a412-7a3be8380e5c", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "00:45", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26390-honey-i-broke-the-pytorch-model-debugging-custom-pytorch-models-in-a-structured-manner", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/GXAKV8/", "title": "Honey, I broke the PyTorch model >.< - Debugging custom PyTorch models in a structured manner", "subtitle": "", "track": "PyData: Deep Learning", "type": "Talk (long)", "language": "en", "abstract": "When building PyTorch models for custom applications from scratch there's usually one problem: The model does not learn anything. In a complex project, it can be tricky to identify the cause: Is it the data? A bug in the model? Choosing the wrong loss function at 3 am after an 8-hour coding session?\r\n\r\nIn this talk, we will build a toolbox to find the culprits in a structured manner. We will focus on simple ways to ensure a training loop is correct, generate synthetic training data to determine whether we have a model bug or problematic real-world data, and leverage pytest to safely refactor PyTorch models. \r\n\r\nAfter this talk, visitors will be well equipped to take the right steps when a model is not learning, quickly identify the underlying reasons, and prevent bugs in the future.", "description": "PyTorch models for off-the-shelf applications are easy to build and debug. But in real-world ML applications, debugging can become quite tricky - especially when model complexity is high and only noisy real-world data is available. \r\n\r\nWhen our DNN is not learning many factors can be at fault: \r\n- Is there a bug in the model structure - for example mixed-up channels or timesteps? \r\n- Is our dataset not large or homogeneous enough to learn something? Have we mixed up labels in the preprocessing? \r\n- Have we chosen incorrect losses, accidentally skipped layers, or chosen inappropriate activation functions?\r\n\r\nThe plethora of potential reasons can be overwhelming to engineers. This talk will introduce a structured approach and valuable tools for efficiently debugging PyTorch models.\r\nWe'll start with techniques to check for correct training loops, such as ensuring our model overfits with a single training example. In the second step, we'll investigate how to generate simple, synthetic data for arbitrary input and output formats to validate our model. At last, we'll look at how to avoid model bugs altogether, by setting up universal tests that can be used during development and refactoring to prevent breaking PyTorch models.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25895, "code": "D7DKZV", "public_name": "Clara Hoffmann", "biography": "I'm a former ML Engineer in the geospatial domain and currently a Ph.D. student for trustworthy ML and Data Science at the RC Trust Ruhr. My main field of interest is Computer Vision and my guilty pleasure is assigning probability densities to all relevant variables in CV models. Application-wise I focus on Remote Sensing (Synthetic Aperture Radar) and Neuroscience (modeling trajectories of disease severity from MRI scans). I also collected a splash of experience in autonomous driving. \r\nI'm classically trained in Bayesian Statistics and interested in combining Bayesian approaches with self-supervised learning and deterministic DNNs.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26354, "guid": "47fdc6fd-6081-50dd-8b41-61b11752ad4d", "logo": "", "date": "2023-04-17T11:40:00+02:00", "start": "11:40", "duration": "00:45", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26354-autogluon-automl-for-tabular-multimodal-and-time-series-data", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WMAXSV/", "title": "AutoGluon: AutoML for Tabular, Multimodal and Time Series Data", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk (long)", "language": "en", "abstract": "AutoML, or automated machine learning, offers the promise of transforming raw data into accurate predictions with minimal human intervention, expertise, and manual experimentation. In this talk, we will introduce AutoGluon, a cutting-edge toolkit that enables AutoML for tabular, multimodal and time series data. AutoGluon emphasizes usability, enabling a wide variety of tasks from regression to time series forecasting and image classification through a unified and intuitive API. We will specifically focus on tasks on tabular and time series tasks where AutoGluon is the current state-of-the-art, and demonstrate how AutoGluon can be used to achieve competitive performance on tabular and time series competition data sets. We will also discuss the techniques used to automatically build and train these models, peeking under the hood of AutoGluon.", "description": "[AutoGluon](http://auto.gluon.ai) is a Python machine learning library which offers cutting edge accuracy and value-for-compute on a wide variety of tasks.  These tasks include regression, classification and quantile regression in tabular data, as well as multimodal tasks such as image classification, image-to-text and text-to-text similarity. A recent addition to AutoGluon is AutoGluon-TimeSeries, the library's module for time series forecasting tasks. \r\n\r\nAutoGluon is organized into modules for Tabular, Multimodal and Time Series tasks all of which share an intuitive scikit-learn-like API for fitting and performing inference with cutting-edge machine learning in as little as three lines of code, with no in-depth understanding of ML. AutoGluon is widely considered the state-of-the-art in tabular tasks as confirmed by the independent [AutoML Benchmark](https://openml.github.io/automlbenchmark/papers.html), and is the current top performer on multimodal tasks on the RAFT leaderboard. In this talk, we will focus on the tabular and time series modules and showcase how the library can be used to get competitive results on competition platforms such as Kaggle. \r\n\r\nAutoGluon also differs quite significantly under the hood from other AutoML frameworks. The library does not take AutoML to primarily mean hyperparameter optimization, but leans heavily into building (stack) ensembles of strong but varied learning algorithms to achieve superior results. We will also showcase some of the theory and building blocks of AutoGluon, describing how we built an AutoML system that takes model ensembling as a central element.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25871, "code": "XFKH3E", "public_name": "Caner Turkmen", "biography": "Caner Turkmen is a Senior Applied Scientist at Amazon Web Services, where he works on problems at the intersection of machine learning and forecasting, in addition to developing AutoGluon-TimeSeries. Before joining AWS, he worked in the management consulting industry as a data scientist, serving the financial services and telecommunications industries on projects across the globe. Caner\u2019s personal research interests span a range of topics, including forecasting, causal inference, and AutoML.", "answers": []}, {"id": 25875, "code": "PAPTCY", "public_name": "Oleksandr Shchur", "biography": "Oleksandr Shchur is an Applied Scientist at Amazon Web Services, where he works on time series forecasting in AutoGluon. Before joining AWS, he completed a PhD in Machine Learning at the Technical University of Munich, Germany, doing research on probabilistic models for event data. His research interests include machine learning for temporal data and generative modeling", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26375, "guid": "529c9bcb-87df-5653-bce7-20985948a8e3", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26375-common-issues-with-time-series-data-and-how-to-solve-them", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/ZRAFKA/", "title": "Common issues with Time Series data and how to solve them", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "Time-series data is all around us: from logistics to digital marketing, from pricing to stock\r\nmarkets. It\u2019s hard to imagine a modern business that has no time series data to forecast.\r\nHowever, mastering such forecasting is not an easy task.\r\nFor this talk, together with other domain experts, I have collected a list of common time\r\nseries issues that data professionals commonly run into. After this talk, you will learn to\r\nidentify, understand, and resolve such issues. This will include stabilising divergent time\r\nseries, organising delayed / irregular data, handling missing values without anomaly propagation,\r\nand reducing the impact of noise and outliers on your forecasting models.", "description": "This talk will walk you through 4 common issues with Time Series and illustrate them using\r\nthe context of energy demand forecasting. For each of these issues you will learn to identify,\r\nunderstand, and resolve them better. These issues are time series instability, delayed and\r\nirregular time series data, hard-to-impute missing values, impact of noise and outliers on\r\nforecasting models. The talk is therefore split into 4 parts each with some room for\r\nquestions. Each part will provide some high-level background, explanations, examples and\r\ncode snippets, while avoiding unnecessary in-depth computations and formulas. Therefore,\r\nthe whole talk is accessible to both specialists with experience in Time Series analytics as\r\nwell as those without such experience who nonetheless intend to broaden their\r\nunderstanding of this field and gain some valuable insights for the business problems that\r\nthey are likely to encounter in the future.\r\n\r\nData Scientists / Analysts working with time series data and understanding at least the\r\nbasics of Pandas / Scikit-learn Python libraries as well as what a time series forecasting\r\nproblem entails would benefit the most from this talk. However, other less technical\r\nspecialists (management, product owners etc.) can still gain valuable domain knowledge in\r\nthis field.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25891, "code": "JF8QLJ", "public_name": "Vadim Nelidov", "biography": "Vadim Nelidov is a Lead Data Science consultant at Xebia Data with diverse experience in the data domain in a variety of industries from energy sector and banking to skincare and agriculture. Throughout his years in the data world, Vadim has been combining advanced data science with business insights to make data work with an impact. He aspires to see far beyond what is on the surface and get to the essence of the problems, discovering robust and scalable long-term solutions rather than temporary fixes.\r\n\r\nVadim is passionate about sharing his knowledge and insights, believing that Data literacy should not be a privilege of a few. And his goal is to be there to make this a reality. Making the intricacies of data science intelligible and uncovering the regularities hiding in the data is a major source of inspiration for Vadim. With this goal in mind, he combines his years of experience in consulting with his background in statistics, research and teaching to make this knowledge accessible to businesses and individuals in need.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26182, "guid": "e6917cc4-00eb-53a7-9a16-de79a2d275f5", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26182-wald-a-modern-sustainable-analytics-stack", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/TP7ABB/", "title": "WALD: A Modern & Sustainable Analytics Stack", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "The name **WALD**-stack stems from the four technologies it is composed of, i.e. a cloud-computing **W**arehouse like Snowflake or Google BigQuery, the open-source data integration engine **A**irbyte, the open-source full-stack\r\nBI platform **L**ightdash, and the open-source data transformation tool **D**BT.\r\n\r\nUsing a Formula 1 Grand Prix dataset, I will give an overview of how these four tools complement each other perfectly for analytics tasks in an ELT approach. You will learn the specific uses of each tool as well as their particular features. My talk is based on a full tutorial, which you can find under [waldstack.org](https://waldstack.org).", "description": "The current zeitgeist is that the data lake concept from classical data engineering and modern data warehousing from business intelligence are converging more and more. This is also driving the shift from ETL to ELT, and so tools such as [dbt] are becoming increasingly important in combination with modern Big Data warehouses such as [Snowflake] and [Google BigQuery]. For typical data and MI engineers, this is quite a departure from familiar tools like [Spark]. \r\n\r\nHaving a pure Spark and ETL background myself, this trend motivated me to explore the foreign realms of ELT, data warehousing and especially the fuzz about [dbt]. In this talk I want to share my key insights with classical data / ml engineers that might have only heard about [Snowflake], [dbt], [Airbyte] and [Lightdash] but have never cared to dig deeper.\r\n\r\nMy talk is structured like this:\r\n* short introduction to the differences of data lake vs data warehouse, ETL vs ELT\r\n* high-level introduction of Snowflake, Airbyte, dbt, and Lightdash\r\n* demonstration based on the [Kaggle Formula 1 World Championship dataset] to see those four tools in action\r\n* my main take-aways and key insights\r\n\r\nAfter this talk, you will have learned the differences between ETL & ELT, what these four tools do and in which cases you should consider the WALD stack. Also, you will know how to use Python instead of SQL to define models in dbt, which is a brand-new feature.\r\n\r\nThe WALD-stack is sustainable since it consists mainly of open-source technologies, however all technologies are also offered as managed cloud services. The data warehouse itself, i.e. [Snowflake] or [Google BigQuery], is the only non-open-source\r\ntechnology in the WALD-stack. In my talk, I will focus on the open-source parts of the WALD-stack.\r\n\r\n[dbt]: https://www.getdbt.com/\r\n[Snowflake]: https://www.snowflake.com/\r\n[Lightdash]: https://github.com/lightdash/lightdash\r\n[Airbyte]: https://airbyte.com/\r\n[Google BigQuery]: https://cloud.google.com/bigquery\r\n[Kaggle Formula 1 World Championship dataset]: https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020\r\n[Spark]: https://spark.apache.org/", "recording_license": "", "do_not_record": false, "persons": [{"id": 102, "code": "8LQU9C", "public_name": "Florian Wilhelm", "biography": "Data Scientist and Python developer with a strong mathematical background. Always looking to apply mathematics to real-world problems and enthusiastic about everything math.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25279, "guid": "cdeed9db-0e1b-56ef-8bd4-8c6416c41daa", "logo": "", "date": "2023-04-17T16:20:00+02:00", "start": "16:20", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-25279-bhad-explainable-unsupervised-anomaly-detection-using-bayesian-histograms", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/HNKMMP/", "title": "BHAD: Explainable unsupervised anomaly detection using Bayesian histograms", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "The detection of outliers or anomalous data patterns is one of the most prominent machine learning use cases in industrial applications. I present a Bayesian histogram anomaly detector (BHAD), where the number of bins is treated as an additional unknown model parameter with an assigned prior distribution. BHAD scales linearly with the sample size and enables a straightforward explanation of individual scores, which makes it very suitable for industrial applications when model interpretability is crucial. I study the predictive performance of the proposed BHAD algorithm with various SoA anomaly detection approaches using simulated data and also using popular benchmark datasets for outlier detection. The reported results indicate that BHAD has very competitive predictive accuracy\r\ncompared to other more complex and computationally more expensive algorithms, while being explainable and fast.", "description": "I present an unsupervised and explainable Bayesian anomaly detection algorithm. For this I consider the posterior predictive distribution of a Categorical-Dirichlet distribution and use it to construct a Bayesian histogram-based anomaly detector (BHAD). \r\nBHAD scales linearly with the size of the data and allows a direct explanation of individual anomaly scores due to its simple linear functional form, which makes it very suitable for practical applications when model interpretability is crucial. Based on simulated data and also using popular benchmark datasets for outlier detetcion I analyze the predictive performances of the used candidate models and also compare them with outlier ensemble approaches. The results suggest that the proposed BHAD model has very competitive performance compared to other more complex models like variational autoencoders, in fact it is among the best performing candidates while offering individual and global model explainability.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25013, "code": "ZKSYAM", "public_name": "Alexander Vosseler", "biography": "Alexander Vosseler works as Principal Data Scientist at the Advanced Analytics Claims team of Allianz Germany - Chief Data Office in Munich. He has many years of industry experience as a data scientist and holds a PhD in Statistics with majors in Bayesian and Computational statistics. During his industry career as a data scientist he worked for companies such as Siemens AG, Allianz Global Corporate & Specialty SE and Allianz Germany.\r\n\r\nHis current methodological interests lies in probabilistic machine learning and uncertainty quantification with applications in time series methods, anomaly detection and NLP. In his spare time he likes to go jogging and play the drums.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A1": [{"id": 26251, "guid": "7e94b2d4-94e3-5d82-88e6-4f14bd3a5cba", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "00:45", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26251-from-notebook-to-pipeline-in-no-time-with-lineapy", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WAVRYZ/", "title": "From notebook to pipeline in no time with LineaPy", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk (long)", "language": "en", "abstract": "The nightmare before data science production: You found a working prototype for your problem using a Jupyter notebook and now it's time to build a production grade solution from that notebook. Unfortunately, your notebook looks anything but production grade. The good news is, there's finally a cure!\r\n\r\nThe open-source python package LineaPy aims to automate data science workflow generation and expediting the process of going from data science development to production. And truly, it transforms messy notebooks into data pipelines like Apache Airflow, DVC, Argo, Kubeflow, and many more. And if you can't find your favorite orchestration framework, you are welcome to work with the creators of LineaPy to contribute a plugin for it!\r\n\r\nIn this talk, you will learn the basic concepts of LineaPy and how it supports your everyday tasks as a data practitioner. For this purpose, we will transform a notebook step by step together to create a DVC pipeline. Finally, we will discuss what place LineaPy will take in the MLOps universe. Will you only have to check in your notebook in the future?", "description": "The nightmare before data science production: You found a working prototype for your problem using a Jupyter notebook and now it's time to build a production grade solution from that notebook. Unfortunately, your notebook looks anything but production grade. You embark on a time-consuming journey of refactoring the notebook. You come across irrelevant and relevant code snippets that are scattered in different cells but you persevere. Midway through your journey, you realize that your refactoring is not immune from the reproducibility issues caused by deleted cells and out-of-order cell executions. We haven't even talked about the creation of a pipeline from that notebook yet! A desperate situation indeed. The good news is, there's finally a cure!\r\n\r\nThe open-source python package LineaPy aims to automate data science workflow generation and expediting the process of going from data science development to production. And truly, it transforms messy notebooks into data pipelines like Apache Airflow, DVC, Argo, Kubeflow, and many more. And if you can't find your favorite orchestration framework, you are welcome to work with the creators of LineaPy to contribute a plugin for it!\r\n\r\nIn this talk, you will learn the basic concepts of LineaPy and how it supports your everyday tasks as a data practitioner. For this purpose, we will transform a notebook step by step together to create a DVC pipeline. Finally, we will discuss what place LineaPy will take in the MLOps universe. Will you only have to check in your notebook in the future?", "recording_license": "", "do_not_record": false, "persons": [{"id": 25800, "code": "F8ANLQ", "public_name": "Thomas Fraunholz", "biography": "Thomas has a great fondness for science. Strictly speaking for numerics. After his doctorate, he went to the school of embedded programming. During this time he got to know and love DevOps. His enthusiasm for number crunching ultimately led him to the topic of artificial intelligence. He is currently in charge of publicly funded open source research programs. When he\u2019s not trying to convince his colleagues to use DVC, he\u2019s busy with MLOps, CML and his low-budget bark beetle detection drone \u2013 once you\u2019ve done emdedded you just can\u2019t get away from it.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26484, "guid": "1422dabb-836b-50ab-9865-c9597e6927eb", "logo": "", "date": "2023-04-17T11:40:00+02:00", "start": "11:40", "duration": "00:45", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26484-incorporating-gpt-3-into-practical-nlp-workflows", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/77MWVW/", "title": "Incorporating GPT-3 into practical NLP workflows", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk (long)", "language": "en", "abstract": "In this talk, I'll show how large language models such as GPT-3 complement rather than replace existing machine learning workflows. Initial annotations are gathered from the OpenAI API via zero- or few-shot learning, and then corrected by a human decision maker using an annotation tool. The resulting annotations can then be used to train and evaluate models as normal. This process results in higher accuracy than can be achieved from the OpenAI API alone, with the added benefit that you'll own and control the model for runtime.", "description": "Software engineering is all about getting computers to do what we want them to do. As machine learning methods have improved, they've introduced a new way to specify the desired behaviour. Instead of writing code, you can prepare example data. Large language models are now starting to introduce a third option: instead of example data, you can provide a natural language prompt.\r\n\r\nWriting a prompt is far quicker than building a good set of training examples, but it's also a much less precise way to get the behaviour you want. There's also no reliable way to incrementally improve the results, even if better performance would be very valuable to you. Essentially, this new approach has a high floor, but a low ceiling.\r\n\r\nIn this talk, I'll show how large language models such as GPT3 complement rather than replace existing machine learning workflows. Initial annotations are gathered from the OpenAI API via zero- or few-shot learning, and then corrected by a human decision maker using the Prodigy annotation tool. The resulting annotations can then be used to train and evaluate models as normal. This process results in higher accuracy than can be achieved from the OpenAI API alone, with the added benefit that you'll own and control the model for runtime.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25952, "code": "FZKG9N", "public_name": "Ines Montani", "biography": "Ines Montani is a developer specializing in tools for AI and NLP technology. She\u2019s a Fellow of the Python Software Foundation, the co-founder and CEO of [Explosion](https://explosion.ai) and a core developer of [spaCy](https://spacy.io), one of the leading open-source libraries for Natural Language Processing in Python, and [Prodigy](https://prodi.gy), a modern annotation tool for creating training data for machine learning models.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26435, "guid": "7584de0a-a29a-5617-b0a9-27c920e9f79e", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26435-staying-alert-how-to-implement-continuous-testing-for-machine-learning-models", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/CHLT3D/", "title": "Staying Alert: How to Implement Continuous Testing for Machine Learning Models", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "Proper monitoring of machine learning models in production is essential to avoid performance issues. Setting up monitoring can be easy for a single model, but it often becomes challenging at scale or when you face alert fatigue based on many metrics and dashboards. \r\n\r\nIn this talk, I will introduce the concept of test-based ML monitoring. I will explore how to prioritize metrics based on risks and model use cases, integrate checks in the prediction pipeline and standardize them across similar models and model lifecycle. I will also take an in-depth look at batch model monitoring architecture and the use of open-source tools for setup and analysis.", "description": "Have you ever deployed a machine learning model in production only to realize that it wasn't performing as well as you thought it would, or was late to detect a model performance drop due to corrupted data? Proper monitoring can help avoid it. Typically, this involves checking the quality of the input data, monitoring the model's responses, and detecting any changes that might lead to model quality drops. \r\n\r\nHowever, setting up monitoring is often easier said than done. First, while it is easy to write a few assertions for data quality checks or track accuracy for a single model you created, it is much more challenging to do so consistently and at scale as the number of models, pipelines, and the volume of data increases. Second, building monitoring dashboards to track many metrics often leads to alert fatigue and does not help with root cause analysis of the problem.\r\n\r\nIn this talk, I will introduce the idea of test-based ML monitoring and how it can help you keep your models in check in production. I will cover the following:\r\n- The difference between testing and monitoring and when one is better than other\r\n- How to prioritize metrics and tests for each model based on risks and model use cases \r\n- How to integrate checks in the model prediction pipeline and standardize them across similar models and model lifecycle \r\n- An in-depth look at batch model monitoring architecture, including setup and analysis of results using open-source tools", "recording_license": "", "do_not_record": false, "persons": [{"id": 15956, "code": "VHQ7QV", "public_name": "Emeli Dral", "biography": "Emeli Dral is a Co-founder and CTO at Evidently AI, a startup developing open-source tools to evaluate, test, and monitor the performance of machine learning models.\r\n\r\nEarlier, she co-founded an industrial AI startup and served as the Chief Data Scientist at Yandex Data Factory. She led over 50 applied ML projects for various industries - from banking to manufacturing. Emeli is a data science lecturer at GSOM SpBU and Harbour.Space University. She is a co-author of the Machine Learning and Data Analysis curriculum at Coursera with over 100,000 students.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25673, "guid": "c3a5771e-75b1-5c34-9496-8da3685bb05d", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-25673-driving-down-the-memray-lane-profiling-your-data-science-work", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WHACAT/", "title": "Driving down the Memray lane - Profiling your data science work", "subtitle": "", "track": "PyData: Jupyter", "type": "Talk", "language": "en", "abstract": "When handling a large amount of data, memory profiling the data science workflow becomes more important. It gives you insight into which process consumes lots of memory.  In this talk, we will introduce Mamray, a Python memory profiling tool and its new Jupyter plugin.", "description": "In this talk, we will be exploring what memory profiling is, and how it can help with data science work. We will start the talk with a basic explanation of how Python arrange memories for various objects. This lays the foundation explanation of why we need a special tool to memory profile Python programs.\r\n\r\nThen we will be going through a data science use case where we memory profiles some part of the process with the Memray Jupyter plug-in. This would be a use case that a data science practitioner or learner would be familiar with and they can see how memory profiling could be useful. \r\n\r\nWe will then explain how to interpret the frame diagram in Memray, a commonly used diagram in memory profiling to understand how much memory a process and its sub-process uses. This is something that for a new user, it could be hard to understand and not know what to look into. From this example, audiences can see what they can learn about from the frame diagram.\r\n\r\n## Goal\r\n\r\nThis talk is for data scientists, learners or anyone who is interested in memory profiling their Python program. Although the talk will be using a data science use case as an example, the explanation and the tool can be expanded to be used in any Python program. However, for data science practitioners and learners who have been using Python to process data, this may be a step forward for them to improve their data workflow and prevent memory leaks from their programs.\r\n\r\n## Outline\r\n\r\n- Introduction (5 mins)\r\n- Why we need a special tool for memory profiling (5 mins)\r\n- How to use Memray in Jupyter notebook (5 mins)\r\n- Demonstration for using Memray in data science work (5 mins)\r\n- How to interpret a frame diagram (5 mins)\r\n- Conclusion (5 mins)", "recording_license": "", "do_not_record": false, "persons": [{"id": 54, "code": "8EGVC9", "public_name": "Cheuk Ting Ho", "biography": "After having a career in data science, Cheuk now brings her knowledge of data and passion for the tech community as the developer advocate for Anaconda. Cheuk constantly contributes to the open-source community by giving free talks and tutorials and organising sprints to encourage diverse contributions.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26424, "guid": "d6ea531c-e3ee-53bd-a415-07b29dd7b9d1", "logo": "", "date": "2023-04-17T16:20:00+02:00", "start": "16:20", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26424-specifying-behavior-with-protocols-typeclasses-or-traits-who-wears-it-better-python-scala-3-rust-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MJRFLC/", "title": "Specifying behavior with Protocols, Typeclasses or Traits. Who wears it better (Python, Scala 3, Rust)?", "subtitle": "", "track": "PyCon: Python Language", "type": "Talk", "language": "en", "abstract": "In this talk, we will explore the use of Python's `typing.Protocol`, Scala's Typeclasses, and Rust's Traits. \r\nThey all offer a very powerful & elegant mechanism for abstracting over various concepts (such as Serialization) in a modular manner.\r\nWe will compare and contrast the syntax and implementation of these constructs in each language and discuss their strengths and weaknesses. We will also look at real-world examples of how these features are used in each language to specify behavior, and consider differences in terms of type system expressiveness and effectiveness. By the end of the talk, attendees will have a better understanding of the differences and similarities between these three language features, and will be able to make informed decisions about which one is best suited for their needs.", "description": "Within simple applications abstractions are only needed to a certain degree. \r\nE.g., why would someone need a complex class hierarchy, if the task at hand could be solved more pragmatically?\r\nHowever, as applications and the business get more complex, abstractions can become crucial for improving the quality and maintainability of your code.\r\nWith `typing.Protocol` a great Python language feature was introduced, which allows abstraction and modularization while also having static typing. This allows for very robust software development. \r\nHow do other languages solve that problem?\r\nBesides `typing.Protocol` we\u2019ll also dive into the world of Scala Typeclasses, and Rust Traits, and explore how these features are used in each language to ensure the correctness and safety of code.\r\nAll these mechanisms have in common that they specify behavior for types in a very flexible and safe manner.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25918, "code": "KND3NG", "public_name": "Kolja Maier", "biography": "I love architecting & building data products that have business impact. Drilling into business domains and leveraging data to thrive the business excites me. Passionate about shaping company culture around and towards data.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A03-A04": [{"id": 26109, "guid": "d4d6b141-185e-5dea-8cc3-a42771f2d754", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-26109-how-to-teach-nlp-to-a-newbie-get-them-started-on-their-first-project", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/BCRDQ8/", "title": "How to teach NLP to a newbie & get them started on their first project", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Tutorial", "language": "en", "abstract": "The materials presented during this tutorial are open source and can be used by coaches and tutors who want to teach their students how to use Python for text processing and text classification. (A minimal understanding of programming (in any language) is required by the students)", "description": "The materials presented at this tutorial were initially created for high school and university students to help them to get started with their first machine learning project using textual data. Machine learning on textual data is more accessible for beginners because it does not involve missing data imputation, normalisation and scaling. It is also easier to analyse and interpret the results (e.g. why something was misclassified). There are many introductory courses on NLP on the internet, however, they are not for free and they either only cover complete basics\u00b9, or do not cover machine learning algorithms\u00b2 and treat models as a black box. Also, they do not show how to do research correctly (e.g. setting a baseline, making design decisions based on correct validation etc). These materials in the form of jupyter notebooks can be used by teachers to guide their students through an NLP research project from start to finish.\r\n\r\nThese materials are of course not limited to teachers and tutors at academic institutions. Many companies rely on customer reviews, social media, client records, and various other content created in natural language, but often use sub-optimal solutions to analyse it (like MS Excel). These materials will give working professionals all the tools to get started with text analysis, as well as teach them the fundamentals of machine learning, so they can automate document labelling and other manual tasks with the help of document classification (e.g. Is a customer review positive or negative? Is a certain document about topic X or topic Y?). A minimal understanding of programming (in any language) is required. However, all necessary Python libraries will be covered.\r\n\r\nThe aim of the tutorial would be to present the materials which contains 7 \u201clectures\u201d, several practical exercises with solutions, and a case study and hence can be covered in either 10 hours (10 weeks) over a term or a 2-day workshop.\r\n\r\n\r\n\r\n\u00b9https://www.udemy.com/course/natural-language-processing/\r\n\r\n\u00b2https://www.udemy.com/course/nlp-natural-language-processing-with-python/", "recording_license": "", "do_not_record": false, "persons": [{"id": 25690, "code": "GPD9BB", "public_name": "Lisa Andreevna Chalaguine", "biography": "Originally from Belarus, I currently live in London and work as a data scientist at ProcureAI. I did my PhD at University College London, where I used to develop algorithms for chatbots that can engage in argumentative dialogues, trying to persuade the user to accept the chatbot's stance. My expertise therefore mainly lies in knowledge acquisition/representation and, of course, natural language processing. I also have over 10 years of experience in teaching and tutoring. I am also a tutor at The Profs, and a member of the part-time tutor panel at the University of Oxford in the Department of Continuing Education.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26201, "guid": "da040883-961c-5229-86ce-422562b89d60", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-26201-how-to-baseline-in-nlp-and-where-to-go-from-there", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/B7PCUR/", "title": "How to baseline in NLP and where to go from there", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "In this talk, we will explore the build-measure-learn paradigm and the role of baselines in natural language processing (NLP). We will cover the common NLP tasks of classification, clustering, search, and named entity recognition, and describe the baseline approaches that can be used for each task. We will also discuss how to move beyond these baselines through weak learning and transfer learning. By the end of this talk, attendees will have a better understanding of how to establish and improve upon baselines in NLP.", "description": "In this talk, we will explore the role of baselines in natural language processing (NLP)  and discuss how to move beyond these baselines through weak learning and transfer learning.\r\n\r\nFirst, I will introduce the build-measure-learn paradigm, which is a framework for developing and improving products or systems. This paradigm involves building a solution, measuring its performance, and learning from the results to iteratively improve the solution. Baselines are an essential part of this process because they provide a starting point for comparison and a benchmark to measure against.\r\n\r\nNext, I will delve into the common NLP tasks of classification, clustering, search, and named entity recognition (NER). For each task, I will describe the baseline approaches that can be used. These baselines may not be the most advanced or sophisticated solutions, but they are often quick and easy to implement, and they can serve as a useful reference and guidance for further improvement.\r\n\r\nFinally, I will discuss how to move on from these baselines. One option is to use insights from the baselines to build a weak learning system, which is a machine learning model that relies on human-generated rules or patterns rather than a large dataset. Another option is to leverage transfer learning, which involves adapting a pre-trained model to a new task or domain by fine-tuning its parameters on a smaller dataset.\r\n\r\nIn conclusion, this talk will provide a practical guide to establishing baselines in NLP and moving beyond them through weak learning and transfer learning.", "recording_license": "", "do_not_record": false, "persons": [{"id": 1673, "code": "TVP7FL", "public_name": "Tobias Sterbak", "biography": "Tobias Sterbak is a Data Scientist and Software Developer from Berlin. He has been working as a freelancer in the field of Machine Learning and Natural Language Processing since 2018. On the blog www.depends-on-the-definition.com he occasionally writes about these topics. In his private life he is interested in data privacy, open source software, remote work and dogs.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26223, "guid": "5f4fbbac-2da7-5ab5-be3f-5a8cfb40a93c", "logo": "", "date": "2023-04-17T15:45:00+02:00", "start": "15:45", "duration": "00:45", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-26223-performing-root-cause-analysis-with-dowhy-a-causal-machine-learning-library", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/FVWF7R/", "title": "Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk (long)", "language": "en", "abstract": "In this talk, we will introduce the audience to [DoWhy](https://www.pywhy.org/dowhy), a library for causal machine-learning (ML). We will introduce typical problems where causal ML can be applied and will specifically do a deep dive on root cause analysis using DoWhy. To do this, we will lay out what typical problem spaces for causal ML look like, what kind of problems we're trying to solve, and then show how to use DoWhy's API to solve these problems. Expect to see a lot of code with a hands-on example. We will close this session by zooming out a bit and also talk about the PyWhy organization governing DoWhy.", "description": "_\"Much like machine learning libraries have done for prediction, DoWhy is a Python library that aims to spark causal thinking and analysis. DoWhy provides a wide variety of algorithms for effect estimation, causal structure learning, diagnosis of causal structures, root cause analysis, interventions and counterfactuals.\"_\r\n\r\nThe field of causal machine-learning (ML) is not as well-known as typical machine-learning problems and libraries. DoWhy is one of the more popular open-source libraries for causal ML. And not for nothing: DoWhy is based on the two major scientific frameworks, Potential Outcome and Graphical Causal Models and offers a large variety of features.\r\n\r\nProblems where causal ML can be applied, come from any imaginable domain, be that distributed computer systems, supply chain, workflow management, manufacturing, etc. As long as a complex system can be represented as a causal graph, one can also apply causal ML.\r\n\r\nIn the talk, we will specifically dive into a microservice architecture, as this is an example which an audience like the one at PyCon can most likely relate to. We will present some data and then inject outliers (or anomalies) into that data, see how those propagate through the system, and then use DoWhy's algorithms to show us the root cause.\r\n\r\nBy the end of the talk, the audience should have a good understanding of typical problem domains for causal ML and a good sense of how to use DoWhy to solve such problems.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25881, "code": "RNYUBB", "public_name": "Patrick Bl\u00f6baum", "biography": "Patrick Bl\u00f6baum is a Senior Applied Scientist at AWS, where he develops, implements and applies novel causal inference methods to business problems. He is also a main contributor to the open-source library DoWhy and the PyWhy organization. Prior to working at AWS, he got his PhD degree in the area of causality focusing on graphical causal models. His research interests include topics such as root cause analysis and causal discovery.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A05-A06": [{"id": 26232, "guid": "e05b96c4-5b88-5de5-b219-a305ce4e4e6e", "logo": "", "date": "2023-04-17T10:50:00+02:00", "start": "10:50", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26232-accelerate-python-with-julia", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/UQ3KXD/", "title": "Accelerate Python with Julia", "subtitle": "", "track": "General: Others", "type": "Tutorial", "language": "en", "abstract": "Speeding up Python code has traditionally been achieved by writing C/C++ \u2014 an alien world for most Python users. Today, you can write high performance code in Julia instead, which is much much easier for Python users. This tutorial will give you hands-on experience writing a Python library that incorporates Julia for performance optimization.", "description": "Julia is a modern data science language which solves the two-language problem by being both easy to use and high performant. Although different from Python, the language can be quickly learned by  Python users, making it a good choice for speeding up pieces of code. Julia, in addition to being a similar language, is designed for high-performance applied mathematics and has high-quality libraries for multi-dimensional arrays, dataframes, distributed computing and more.\r\n\r\nThe older alternative of writing pieces of code in C, C++, Cython or Rust is much more cumbersome: Here, programmers have to cope with a low-level language, static types, pointers, no garbage collector, lack of scientific libraries and other difficulties not normally faced by Python users. There was simply no better alternative.\r\n\r\nThe tutorial will be fully hands-on, using Jupyter Notebook and Binder to provide a smooth and easy-to-use environment for each participant. Both Julia and Python work seamlessly within Jupyter. We start with an introduction to the basics of Julia, focusing on the core differences with Python and how to work around common translation difficulties. Then we'll take a look at the interfaces between Julia and Python and build a Python sample project that runs Julia code. Finally, we will benchmark our solution.\r\n\r\nAfter the tutorial you will be able to use Julia to speed up your Python code.", "recording_license": "", "do_not_record": false, "persons": [{"id": 4390, "code": "MJPZGQ", "public_name": "Stephan Sahm", "biography": "Stephan Sahm is founder of the Julia consultancy Jolin.io, full stack senior data/ml consultant, and organizer of the Julia User Group Munich. \r\n\r\nStephan Sahm's top interest are in green computing, probabilistic programming, real time analysis, big data, applied machine learning and in general industry applications of Julia.\r\n\r\nAside Julia and sustainable computing, he likes chatting about Philosophy of Mind, Ethics, Consciousness, Artificial Intelligence and other Cognitive Science topics.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26300, "guid": "bde5791b-861a-50ca-b01b-317eb386ace6", "logo": "", "date": "2023-04-17T15:10:00+02:00", "start": "15:10", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26300-practical-session-learning-on-heterogeneous-graphs-with-pyg", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/PQZR3Q/", "title": "Practical Session: Learning on Heterogeneous Graphs with PyG", "subtitle": "", "track": "PyCon: Libraries", "type": "Tutorial", "language": "en", "abstract": "Learn how to build and analyze heterogeneous graphs using PyG, a machine graph learning library in Python. This workshop will provide a practical introduction to the concept of heterogeneous graphs and their applications, including their ability to capture the complexity and diversity of real-world systems. Participants will gain experience in creating a heterogeneous graph from multiple data tables, preparing a dataset, and implementing and training a model using PyG.", "description": "Heterogeneous graphs are powerful tools for representing and analyzing complex systems. They are able to capture the complexity and diversity of data, provide more accurate and relevant insights, integrate multiple data sources, and support the development of sophisticated graph algorithms. In this workshop, we will use PyG, a machine graph learning library in Python, to build and analyze heterogeneous graphs.\r\nWe will start with a discussion of the concept of heterogeneous graphs and their applications, and then move on to a practical session. Participants will learn how to create a heterogeneous graph from multiple data tables and use PyG to implement and train a model. By the end of the workshop, participants will have a solid understanding of the benefits and capabilities of heterogeneous graphs, as well as practical skills for building and analyzing them with PyG.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25521, "code": "CTYCUT", "public_name": "Ramona Bendias", "biography": "I have a Master's degree in science and am currently working as a Applied Machine Learning Engineer at Kumo,ai, where I use my skills in machine learning and data analysis to solve challenging problems. In addition to my work at Kumo, I am also a contributor to PyG, a machine graph learning library in Python.", "answers": []}, {"id": 25867, "code": "FYHXQT", "public_name": "Matthias Fey", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}]}}, {"index": 2, "date": "2023-04-18", "day_start": "2023-04-18T04:00:00+02:00", "day_end": "2023-04-19T03:59:00+02:00", "rooms": {"Kuppelsaal": [{"id": 28578, "guid": "0ed1ebe8-3fb2-55ef-aefb-9e7ce704ba66", "logo": "", "date": "2023-04-18T09:15:00+02:00", "start": "09:15", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-28578-keynote-how-are-we-managing-data-teams-management-irl", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MBH7GB/", "title": "Keynote - How Are We Managing? Data Teams Management IRL", "subtitle": "", "track": "Plenary", "type": "Keynote", "language": "en", "abstract": "The title \u201cData Scientist\u201d has been in use for 15 years now. We have been attending PyData conferences for over 10 years as well. The hype around data science and AI seems higher than ever before. But How are we managing?", "description": "Most of our conferences are about practical applications, methodologies, and platforms. In this talk, I want to focus on contemporary data science management. Including: \r\n- Our patterns and antipatterns. \r\n- The challenges we are facing as individual contributors, teams, managers, and leaders. \r\n- How the data science function has matured. \r\n- The unique aspects of Data Science compared to management in general, and software engineering in particular.\r\n\r\nIf you are a data scientist, or work with some of us, you might be interested to learn about what makes us tick, what makes us great colleagues, and yes, even what makes us challenging to work with \ud83d\ude09.", "recording_license": "", "do_not_record": false, "persons": [{"id": 15468, "code": "AHQBCP", "public_name": "Noa Tamir", "biography": "Noa have been involved with the R and PyData communities for some time, with a focus on community building and DEI. They are a NumFOCUS member of the Board of Directors and DISC committee, PyLadies Organizer, and chaired the PyData Berlin 2022 conference. In addition, they are a Lead Data Science Coach at neue fische, contributing to pandas, and are currently developing the Contributor Experience Community and Handbook with Inessa Pawson and Melissa Mendon\u00e7a.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26188, "guid": "89d4de74-c436-5384-9f06-573bf15de028", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26188-what-could-possibly-go-wrong-an-incomplete-guide-on-how-to-prevent-detect-mitigate-biases-in-data-products", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/NUF87W/", "title": "What could possibly go wrong? - An incomplete guide on how to prevent, detect & mitigate biases in data products", "subtitle": "", "track": "General: Ethics & Privacy", "type": "Talk", "language": "en", "abstract": "Within this talk, I want to look at the topic of data ethics with a practical lens and facilitate the discussion about how we can establish ethical data practices into our day to day work. I will shed some light on the multiple sources of biases in data applications: Where are potential pitfalls and how can we prevent, detect and mitigate them early so they never become a risk for our data product. I will walk you through the different stages of a data product lifecycle and dive deeper into the questions we as data professionals have to ask ourselves throughout the process. Furthermore, I will present methods, tools and libraries that can support our work. Being well aware that there is no universal solution as tools and strategies need to be chosen to specifically address requirements of the use-case and models at hand, my talk will provide a good starting point for your own data ethics journey.", "description": "Terms like trustworthy, responsible or ethical AI have been popular buzzwords for some time. But while we've seen some startling examples of \u2018AI gone wrong\u2019, such as when <a href=\"https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html\">Facebook falsely classified black persons as \u2018Primates\u2019</a>, <a href=\"https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine\">Amazon\u2019s hiring algorithm discriminated against women</a> or the <a href=\"https://blogs.lse.ac.uk/impactofsocialsciences/2020/08/26/fk-the-algorithm-what-the-world-can-learn-from-the-uks-a-level-grading-fiasco/\">A-level algorithmic grading fiasco in the UK</a>, for many data projects ethical considerations only come into play as an afterthought - if at all. Experience has shown that more accountability and transparency are needed in AI systems, and regulatory initiatives such as the <a href=\"https://artificialintelligenceact.eu/\">EU AI Act</a> make it increasingly important to treat the topic as a first-class citizen throughout the whole development process.\r\n\r\nWhile the implementation of legal initiatives and ethics guidelines raise awareness and bring the topic into focus, it often remains quite abstract and difficult to translate into our day to day work. Therefore, I want to look at the topic with a practical lens and facilitate the discussion about how we can establish ethical data practices. I will shed some light on the multiple sources of biases in data applications: Where are potential pitfalls and how can we prevent, detect and mitigate them early so they never become a risk for our data product. \r\n\r\nI will walk you through the different stages of a data product lifecycle and dive deeper into the questions we as data professionals have to ask ourselves throughout the process. Furthermore, I will present methods, tools and libraries that can support our work. Being well aware that there is no universal solution as tools and strategies need to be chosen to specifically address requirements of the use-case and models at hand, my talk will provide a good starting point for your own data ethics journey.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25758, "code": "GBF7R3", "public_name": "Lea Petters", "biography": "Data Scientist & Data PM @ inovex | PhD Behavioral Economics | special interests: data ethics, causality, mathematical modeling, data strategy", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26434, "guid": "8db2e738-2232-5b40-bca8-eb506d45f46c", "logo": "", "date": "2023-04-18T11:05:00+02:00", "start": "11:05", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26434-rusty-python-a-case-study", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/LMGF8V/", "title": "Rusty Python: A Case Study", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Python is a very expressive and powerful language, but it is not always the fastest option for performance-critical parts of an application. Rust, on the other hand, is known for its lightning-fast runtime and low-level control, making it an attractive option for speeding up performance-sensitive portions of Python programs.\r\n\r\nIn this talk, we will present a case study of using Rust to speed up a critical component of a Python application. We will cover the following topics:\r\n\r\n* An overview of Rust and its benefits for Python developers\r\n* Profiling and identifying performance bottlenecks in Python application\r\n* Implementing a solution in Rust and integrating it with the Python application using PyO3\r\n* Measuring the performance improvements and comparing them to other optimization techniques\r\n\r\nAttendees will learn about the potential for using Rust to boost the performance of their Python programs and how to go about doing so in their own projects.", "description": "# Context\r\nIn the past, C and C++ were the go-to languages for optimizing Python code while still maintaining a high-level interface. This approach was used by well-known numerical libraries such as Numpy and Pandas. However, with the increasing popularity of Rust and the emergence of PyO3, this is no longer the only solution available. Rust's impressive performance and expressive syntax, combined with its comprehensive library ecosystem, make it a viable alternative for optimizing performance-sensitive parts of Python applications. Additionally, Rust's mature support for asynchronous programming gives it an advantage over C foreign function interfaces when interacting with Python coroutines. Some library maintainers are even considering using Rust for their projects, such as Pydantic, which is implementing version 2 in Rust and achieving similar speed improvements to those obtained using C.\r\n\r\n# Timeplan\r\nIn minutes\r\n* 0-2: Welcome, explanation of title\r\n* 2-7: What is Rust and how is it different to other \"bare metal\" languages\r\n* 7-10: Introducing the case study, running the code, getting feel for performance\r\n* 10-15: Code profiling, finding of bottle neck\r\n* 15-17: Introducing PyO3\r\n* 17-22: Walking through the Rust code that optimizes the bottle neck\r\n* 22-25: Running the code live, showing the speedup\r\n* 25-28: Mention extensions provided by PyO3, caveats and what code might not be a good goal to optimize. Mention tradeoffs to other foreign function interfaces.\r\n* 28-30: Buffer / Q&A", "recording_license": "", "do_not_record": false, "persons": [{"id": 25922, "code": "QNDB8Z", "public_name": "Robin Raymond", "biography": "I have had a diverse career, starting out in academia as a mathematician and transitioning to work in startups as an engineer in the second half of my career. I thrive on wearing many hats and having a strong impact on the direction of the company. For the past two years, I have been working at Taktile, a series A startup with around 50 employees, where I serve as the lead technical engineer. My passion for programming has led me to work with a wide range of languages, including Haskell, Rust, C++, Typescript, and Python.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26695, "guid": "5c05eb3f-2bf1-5ba4-8d77-63c329b93963", "logo": "", "date": "2023-04-18T11:40:00+02:00", "start": "11:40", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26695-5-things-about-fastapi-i-wish-we-had-known-beforehand", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/GBYWCY/", "title": "5 Things about fastAPI I wish we had known beforehand", "subtitle": "", "track": "PyCon: Libraries", "type": "Talk", "language": "en", "abstract": "An exchange of views on fastAPI in practice.\r\n\r\nFastAPI is great, it helps many developers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. \r\n\r\nFastAPI does a great job of getting people started with APIs quickly.\r\n\r\nThis talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions.", "description": "An exchange of views on fastAPI in practice.\r\n\r\nFastAPI is great, it helps many developers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. \r\n\r\nFastAPI does a great job of getting people started with APIs quickly.\r\n\r\nThis talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions.\r\n\r\nThis talk will include the following:\r\n\r\n### fastAPI is built on the shoulders of giants I: [pydantic](https://docs.pydantic.dev/)\r\nFastAPI makes extensive use of [pydantic](https://docs.pydantic.dev/). [pydantic](https://docs.pydantic.dev/) parses data, can validate (and transform) data, and has built-in interfaces to export OpenAPI definitions among many other features. \r\n\r\n### fastAPI is built on the shoulders of giants I: [starlette](https://www.starlette.io)\r\nRoutes and middleware are managed by [starlette](https://www.starlette.io). In this section we will explore how to create custom middleware and what we learned along the way. \r\n\r\n### fastAPI has tutorials, but is this documentation?\r\nThe fastAPI page provides a good introduction. The more we worked with fastAPI, the harder it was to find accurate documentation. Looking at the source code, we really missed DocStrings! Introspection to the rescue - will probably include a rant about missing DocStrings!\r\n\r\n### DRY (\"Don't repeat yourself\") with pydantic\r\n\r\nFor our use case, we decided to use strict models to validate our data structures, as we work in a highly regulated industry where no mistakes are allowed to happen. Setting up the REST API was much easier than developing consistent models that generalise well. We follow the \"single source of truth\" paradigm, entering redundant definitions is an absolute no-go.\r\nIn this section we show how to create highly reusable pydantic model pools with inheritance for use in fastAPI. For testing, we also created models from metadata!\r\n\r\n\r\n### \"The road not taken\": pydantic Depends()!\r\n\r\nAPI routes often consist of a request model and a response model. But what about cases where the models alone don't work and a model and e.g. query parameters need to be mixed?\r\nApart from flake8 complaining about having callables in the signature, this can be quite a difficult use case. Strategies for resolving model/parameter conflicts.\r\n\r\nBonus - if time:\r\n### Integrating fastAPI with Sphinx.\r\nDemonstrate how to integrate OpenAPI with your Sphinx documentation.\r\n\r\nThe talk will show how fastAPI is built and how well introspection can help you understand what is going on under the hood and which library is actually doing the heavy lifting where.", "recording_license": "", "do_not_record": false, "persons": [{"id": 228, "code": "8F38DV", "public_name": "Alexander CS Hendorf", "biography": "Alexander Hendorf is responsible for data and artificial intelligence at the boutique consultancy K\u00d6NIGSWEG GmbH. Through his commitment as a speaker and chair of various international conferences as PyConDE & PyData Berlin, he is a proven expert in the field of data intelligence. He's been appointed Python Software Foundation and EuroPython fellow for this various contributions. He has many years of experience in the practical application, introduction and communication of data and AI-driven strategies and decision-making processes.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28270, "guid": "01bd22f4-0ee6-5451-9d8b-3f3a4896d906", "logo": "", "date": "2023-04-18T13:15:00+02:00", "start": "13:15", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-28270-keynote-towards-learned-database-systems", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/JZSYA3/", "title": "Keynote - Towards Learned Database Systems", "subtitle": "", "track": "Plenary", "type": "Keynote", "language": "en", "abstract": "Database Management Systems (DBMSs) are the backbone for managing large volumes of data efficiently and thus play a central role in business and science today. For providing high performance, many of the most complex DBMS components such as query optimizers or schedulers involve solving non-trivial problems. To tackle such problems, very recent work has outlined a new direction of so-called learned DBMSs where core parts of DBMSs are being replaced by machine learning (ML) models which has shown to provide significant performance benefits. However, a major drawback of the current approaches to enabling learned DBMS components is that they not only cause very high overhead for training an ML model to replace a DBMS component but that the overhead occurs repeatedly which renders these approaches far from practical. Hence, in this talk, I present my vision of Learned DBMS Components 2.0 to tackle these issues. First, I will introduce data-driven learning where the idea is to learn the data distribution over a complex relational schema. In contrast to workload-driven learning, no large workload has to be executed on the database to gather training data. While data-driven learning has many applications such as cardinality estimation or approximate query processing, many DBMS tasks such as physical cost estimation cannot be supported. I thus propose a second technique called zero-shot learning which is a general paradigm for learned DBMS components. Here, the idea is to train model", "description": "Database Management Systems (DBMSs) are the backbone for managing large volumes of data efficiently and thus play a central role in business and science today. For providing high performance, many of the most complex DBMS components such as query optimizers or schedulers involve solving non-trivial problems. To tackle such problems, very recent work has outlined a new direction of so-called learned DBMSs where core parts of DBMSs are being replaced by machine learning (ML) models which has shown to provide significant performance benefits. However, a major drawback of the current approaches to enabling learned DBMS components is that they not only cause very high overhead for training an ML model to replace a DBMS component but that the overhead occurs repeatedly which renders these approaches far from practical. Hence, in this talk, I present my vision of Learned DBMS Components 2.0 to tackle these issues. First, I will introduce data-driven learning where the idea is to learn the data distribution over a complex relational schema. In contrast to workload-driven learning, no large workload has to be executed on the database to gather training data. While data-driven learning has many applications such as cardinality estimation or approximate query processing, many DBMS tasks such as physical cost estimation cannot be supported. I thus propose a second technique called zero-shot learning which is a general paradigm for learned DBMS components. Here, the idea is to train models that generalize to unseen data sets out of the box. The idea is to train a model that has observed a variety of workloads on different data sets and can thus generalize. Initial results on the task of physical cost estimates suggest the feasibility of this approach. Finally, I discuss further opportunities which are enabled by zero-shot learning.", "recording_license": "", "do_not_record": false, "persons": [{"id": 27404, "code": "39UWHP", "public_name": "Carsten Binnig", "biography": "Carsten Binnig is a Full Professor in the Computer Science department at TU Darmstadt and a Visiting Researcher at the Google Systems Research Group. Carsten received his Ph.D. at the University of Heidelberg in 2008. Afterwards, he spent time as a postdoctoral researcher in the Systems Group at ETH Zurich and at SAP working on in-memory databases. Currently, his research focus is on the design of scalable data systems on modern hardware as well as machine learning for scalable data systems. His work has been awarded a Google Faculty Award, as well as multiple best paper and best demo awards.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26388, "guid": "ba86f7c4-d336-5379-b0d1-7c2bf119b0ec", "logo": "", "date": "2023-04-18T14:10:00+02:00", "start": "14:10", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26388-getting-started-with-jax", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/TWPBZF/", "title": "Getting started with JAX", "subtitle": "", "track": "PyData: Deep Learning", "type": "Talk", "language": "en", "abstract": "Deepminds JAX ecosystem provides deep learning practitioners with an appealing alternative to TensorFlow and PyTorch. Among its strengths are great functionalities such as native TPU support, as well as easy vectorization and parallelization. Nevertheless, making your first steps in JAX can feel complicated given some of its idiosyncrasies. This talk helps new users getting started in this promising ecosystem by sharing practical tips and best practises.", "description": "Deepminds JAX ecosystem provides deep learning practitioners with an appealing alternative to Tensorflow and Pytorch. Among its strengths are great functionalities such as native TPU support, as well as easy vectorization and parallelization which make JAX and its ecosystem an attractive option for your deep learning projects. Nevertheless, making your first steps can feel complicated. From pure functions and the resulting differences in coding style, to avoiding recompilation, JAX comes with its own set of restrictions and design decisions to be taken by the user. \r\n\r\nThis talk wants to help new and prospective users in their JAX learning journey, by providing guidance regarding practical problems they are likely to encounter when transitioning into the JAX ecosystem. Having recently switched to using Jax and Flax for my daily work this talk shares some of the insights I gained and wants to help them to avoid some of the mistakes I made early on. The talk will have a systematic look at selected situations in which JAX provides users with choices, seeing how they differ, and which is the best option given different circumstances. \r\n\r\nThe talk covers: \r\n- Why bother switching to JAX? \r\n- A brief introduction to JAX including a list of JAX\u2019s idiosyncrasies \r\n- Pure functions and the resulting architectural decisions \r\n- To  JIT and or not to JIT \r\n- A speed and memory comparison of the different iteration options \r\n- Memory management and profiling", "recording_license": "", "do_not_record": false, "persons": [{"id": 25894, "code": "QLJTCV", "public_name": "Simon Pressler", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26506, "guid": "6b13224c-8a64-5091-ba33-13df77b9c8fa", "logo": "", "date": "2023-04-18T14:45:00+02:00", "start": "14:45", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26506-when-a-b-testing-isn-t-an-option-an-introduction-to-quasi-experimental-methods", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/B8FKHC/", "title": "When A/B testing isn\u2019t an option: an introduction to quasi-experimental methods", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk (long)", "language": "en", "abstract": "Identification of causal relationships through running experiments is not always possible. In this talk, an alternative approach towards it, quasi-experimental frameworks, is discussed. Additionally, I will present how to adjust well-known machine-learning algorithms so they can be used to quantify causal relationships.", "description": "### What problem is the talk addressing?\r\n\r\nExperiments are a gold standard for estimating causal relationships. That being said, they are not always possible. Experiments can be costly, long-lasting, unethical, or illegal. In other cases, the underlying assumptions for identification cannot be met, e.g. it is not possible to split subjects into control and treatment groups randomly or avoid interactions between them.\r\n\r\n### Why is the problem relevant to the audience?\r\n\r\nUnderstanding the magnitude of treatment effects is a premise for designing optimal strategies by policy makers/stakeholders.\r\n\r\n### What are the solutions to the problem?\r\n\r\nPrediction-driven algorithms might not be best-tailored for accurate identification of causal links. In this talk I will show how to shift the goal post of those algorithms from prediction towards identification of treatment effects. First, I will cover classical quasi-experimental frameworks such as difference-in-differences and regression discontinuity design. Then, I shed some light on how to augment those methods with out-of-the-box machine-learning techniques. To this end, orthogonal machine learning will be discussed. \r\n\r\n### What are the main takeaways from the talk?\r\n\r\nI will reiterate that correlation does not imply causation. The audience will get familiarized with causal-inference methods used when laboratory experiments are not feasible. The participants will learn how to adjust off-the-shelf machine-learning algorithms to identify conditional average treatment effects.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25963, "code": "WGQ9TJ", "public_name": "Inga Janczuk", "biography": "A data scientist with a background in economics and econometrics. Currently working at OLX Group in Berlin, focusing on designing and deploying solutions for marketing optimization and automation.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28755, "guid": "7ea5d63a-a9b1-54a8-a66b-48c8335e79eb", "logo": "", "date": "2023-04-18T16:00:00+02:00", "start": "16:00", "duration": "01:00", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-28755-pyladies-panel-session-tech-illusions-and-the-unbalanced-society-finding-solutions-for-a-better-future", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/HDUAM9/", "title": "PyLadies Panel Session. Tech Illusions and the Unbalanced Society: Finding Solutions for a Better Future", "subtitle": "", "track": "General: Python & PyData Friends", "type": "Panel", "language": "en", "abstract": "During this panel, we\u2019ll discuss the significant role PyLadies chapters around the world have played in advocating for gender representation and leadership and combating biases and the gender pay gap.", "description": "In \u201cHacking Diversity,\u201d we gain valuable insights into the experiences of diversity advocates in hackerspaces over a five-year period. While the book highlights the limitations of these advocates in solving social inequalities, it also emphasizes their potential to effect some social change and stresses the importance of each of us doing our part.\r\n\r\nIt\u2019s important to recognize that technology alone cannot save society, and no software can save the world. The belief that technology is all-powerful can sometimes blind us to the need to help others and elevate their voices.\r\n\r\nDuring this panel, we\u2019ll discuss the significant role PyLadies chapters around the world have played in advocating for gender representation and leadership and combating biases and the gender pay gap.", "recording_license": "", "do_not_record": false, "persons": [], "links": [], "attachments": [], "answers": []}], "B09": [{"id": 26056, "guid": "e53065ef-4012-5a07-924e-7587a034e2c3", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26056-software-design-pattern-for-data-science", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/CBGJNY/", "title": "Software Design Pattern for Data Science", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "Even if every data science work is special, a lot can be learned from similar problems solved in the past. In this talk, I will share some specific software design concepts that data scientists can use to build better data products.", "description": "Data science has evolved from magic models measured by accuracy to software components with an ML core. As such, data scientists\u2019 work should also follow best practices and have a suitable architecture. \r\n\r\nIt is where design patterns can help advance the discipline. A design pattern is a reusable solution to a commonly occurring problem. It is not a concrete piece of code that can be used directly but identifying a pattern help understand the problem and also help build a common language around it. \r\n\r\nIn this talk, I will share some specific software design concepts that data scientists can use to build better data products. I will not focus on patterns that will improve the performance of your model (you can already find a lot about it online) but on the ones that will help you bring your model to production.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25666, "code": "EZMJWT", "public_name": "Theodore Meynard", "biography": "Theodore Meynard is a data scientist at GetYourGuide. He works on our ranking algorithm to help customers to find the best activities to book and locations to explore. He is one of the co-organisers of the Pydata Berlin meetup. When he is not programming, he loves riding his bike looking for the best bakery-patisserie in town.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26252, "guid": "c588816f-0250-5601-bc8d-8aff10b67252", "logo": "", "date": "2023-04-18T11:05:00+02:00", "start": "11:05", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26252--who-is-an-nlp-expert-lessons-learned-from-building-an-in-house-qa-system", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/U7WAQW/", "title": "\u201cWho is an NLP expert?\u201d - Lessons Learned from building an in-house QA-system", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Innovations such as sentence-transformers, neural search and vector databases fueled a very fast development of question-answering systems recently. At scieneers, we wanted to test those components to satisfy our own information needs using a slack-bot that will answer our questions by reading through our internal documents and slack-conversations. We therefore leveraged the HayStack QA-Framework in combination with a Weaviate vector database and many fine-tuned NLP-models.\r\nThis talk will give you insights in both, the technical challenges we faced and the organizational learnings we took.", "description": "Innovations such as sentence-transformers, neural search and vector databases fueled a very fast development of question-answering systems recently. At scieneers, we wanted to test those components to satisfy our own information needs using a slack-bot that will answer our questions by reading through our internal documents and slack-conversations. We therefore leveraged the HayStack QA-Framework in combination with a Weaviate vector database and many fine-tuned NLP-models.\r\nThis talk will give you insights in both, the technical challenges we faced and the organizational learnings we took.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25797, "code": "HWZVSQ", "public_name": "Nico Kreiling", "biography": "Nico is a Data Scientist at scieneers, co-organizer of PyData cologne meetup and host of the Techtiefen podcast. His passions are quick and simple solutions and the constant expansion of his and the communities' knowledge base.", "answers": []}, {"id": 25864, "code": "XDB7AP", "public_name": "Alina Bickel", "biography": "Alina studies Data Science at University of Applied Sciences Karlsruhe and worked on the QA project within her internship semester at scieneers. Her passion lies in the extraction of knowledge using Data Science for the benefit of the public.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26457, "guid": "b5649eb9-efc2-5aa7-b880-3e048e8463e8", "logo": "", "date": "2023-04-18T11:40:00+02:00", "start": "11:40", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26457-observability-for-distributed-computing-with-dask", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/SSTCTS/", "title": "Observability for Distributed Computing with Dask", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "Debugging is hard. Distributed debugging is hell.\r\n\r\nDask is a popular library for parallel and distributed computing in Python. Dask is commonly used in data science, actual science, data engineering, and machine learning to distribute workloads onto clusters of many hundreds of workers with ease.\r\n\r\nHowever, when things go wrong life can become difficult due to all of the moving parts. These parts include your code, other PyData libraries like NumPy/pandas, the machines you\u2019re running on, the network between them, storage, the cloud, and of course issues with Dask itself. It can be difficult to understand what is going on, especially when things seem slower than they should be or fail unexpectedly. Observability is the key to sanity and success.\r\n\r\nIn this talk, we describe the tools Dask offers to help you observe your distributed cluster, analyze performance, and monitor your cluster to react to unexpected changes quickly. We will dive into distributed logging, automated metrics, event-based monitoring, and root-causing problems with diagnostic tooling. Throughout the talk, we will leverage real-world use cases to show how these tools help to identify and solve problems for large-scale users in the wild.\r\n  \r\nThis talk should be particularly insightful for Dask users, but the approaches to observing distributed systems should be relevant to anyone operating at scale in production.", "description": "Debugging is hard. Distributed debugging is hell.\r\n\r\nDask is a popular library for parallel and distributed computing in Python. Dask is commonly used in data science, actual science, data engineering, and machine learning to distribute workloads onto clusters of many hundreds of workers with ease.\r\n\r\nHowever, when things go wrong life can become difficult due to all of the moving parts. These parts include your code, other PyData libraries like NumPy/pandas, the machines you\u2019re running on, the network between them, storage, the cloud, and of course issues with Dask itself. It can be difficult to understand what is going on, especially when things seem slower than they should be or fail unexpectedly. Observability is the key to sanity and success.\r\n\r\nIn this talk, we describe the tools Dask offers to help you observe your distributed cluster, analyze performance, and monitor your cluster to react to unexpected changes quickly. We will dive into distributed logging, automated metrics, event-based monitoring, and root-causing problems with diagnostic tooling. Throughout the talk, we will leverage real-world use cases to show how these tools help to identify and solve problems for large-scale users in the wild.\r\n  \r\nThis talk should be particularly insightful for Dask users, but the approaches to observing distributed systems should be relevant to anyone operating at scale in production.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25930, "code": "SY8ZUX", "public_name": "Hendrik Makait", "biography": "Hendrik Makait is a data and software engineer building systems at the intersection of large-scale data management and machine learning. Currently, he works as an Open Source Engineer at Coiled improving Dask and its distributed execution engine.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26308, "guid": "e99effcf-7e81-5f67-80f3-52eebe4e70d1", "logo": "", "date": "2023-04-18T14:10:00+02:00", "start": "14:10", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26308-you-are-what-you-read-building-a-personal-internet-front-page-with-spacy-and-prodigy", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/NWSLUH/", "title": "You are what you read: Building a personal internet front-page with spaCy and Prodigy", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Sometimes the internet can be a bit overwhelming, so I thought I would make a tool to create a personalized summary of it! In this talk, I'll demonstrate a personal front-page project that allows me to filter info on the internet on a certain topic, built using spaCy, an open-source library for NLP, and Prodigy, a scriptable annotation tool. With this project, I learned about the power of working with tools that provide extensive customizability without sacrificing ease of use. Throughout the talk, I'll also discuss how design concepts of developer tools can improve the development experience when building complex and adaptable software.", "description": "Sometimes the internet can be a bit overwhelming, so I thought I would make a tool to create a personalized summary of it! In this talk, I'll demonstrate an open-source front-page project that allows me to filter info on the internet on a certain topic, customized and adapted to the user's preference.\r\n\r\nWhile building this project, I have been able to further explore the open-source NLP library, spaCy, and the scriptable annotation tool, Prodigy. Part of this talk will discuss how this project was implemented with regard to data collection, annotation and modeling. I developed a custom annotation interface, created a spaCy NLP pipeline, and explored different model architectures. \r\n\r\nThrough the project, I learned about the power of working with tools that offer both good guide-rails and extensive customizability. In this talk, we'll also look at the design concepts of spaCy and Prodigy and how they've enhanced the developer experience for different types of projects, including my personal front-page. I'll discuss what I've discovered about how customizable tooling can improve the developer experience when building complex and adaptable software.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25843, "code": "LJDK8V", "public_name": "Victoria Slocum", "biography": "Victoria is a Developer Advocate at Explosion, where she supports the Natural Language Processing community around the popular open-source library spaCy, the annotation tool Prodigy and other developer tools. Besides running marathons, learning new languages, and building fun machine learning projects about music and food, she loves learning about natural language processing and ensures that the open-source community has everything they need to do the same.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28574, "guid": "6679e5b5-f7f2-5815-b1b9-5739ba8396aa", "logo": "", "date": "2023-04-18T14:45:00+02:00", "start": "14:45", "duration": "00:45", "room": "B09", "slug": "pyconde-pydata-berlin-2023-28574-delivering-ai-at-scale", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/DTBTVF/", "title": "Delivering AI at Scale", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk (long)", "language": "en", "abstract": "Everybody knows our yellow vans, trucks and planes around the world. But do you know how data\r\ndrives our business and how we leverage algorithms and technology in our core operations? We will\r\nshare some \u201cbehind the scenes\u201d insights on Deutsche Post DHL Group\u2019s journey towards a Data-Driven\r\nCompany.\r\n\u2022 Large-Scale Use Cases: Challenging and high impact Use Cases in all major areas of logistics,\r\nincluding Computer Vision and NLP\r\n\u2022 Fancy Algorithms: Deep-Neural Networks, TSP Solvers and the standard toolkit of a Data\r\nScientist\r\n\u2022 Modern Tooling: Cloud Platforms, Kubernetes , Kubeflow, Auto ML\r\n\u2022 No rusty working mode: small, self-organized, agile project teams, combining state of the art\r\nMachine Learning with MLOps best practices\r\n\u2022 A young, motivated and international team \u2013 German skills are only \u201cnice to have\u201d\r\nBut we have more to offer than slides filled with buzzwords. We will demonstrate our passion for our\r\nwork, deep dive into our largest use cases that impact your everyday life and share our approach for a\r\ntimeseries forecasting library - combining data science, software engineering and technology for\r\nefficient and easy to maintain machine learning projects..", "description": "Everybody knows our yellow vans, trucks and planes around the world. But do you know how data\r\ndrives our business and how we leverage algorithms and technology in our core operations? We will\r\nshare some \u201cbehind the scenes\u201d insights on Deutsche Post DHL Group\u2019s journey towards a Data-Driven\r\nCompany.\r\n\u2022 Large-Scale Use Cases: Challenging and high impact Use Cases in all major areas of logistics,\r\nincluding Computer Vision and NLP\r\n\u2022 Fancy Algorithms: Deep-Neural Networks, TSP Solvers and the standard toolkit of a Data\r\nScientist\r\n\u2022 Modern Tooling: Cloud Platforms, Kubernetes , Kubeflow, Auto ML\r\n\u2022 No rusty working mode: small, self-organized, agile project teams, combining state of the art\r\nMachine Learning with MLOps best practices\r\n\u2022 A young, motivated and international team \u2013 German skills are only \u201cnice to have\u201d\r\nBut we have more to offer than slides filled with buzzwords. We will demonstrate our passion for our\r\nwork, deep dive into our largest use cases that impact your everyday life and share our approach for a\r\ntimeseries forecasting library - combining data science, software engineering and technology for\r\nefficient and easy to maintain machine learning projects..", "recording_license": "", "do_not_record": false, "persons": [{"id": 27859, "code": "NCTBKA", "public_name": "Anna Achenbach", "biography": "After pursuing her PhD in Data Science Anna started her work at DPDHL back in 2018. With a background in Logistics from her Bachelor's and Master's studies Data Science at DPDHL combines what she enjoys most: Work with a group of talented\r\nMachine Learning Experts and Analytics enthusiasts and develop projects to embed data (science)-\r\ndriven decision making deeply into our business processes. Besides regular project work Anna focuses on developing trainings for Non-Data Scientists ranging from data literacy to management trainings.", "answers": []}, {"id": 26112, "code": "J7UPJU", "public_name": "Severin Schmitt", "biography": "Severin is a Senior Data Scientist at Deutsche Post DHL Group, leading the forecasting tech team, main\r\ndeveloper of DPDHL\u2019s forecasting library and holds a PhD in mechanical engineering. He is passionate\r\nabout combining Data Science and Software Engineering for long lasting and maintainable machine\r\nlearning projects; he loves guiding the scoping of new projects as well as the change management\r\nprocesses necessary to bring small and big solutions to life; he is curious about timeseries forecasting\r\nand constantly looking for interesting discussions.", "answers": []}, {"id": 27860, "code": "JP87SV", "public_name": "Thorsten Kranz", "biography": "With a background in Physics and Neuroscience Research Thorsten has been working as a Data Scientist\r\nfor many industries. He is driving DPDHL\u2019s efforts of increasing the efficiency for building productionquality, large scale Data Science solutions for the business together with his team. While working as a\r\nManager for many years now he has remained a nerd at heart \u2013 with a passion for data, algorithms and\r\nSoftware Development in Python.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 30687, "guid": "8ad07611-60d3-5b85-a567-1e96216e4b7b", "logo": "", "date": "2023-04-18T16:00:00+02:00", "start": "16:00", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-30687-enabling-machine-learning-how-to-optimize-infrastructure-tools-and-teams-for-ml-workflows", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WZWXLF/", "title": "Enabling Machine Learning: How to Optimize Infrastructure, Tools and Teams for ML Workflows", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "In this talk, we will explore the role of a machine learning enabler engineer in facilitating the development and deployment of machine learning models. We will discuss best practices for optimizing infrastructure and tools to streamline the machine learning workflow, reduce time to deployment, and enable data scientists to extract insights and value from data more efficiently. We will also examine case studies and examples of successful machine learning enabler engineering projects and share practical tips and insights for anyone interested in this field.", "description": "In this talk, we will explore the role of a machine learning enabler engineer in facilitating the development and deployment of machine learning models. We will discuss best practices for optimizing infrastructure and tools to streamline the machine learning workflow, reduce time to deployment, and enable data scientists to extract insights and value from data more efficiently. We will also examine case studies and examples of successful machine learning enabler engineering projects and share practical tips and insights for anyone interested in this field.", "recording_license": "", "do_not_record": false, "persons": [{"id": 29317, "code": "QMJUQA", "public_name": "Yann Lemonnier", "biography": "Yann Lemonnier is an experienced ML/Data Engineer with a strong background in data analysis, machine learning, and time-series predictions. He is currently focusing on enabling production machine learning projects with MLOps. Yann has a Master's degree in Physics from the Universit\u00e9 de Sherbrooke in Canada and is a certified AWS Cloud Practitioner and GCP Professional Data Engineer. Yann is passionate about using his expertise to drive business value and innovation through data-driven insights. In his previous work experiences, he has worked as a Data Engineer, Tech Lead, Machine Learning Engineer, Flight Test Data Analyst and acoustic engineer. Yann has supervised data scientists teams, designed software solutions, and did project screening and reviews. Some of his past projects include L\u2019Or\u00e9al supply chain ETL, tabular data AutoML for Aircraft design offices, Machine activity monitoring from vibration IoT, Aircraft Noise Classification, Aircraft Vibration Event Impact Assessment, Aircraft External Noise Cartography, and vibration flight test analysis. Yann has worked in companies like L\u2019Or\u00e9al, Alteia, COREIoT, and Airbus Flight Test Center. He is now working at Adevinta in the ReCommerce (second hand marketplace) industry.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28573, "guid": "3cba83b3-298c-5c86-b528-7721278e17d8", "logo": "", "date": "2023-04-18T16:35:00+02:00", "start": "16:35", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-28573-use-spark-from-anywhere-a-spark-client-in-python-powered-by-spark-connect", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/UNA9AN/", "title": "Use Spark from anywhere: A Spark client in Python powered by Spark Connect", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "Over the past decade, developers, researchers, and the community have successfully built tens of thousands of data applications using Spark. Since then, use cases and requirements of data applications have evolved: Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\r\n\r\nHowever, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements: there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\r\n\r\nSpark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.\r\n\r\nThis talk highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and give an outlook of how the community can participate in the extension of Spark Connect for new programming languages and frameworks - to bring the power of Spark everywhere.", "description": "Over the past decade, developers, researchers, and the community have successfully built tens of thousands of data applications using Spark. Since then, use cases and requirements of data applications have evolved: Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\r\n\r\n\r\nHowever, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements: there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\r\n\r\n\r\nSpark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.\r\n\r\n\r\nThis talk highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and give an outlook of how the community can participate in the extension of Spark Connect for new programming languages and frameworks - to bring the power of Spark everywhere.", "recording_license": "", "do_not_record": false, "persons": [{"id": 28493, "code": "9LZW8R", "public_name": "Martin Grund", "biography": "Martin Grund is a Senior Staff Software Engineer and Tech Lead at Databricks working at the intersection between query processing, data governance and security. He's currently leading the design and engineering efforts for Spark Connect as part of Apache Spark. Martin has previously led the engineering for Amazon Redshift Spectrum and worked on Cloudera Impala. He holds a PhD in computer science from the Hasso-Plattner-Institute in Germany.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B07-B08": [{"id": 26451, "guid": "d44f53ac-742c-5760-a65d-effed545a53a", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26451-improving-machine-learning-from-human-feedback", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/AUJYP7/", "title": "Improving Machine Learning from Human Feedback", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from \u201cCommon Crawl\u201d and \u201cWeb Text\u201d, among other sources. As the saying goes \u2014 bigger isn\u2019t always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their \u201cinternet-scale biases\u201d along with their \u201cinternet-trained models.\u201d While powerful, these models beg the question \u2014 is unsupervised learning the best future for machine learning?  \r\n\r\nML researchers have developed new model-tuning techniques to address the known biases within existing models and improve their performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the initial training cost. In this talk, we will explore these techniques, known as Reinforcement Learning from Human Feedback (RLHF), and how open-source machine learning tools like PyTorch and Label Studio can be used to tune off-the-shelf models using direct human feedback.", "description": "Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from \u201cCommon Crawl\u201d and \u201cWeb Text\u201d, among other sources. As the saying goes \u2014 bigger isn\u2019t always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their \u201cinternet-scale biases\u201d along with their \u201cinternet-trained models.\u201d While powerful, these models beg the question \u2014 is unsupervised learning the best future for machine learning?\r\n\r\nML researchers have developed new model-tuning techniques to address the known biases within existing models and improve the model\u2019s performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the training cost is very low compared to the initial training cost. This talk will explore these Reinforcement Learning from Human Feedback (RLHF) techniques and how open-source machine learning tools like PyTorch and Label Studio can tune off-the-shelf models using direct human feedback.\r\n\r\nWe\u2019ll start by covering traditional RLHF, in which a model is given a set of prompts to generate outputs. These prompt/output pairs are then graded by human annotators who rank pairs according to a desired metric, which are then used as a reinforcement learning data set to optimize the model to produce results closer to the metric criteria.\r\n\r\nNext, we\u2019ll discuss recent advances within this field and the advantages they provide. One advance we\u2019ll dive into is the use of Human Language Feedback, in which ranks are replaced with human-language summaries that take full advantage of the \u201cfull expressiveness of language that humans use.\u201d  This contextual feedback, along with the original prompt and output of the model, is used to generate a new set of model refinements. The model is then tuned with these refinements to match the new output to the human feedback. In a 2022 study, researchers at NYU reported that \u201cusing only 100 samples of human-written feedback finetunes a GPT-3 model to roughly human-level summarization ability.\u201d It\u2019s advances like these that are providing advantages in terms of accuracy and bias reduction.\r\n\r\nFinally, we\u2019ll leave you with examples and resources on implementing these training methods using publicly available models and open-source tools like PyTorch and Label Studio to help retrain models for targeted applications. As this industry continues to grow, evolve, and develop into more widespread applications, we must approach this space with ethics and sustainability in mind.  By combining the power and expansiveness of these widely-popular \u201cinternet-scale models\u201d with specific, targeted, human approaches, we can avoid the \u201cinternet-scale biases\u201d that threaten the legitimacy and trustworthiness of the industry as a whole.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25924, "code": "LX9JH3", "public_name": "Erin Mikail Staples", "biography": "Erin Mikail Staples is a very online individual passionate about facilitating better connections online and off. She\u2019s forever thinking about how we can communicate, educate and elevate others through collaborative experiences.\r\n\r\nCurrently, Erin is a Senior Developer Community Advocate at Label Studio. At Label Studio \u2014 she empowers the open-source community through education and advocacy efforts.  Outside of her day job, Erin is a comedian, graduate technical advisor, content creator, triathlete, avid reader, and dog parent.\r\n\r\nMost importantly, she believes in the power of being unabashedly \"into things\" and works to help friends, strangers, colleagues, community builders, students, and whoever else might cross her path find their thing.", "answers": []}, {"id": 28310, "code": "KJSUCZ", "public_name": "Nikolai", "biography": "As CTO of Heartex / Label Studio, I specialize in machine learning, data-centric AI, and innovative data labeling techniques. My expertise spans weak supervision, zero-shot and few-shot learning, and reinforcement learning to drive cutting-edge AI solutions.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26466, "guid": "19095341-33fa-542c-840e-01de3e485b37", "logo": "", "date": "2023-04-18T11:05:00+02:00", "start": "11:05", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26466-actionable-machine-learning-in-the-browser-with-pyscript", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/9Q38VT/", "title": "Actionable Machine Learning in the Browser with PyScript", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "PyScript brings the full PyData stack in the browser, opening up to unprecedented use cases for interactive data-intensive applications. In this scenario, the web browser becomes a ubiquitous computing platform, operating within a (nearly) _zero-installation_ & _server-less_ environment.\r\n\r\nIn this talk, we will explore how to create full-fledged interactive front-end machine learning applications using PyScript. We will dive into the the main features of the PyScript platform (e.g. _built-in Javascript integration_ and  _local modules_ ), discussing _new_ data & design patterns (e.g. _loading heterogeneous data in the browser_), required to adapt and to overcome the limitations imposed by the new operating environment (i.e. the browser).", "description": "PyScript is the new open source platform that brings Python to web front-end applications. In fact, PyScript makes it possible to inject *standard* Python code into HTML, which is then _interpreted_ and _executed_ directly in the browser. And all that, with **no server-side** technology needed, and **no installation** required (_not even a local Python interpreter!, ed._) \ud83d\udd2e.\r\n\r\nBut there's more! Thanks to its built-in integration with [`pyodide`](https://pyodide.org/en/stable/), PyScript brings the [full](https://pyodide.org/en/stable/usage/packages-in-pyodide.html) PyData stack into the browser, along with a native integration with the Javascript interpreter, then enabling full support for front-end interactivity. \r\n\r\nAs a result, PyScript has the potential to radically change the way in which interactive data-driven web apps could be designed and developed: the seamless bi-directional integration of **Python** and **Javascript** is complemented by the full support to reliable numerical computation, enabled by the Python scientific ecosystem (e.g. `numpy` `scikit-learn`), using the browser as a ubiquitous virtual machine.\r\n\r\nIn this talk, we will explore how PyScript enables the creation of full-fledged font-end _interactive machine learning_ (`ML`) apps using PyScript. Multiple examples of supervised and unsupervised ML apps will be presented, and analysed in details, in order to fully understand how PyScript works, and what key features are provided (e.g. _built-in Javascript integration_;  _local modules_ ). Similarly, we will also discuss new_ data & design patterns (e.g. _loading heterogeneous data in the browser_; _multi-core vs multi-threading; _performance  considerations_) which are required to adapt to the new _atypical_ environment in which we operate: the **browser**. \r\n\r\nNo specific prior knowledge is required to attend the talk. Familiarity with Python programming, and the main `pydata` packages (i.e. `numpy`, `scikit-learn`, `Matplotlib` ) is desirable, along with a general understanding of how the web DOM works (for the Javascript interaction part) and basic principles of data processing.\r\n**Domain** knowledge: _Novice_; **Python** knowledge: _Intermediate_", "recording_license": "", "do_not_record": false, "persons": [{"id": 24927, "code": "GHGDNR", "public_name": "Valerio Maggio", "biography": "Valerio Maggio is a Data scientist, a Developer Advocate at Anaconda. Valerio is well versed into open science and research software, advocating the use of best software development practice in Data Science. He is member of the Software Sustainability Institute ([profile](https://www.software.ac.uk/about/fellows/valerio-maggio)) where he has been awarded a fellowing to develop focusing on Privacy-Preserving Machine learning technologies. Valerio is an active member of the Python community. Over the last twelve years Valerio has contributed and volunteered to the organisation of many international conferences and local meet-ups like PyCon Italy, EuroPython, EuroSciPy and PyData. All his talks, workshop materials and open source contributions are publicly available on his [Speaker Deck](https://speakerdeck.com/leriomaggio) and [GitHub](https://github.com/leriomaggio) profile pages.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28577, "guid": "7c691abd-24b6-58d9-a96e-7593d7708c13", "logo": "", "date": "2023-04-18T11:40:00+02:00", "start": "11:40", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-28577-how-python-enables-future-computer-chips", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/VRGANP/", "title": "How Python enables future computer chips", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "At the semiconductor division of Carl Zeiss it's our mission to continuously make computer chips faster and more energy efficient. To do so, we go to the very limits of what is possible, both physically and technologically. This is only possible through massive research and development efforts.\r\n\r\nIn this talk, we tell the story how Python became a central tool for our R&D activities. This includes technical aspects as well as organization and culture. How do you make sure that hundreds of people work in consistent environments? \u2013 How do you get all people on board to work together with Python? \u2013 You have lots of domain experts without much software background. How do you prevent them from creating a mess when projects get larger?", "description": "At the semiconductor division of Carl Zeiss it's our mission to continuously make computer chips faster and more energy efficient. To do so, we go to the very limits of what is possible, both physically and technologically. This is only possible through massive research and development efforts.\r\n\r\nIn this talk, we tell the story how Python became a central tool for our R&D activities. This includes technical aspects as well as organization and culture. How do you make sure that hundreds of people work in consistent environments? \u2013 How do you get all people on board to work together with Python? \u2013 You have lots of domain experts without much software background. How do you prevent them from creating a mess when projects get larger?", "recording_license": "", "do_not_record": false, "persons": [{"id": 20316, "code": "X8YQJR", "public_name": "Tim Hoffmann", "biography": "Tim Hoffmann is a physicist and software expert passionate to bring science and high-quality software together. He works as Simulation Architect Digital Twin at Carl Zeiss, where he covers all aspects from coding, architecture, training up to software strategy. Tim is an active contributor in the Python open source community. In particular, he is core developer and API lead for the visualization library matplotlib.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26509, "guid": "f58ad865-386b-52be-ba52-246133cb6ae7", "logo": "", "date": "2023-04-18T14:10:00+02:00", "start": "14:10", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26509-data-driven-design-for-the-dask-scheduler", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/Q9GVEK/", "title": "Data-driven design for the Dask scheduler", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Historically, changes in the scheduling algorithm of Dask have often been based on theory, single use cases, or even gut feeling. Coiled has now moved to using hard, comprehensive performance metrics for all changes - and it's been a turning point!", "description": "Any developer worth their salt scrupulously practices functional regression testing: all functionality is covered by automated tests, and every time anybody changes something all tests must remain green.\r\nPerformance testing however is a much fuzzier and often neglected area, typically due to the fact that, frequently, in order to measure realistic performance you need a production-sized test bench, and that performance typically includes some degree of variance.\r\n\r\nHistorically, changes to the scheduling algorithm in Dask have gone through this thought process. There have always been plenty of functional unit tests that verify that the scheduler does whatever minute decisions the developers expects, but until recently there weren't any end-to-end, production-sized test benches on realistic use cases to measure performance.\r\n\r\nAt Coiled, we have now implemented a new test suite that does just that - statistical analysis of performance metrics - that lets us understand if a change is beneficial or detrimental in terms of runtime and memory usage.\r\n\r\nThis presentation delves into how we collect data, visualize it, and act on it and how much it changed our development process for the better.", "recording_license": "", "do_not_record": false, "persons": [{"id": 16006, "code": "P8HMJU", "public_name": "Guido Imperiale", "biography": "I come from a 12 years career in orchestrating Monte Carlo simulations for finance, sized at 1500+ CPU hours each. For the last two years I've been an OSS engineer at Coiled, building up the foundations of the Dask library.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26495, "guid": "7cd9685a-44c0-51ed-9dc4-8d9abf6a2743", "logo": "", "date": "2023-04-18T14:45:00+02:00", "start": "14:45", "duration": "00:45", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26495-visualizing-your-computer-vision-data-is-not-a-luxury-it-s-a-necessity-without-it-your-models-are-blind-and-so-do-you-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/X89787/", "title": "Visualizing your computer vision data is not a luxury, it's a necessity: without it, your models are blind and so do you.", "subtitle": "", "track": "PyData: Computer Vision", "type": "Talk (long)", "language": "en", "abstract": "Are you ready to take your Computer Vision projects to the next level? Then don't miss this talk!\r\n\r\nData visualization is a crucial ingredient for the success of any computer vision project.\r\nIt allows you to assess the quality of your data, grasp the intricacies of your project, and communicate effectively with stakeholders.\r\n\r\nIn this talk, we'll showcase the power of data visualization with compelling examples. You'll learn about the benefits of data visualization and discover practical methods and tools to elevate your projects.\r\n\r\nDon't let this opportunity pass you by: join us and learn how to make data visualization a core feature of your Computer Vision projects.", "description": "This talk is suitable for computer vision professionals and enthusiasts who want to learn about best practices for visualizing and exploring datasets and how to apply them to their projects. It will provide a valuable foundation for building better machine learning models and producing high-quality results. Data scientists from other domains may also find eye-opening information and ideas.\r\n\r\nWe will explore examples of data issues in various computer vision datasets and tasks, such as object detection, few-shot learning, and visual question answering. We will then examine tools and strategies for inspecting datasets and the results of models, including FiftyOne, KnowYourData, and Streamlit. By the end of the talk, attendees will have a deeper understanding of the importance of visualizing and exploring computer vision datasets and be equipped with the knowledge and skills to apply these techniques in their own projects", "recording_license": "", "do_not_record": false, "persons": [{"id": 4179, "code": "93YQC8", "public_name": "Chazareix Arnault", "biography": "Arnault Chazareix is a data scientist and engineering manager who has worked in the field of data science for more than 5 years. He has specialized in computer vision and natural language processing projects, and is passionate about working with unstructured data such as text and images. In addition to his professional work, Arnault is also passionate about Brazilian jiu-jitsu.\r\n\r\nFor the past 5 years, Arnault has been working at Sicara, a French start-up that helps customers build custom data solutions using data engineering and data science. Prior to working at Sicara, Arnault worked for Feedly, a Silicon Valley start-up that develops a news aggregation and curation software as a service (SaaS).", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26400, "guid": "c2c00d98-06ca-5ae9-8540-abb669305b5b", "logo": "", "date": "2023-04-18T16:00:00+02:00", "start": "16:00", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26400-introducing-fastkafka", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/HLMGHX/", "title": "Introducing FastKafka", "subtitle": "", "track": "PyCon: Libraries", "type": "Talk", "language": "en", "abstract": "FastKafka is a Python library that makes it easy to connect to Apache Kafka queues and send and receive messages. In this talk, we will introduce the library and its features for working with Kafka queues in Python. We will discuss the motivations for creating the library, how it compares to other Kafka client libraries, and how to use its decorators to define functions for consuming and producing messages. We will also demonstrate how to use these functions to build a simple application that sends and receives messages from the queue. This talk will be of interest to Python developers looking for an easy-to-use solution for working with Kafka.\r\n\r\nThe documentation of the library can be found here: https://fastkafka.airt.ai/", "description": "FastKafka is a Python library that simplifies the process of connecting to Apache Kafka queues and sending and receiving messages. It follows a decorator-based approach inspired by the popular FastAPI library, making it easy to define functions for consuming messages from the queue and producing and sending new ones.\r\n\r\nIn this talk, we will introduce FastKafka and its features for working with Kafka in Python. We will start by discussing the motivations for creating the library and how it compares to other Kafka client libraries. We will then delve into a live demonstration of the library's features, showing how to use the decorators to define functions for consuming and producing messages, and how to use these functions to build a simple application that sends and receives messages from the queue.\r\n\r\nFinally, we will discuss some real-world use cases for FastKafka and how it can be used to build scalable, high-performance applications that need to process and transmit large amounts of data. This talk will be of particular interest to Python developers looking for an easy-to-use solution for working with Kafka.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25886, "code": "QFCQHX", "public_name": "Tvrtko Sternak", "biography": "I am currently working as a Python developer at airt. In the past three years, I have gained valuable experience in the industry, including a year working on a microservice product that uses Apache Kafka for communication between services.\r\n\r\nI am a strong believer in the power of open source software, and I enjoy learning from the open source community, hopefully also contributing more this year :). My interests in the field at the moment include machine learning, model deployment, Apache Kafka, and advanced Python programming.\r\n\r\nIn my free time, I enjoy reading fantasy books, staying active through biking and hitting the gym, and watching comedy-drama TV shows. I am always looking for new ways to expand my knowledge and skills, and I am excited to continue growing as a developer in the years ahead.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26314, "guid": "e6d37f4b-decb-5596-8a33-6870d774765e", "logo": "", "date": "2023-04-18T16:35:00+02:00", "start": "16:35", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26314-neo4j-graph-databases-for-climate-policy", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/YTHXML/", "title": "Neo4j graph databases for climate policy", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "In this talk we walkthrough our experience using Neo4j and Python to model climate policy as a graph database. We discuss how we did it, some of the challenges we faced, and what we learnt along the way!", "description": "As the ambition and complexity of climate regulations and policies grows, it is becoming increasingly difficult to represent them in relational databases. For example the EU Sustainable Taxonomy regulation contains thousands of interrelated legal clauses, many of which also reference other legal texts and entities. \r\n\r\nGraph databases such as Neo4j present a possible alternative well suited to model the complicated, interrelated and evolving structure of climate regulations. \r\n\r\nIn this talk we walkthrough our experience using Neo4j and Python to model climate policy such as the EU Sustainable Taxonomy as a graph database. We discuss how we did it, some of the challenges we faced, and what we learnt along the way!", "recording_license": "", "do_not_record": false, "persons": [{"id": 25846, "code": "79NUUS", "public_name": "Marcus Tedesco", "biography": "Tech Lead at Briink, accelerating sustainable finance with machine learning!\r\n\r\nPreviously Senior Software Engineer at Babbel and Senior Software Engineer and Cloud Architect on the Emerging Technology team at Accenture.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B05-B06": [{"id": 26221, "guid": "310fff3c-3c55-53ee-b63e-07b4bd4512d1", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26221-the-state-of-production-machine-learning-in-2023", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MLAGKM/", "title": "The State of Production Machine Learning in 2023", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "As the number of production machine learning use-cases increase, we find ourselves facing new and bigger challenges where more is at stake. Because of this, it's critical to identify the key areas to focus our efforts, so we can ensure our machine learning pipelines are reliable and scalable. In this talk we dive into the state of production machine learning in the Python Ecosystem, and we will cover the concepts that make production machine learning so challenging, as well as some of the recommended tools available to tackle these challenges.\r\n\r\nThis talk will cover key principles, patterns and frameworks around the open source frameworks powering single or multiple phases of the end-to-end ML lifecycle, incluing model training, deploying, monitoring, etc. We will be covering a high level overview of the production ML ecosystem and dive into best practices that have been abstracted from production use-cases of machine learning operations at scale, as well as how to leverage tools to that will allow us to deploy, explain, secure, monitor and scale production machine learning systems.", "description": "As the number of production machine learning use-cases increase, we find ourselves facing new and bigger challenges where more is at stake. Because of this, it's critical to identify the key areas to focus our efforts, so we can ensure our machine learning pipelines are reliable and scalable. In this talk we dive into the state of production machine learning in the Python Ecosystem, and we will cover the concepts that make production machine learning so challenging, as well as some of the recommended tools available to tackle these challenges.\r\n\r\nThis talk will cover key principles, patterns and frameworks around the open source frameworks powering single or multiple phases of the end-to-end ML lifecycle, incluing model training, deploying, monitoring, etc. We will be covering a high level overview of the production ML ecosystem and dive into best practices that have been abstracted from production use-cases of machine learning operations at scale, as well as how to leverage tools to that will allow us to deploy, explain, secure, monitor and scale production machine learning systems.\r\n\r\nThis talk will be relevant for any keen python practitioners or seasoned ML practitioners interested to get an updated overview of the state of the production ML ecosystem in the current year, covering a broad range of sub-fields in the space.\r\n\r\nThis talk will benefit the Python ecosystem by providing cross-functional knowledge, bringing together best practices from data scientists, software engineers and DevOps engineers to tackle the challenge of machine learning at scale. During this talk we will shed light into some of the more popular and up-and-coming libraries to watch in this space, and we will provide a conceptual and practical hands on deep dive which will allow the community to both, tackle this issues and help further the discussion.", "recording_license": "", "do_not_record": false, "persons": [{"id": 18023, "code": "EQMGKH", "public_name": "Alejandro Saucedo", "biography": "Alejandro is the Director of Engineering & Applied Science at Zalando where he leads a cross-functional technology organisation consisting of department heads, managers, principals and ICs across engineering and data science, and is responsible for the development of a large portfolio of (10+) products, the management of one of Zalando's large-scale central data platforms, and the productionisation of SOtA machine learning systems powering high-value & critical use-cases across the organisation. Alejandro is also the Chief Scientist at the Institute for Ethical AI & Machine Learning, where he contributes to policy and industry standards on the responsible design, development and operation of AI, and has led policy contributions including the EU's AI Regulatory Proposal, the Data Act, between others. With over 10 years of software development experience, Alejandro has held technical leadership positions across hyper-growth scale-ups and tech giants, with a strong track record of building cross-functional R&D and Product organisations. He is currently appointed as governing council Member-at-Large at the Association for Computing Machinery (ACM), and is currently the Chairperson of the ML Security Committee at the Linux Foundation.\r\n\r\nLinkedin: https://linkedin.com/in/axsaucedo\r\nTwitter: https://twitter.com/axsaucedo\r\nGithub: https://github.com/axsaucedo\r\nWebsite: https://ethical.institute/", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26412, "guid": "9c16ce63-2c17-5f1c-8eec-e443fc631772", "logo": "", "date": "2023-04-18T11:05:00+02:00", "start": "11:05", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26412-how-chatbots-work-we-need-to-talk-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/AKXXQD/", "title": "How Chatbots work \u2013 We need to talk!", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Chatbots are fun to use, ranging from simple chit-chat (\u201cHow are you today?\u201d) to more sophisticated use cases like shopping assistants, or the diagnosis of technical or medical problems. Despite their mostly simple user interaction, chatbots must combine various complex NLP concepts to deliver convincing, intelligent, or even witty results. \r\n\r\nWith the advancing development of machine learning models and the availability of open source frameworks and libraries, chatbots are becoming more powerful every day and at the same time easier to implement. Yet, depending on the concrete use case, the implementation must be approached in specific ways. In the design process of chatbots it is crucial to define the language processing tasks thoroughly and to choose from a variety of techniques wisely. \r\n\r\nIn this talk, we will look together at common concepts and techniques in modern chatbot implementation as well as practical experiences from an E-mobility bot that was developed using the Rasa framework.", "description": "Chatbots are fun to use, ranging from simple chit-chat (\u201cHow are you today?\u201d) to more sophisticated use cases like shopping assistants, or the diagnosis of technical or medical problems. Despite their mostly simple user interaction, chatbots must combine various complex NLP concepts to deliver convincing, intelligent, or even witty results. \r\n\r\nWith the advancing development of machine learning models and the availability of open source frameworks and libraries, chatbots are becoming more powerful every day and at the same time easier to implement. Yet, depending on the concrete use case, the implementation must be approached in specific ways. In the design process of chatbots it is crucial to define the language processing tasks thoroughly and to choose from a variety of techniques wisely. \r\n\r\nIn this talk, we will look together at common concepts and techniques in modern chatbot implementation as well as practical experiences from an E-mobility bot that was developed using the Rasa framework.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25899, "code": "DLCVJF", "public_name": "Yuqiong Weng", "biography": "Yuqiong recently received her master's degree in data science. She is now working as a junior data scientist at I-WUNDER GmbH, where she deals with data and develops machine learning models. NLP is one of the fields that catches her interest, out of which she developed a chatbot in the domain of E-Mobility to help with information-retrieval tasks.", "answers": []}, {"id": 25906, "code": "UTZYZE", "public_name": "Katrin Reininger", "biography": "With a background in physics, Katrin discovered her enthusiasm for data analysis while exploring laser-molecule interactions during her PhD. Hence, she left science to become a consultant at I-WUNDER. Since 2020 she mainly manages projects in the area of data science and software development. Besides, Katrin has a passion for exploring new (project) management techniques and running workshops.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26310, "guid": "59b26e30-92f6-5f5b-957f-e49c5e4391d4", "logo": "", "date": "2023-04-18T11:40:00+02:00", "start": "11:40", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26310-using-transformers-a-drama-in-512-tokens", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/SJCEFG/", "title": "Using transformers \u2013 a drama in 512 tokens", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "\u201cGot an NLP problem nowadays? Use transformers! Just download a pretrained model from the hub!\u201d - every blog article ever\r\n\r\nAs if it\u2019s that easy, because nearly all pretrained models have a very annoying limitation: they can only process short input sequences. Not every NLP practitioner happens to work on tweets, but instead many of us have to deal with longer input sequences. What started as a minor design choice for BERT, got cemented by the research community over the years and now turns out to be my biggest headache: the 512 tokens limit.\r\n\r\nIn this talk, we\u2019ll ask a lot of dumb questions and get an equal number of unsatisfying answers:\r\n\r\n1. How much text actually fits into 512 tokens? Spoiler: not enough to solve my use case, and I bet a lot of your use cases, too.\r\n\r\n2. I can feed a sequence of any length into an RNN, why do transformers even have a limit? We\u2019ll look into the architecture in more detail to understand that.\r\n\r\n3. Somebody smart must have thought about this sequence length issue before, or not? Prepare yourself for a rant about benchmarks in NLP research.\r\n\r\n4. So what can we do to handle longer input sequences? Enjoy my collection of mediocre workarounds.", "description": "\u201cGot an NLP problem nowadays? Use transformers! Just download a pretrained model from the hub!\u201d - every blog article ever\r\n\r\nAs if it\u2019s that easy, because nearly all pretrained models have a very annoying limitation: they can only process short input sequences. Not every NLP practitioner happens to work on tweets, but instead many of us have to deal with longer input sequences. What started as a minor design choice for BERT, got cemented by the research community over the years and now turns out to be my biggest headache: the 512 tokens limit.\r\n\r\nIn this talk, we\u2019ll ask a lot of dumb questions and get an equal number of unsatisfying answers:\r\n\r\n1. How much text actually fits into 512 tokens? Spoiler: not enough to solve my use case, and I bet a lot of your use cases, too.\r\n\r\n2. I can feed a sequence of any length into an RNN, why do transformers even have a limit? We\u2019ll look into the architecture in more detail to understand that.\r\n\r\n3. Somebody smart must have thought about this sequence length issue before, or not? Prepare yourself for a rant about benchmarks in NLP research.\r\n\r\n4. So what can we do to handle longer input sequences? Enjoy my collection of mediocre workarounds.", "recording_license": "", "do_not_record": false, "persons": [{"id": 1994, "code": "N3BPWK", "public_name": "Marianne Stecklina", "biography": "As a deep learning engineer at omni:us, I'm working on different NLP topics related to document understanding.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26469, "guid": "7fd07624-429f-5220-8d8f-ec72bbc4f2e6", "logo": "", "date": "2023-04-18T14:10:00+02:00", "start": "14:10", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26469-pragmatic-ways-of-using-rust-in-your-data-project", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MQHTHY/", "title": "Pragmatic ways of using Rust in your data project", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "Writing efficient data pipelines in Python can be tricky. The standard recommendation is to use vectorized functions implemented in Numpy, Pandas, or the like. However, what to do, when the processing task does not fit these libraries? Using plain Python for processing can result in lacking performance, in particular when handling large data sets. \r\n\r\nRust is a modern, performance-oriented programming language that is already widely used by the Python community. Augmenting data processing steps with Rust can result in substantial speed ups. In this talk will present strategies of using Rust in a larger Python data processing pipeline with a particular focus on pragmatism and minimizing integration efforts.", "description": "One common strategy is to wrap the Rust part as a Python extension module. With enough care, the extensions module can have a pythonic feel and substantially improve performance. While libraries, such as PyO3, offer streamlined APIs, this task can still require lot of work. \r\n\r\nAn often simpler alternative is to package the Rust part as an executable and communicate via files or network. This talk will focus on JSON messages exchanged via stdin / stdout or dataframe-like data in Arrow-compatible files. JSON is broadly supported in both Python and Rust and serialization can easily be handled with libraries such as SerDe (Rust) or cattrs (Python). The Arrow in-memory format supports complex data types, such as structs, lists, maps, or unions. These files can then be efficiently processed in Python by ever an growing list of libraries, most prominently pandas and polars.\r\n\r\nI will discuss the different strategies using real-world use cases and offer tips on how to implement them. Finally I will end by summarizing the respective strengths and weaknesses of the approaches.", "recording_license": "", "do_not_record": false, "persons": [{"id": 215, "code": "DZWGQ3", "public_name": "Christopher Prohm", "biography": "Christopher is a data scientist and long-time Python user. Recently he started\r\nusing Rust for data projects and became interested in how to combine both\r\nlanguages.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26052, "guid": "a3f2099f-6e7d-5382-9d66-a17a31046343", "logo": "", "date": "2023-04-18T14:45:00+02:00", "start": "14:45", "duration": "00:45", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26052-writing-plugin-friendly-python-applications", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/RDQH3W/", "title": "Writing Plugin Friendly Python Applications", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk (long)", "language": "en", "abstract": "In modern software engineering, plugin systems are a ubiquitous way to extend and modify the behavior of applications and libraries. When software is written in a way that is plugin friendly, it encourages the use of modular organization where the contracts between the core software and the plugin have been well thought out. In this talk, we cover exactly how to define this contract and how you can start designing your software to be more plugin friendly.\r\n\r\nThroughout the talk we will be creating our own plugin friendly application using the [pluggy](https://pluggy.readthedocs.io/en/stable/) library to show these design principles in action. At the end of the talk, I also cover a real-life case study of how the package manager [conda](https://github.com/conda/conda) is currently making its 10 year old code more plugin friendly to illustrate how to retrofit an existing project.", "description": "This talk begins with a general discussion about what plugins are and how they are used in software. We cover important theoretical concepts and show just how pervasive plugins are in much of the software we use everyday.\r\n\r\nWith a firm idea about what plugins allow us to do, we will begin creating our own command line application that downloads images via APIs given a search term. We will write our application with plugins in mind so that we can quickly expand and support any number of image searching backends (e.g. Google, Unsplash, etc.). The presentation will focus on everything we have to do to let plugin authors extend our application and add their own backends.\r\n\r\nA fully functional implementation of this application can be found here: [https://github.com/travishathaway/latz](https://github.com/travishathaway/latz).\r\n\r\nAfter building our own application, I will then present how the [conda](https://github.com/conda/conda) project approaches making its software plugin friendly. Much of what I show in the example also applies to conda's plugin architecture.\r\n\r\nThis talk should prepare those interested in writing their own plugin friendly applications to get started with the [pluggy](https://pluggy.readthedocs.io/en/stable/) library. The [example project](https://github.com/travishathaway/latz) will also provide a great starting point and inspiration for new and existing applications.", "recording_license": "", "do_not_record": false, "persons": [{"id": 16366, "code": "HA3SES", "public_name": "Travis Hathaway", "biography": "I have been a practicing software engineer for just over 10 years now. I've done a lot of work in the past building and maintaining web applications but now develop CLI tools for the conda project. My interest in plugins was largely motivated by the work I've done at Anaconda for the conda project.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26176, "guid": "dc2daa70-9b07-53d4-8013-76bf3e5344f2", "logo": "", "date": "2023-04-18T16:00:00+02:00", "start": "16:00", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26176-the-bumps-in-the-road-a-retrospective-on-my-data-visualisation-mistakes", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/7FTL7H/", "title": "The bumps in the road: A retrospective on my data visualisation mistakes", "subtitle": "", "track": "PyData: Visualisation", "type": "Talk", "language": "en", "abstract": "We will delve into the importance of effective data visualisation in today's world. We will explore how it can help convey insights from data using Matplotlib and best practices for creating informative visualisations. We will also discuss the limitations of static visualisations and examine the role of continuous integration in streamlining the process and avoiding common pitfalls. By the end of this talk, you will have gained valuable insights and techniques for creating informative and accurate data visualisations, no matter what tools you're using.", "description": "In today's world, effective visualisation is crucial for conveying insights from data. We will explore best practices for creating visualisations with Matplotlib. We will discuss the limitations of static visualisations and how continuous integration can help streamline the process and avoid common pitfalls.\r\n\r\nI will share my practical experiences and learned lessons from working with analytics drawing on the insights of well-known experts such as Edward Tufte, Stephen Few, Alberto Cairo, and Dona Wong. The work of these authors has helped shape our understanding of how to create informative and accurate visualisations. I will reflect on what I wish I had known about the best practices in this field. \r\n\r\nThis talk is suitable for professionals who work with data and want to improve the effectiveness of analytics and reporting. Data visualisation is a form of communication that is important to learn how to apply to convey the stories that data tells us. By the end of this talk, you will have gained valuable techniques for creating informative analytics and an understanding of how CI can support your data visualisation projects.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25704, "code": "3W79QG", "public_name": "Artem Kislovskiy", "biography": "I am a software engineer based in Switzerland with a passion for data visualisation. This passion ignited as a student when I worked on various Computational Fluid Dynamics projects. After a few years of focusing on experimental physics in academia, I am now enjoying the opportunity to apply my skills in a real-world setting by building business analytics in my daily job.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 30255, "guid": "97b661ff-d94f-5e46-aa19-843908e2a1dd", "logo": "", "date": "2023-04-18T17:30:00+02:00", "start": "17:30", "duration": "01:00", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-30255-pyladies-workshop", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/7SVZR3/", "title": "PyLadies Workshop", "subtitle": "", "track": "General: Others", "type": "Panel", "language": "en", "abstract": "A workshop for PyLadies members with the Berlin Tech Workers Council discussing the legal frameworks on contracts and termination agreements, as well as how employees can defend themselves in situations where they are made redundant due to mass layoffs.", "description": "The gender pay gap, which measures the percentage difference in average gross hourly earnings between men and women, amounted to 18% in Germany, as of 2021. As in previous years, this was considerably higher than the EU27 average of 13%. Another study also point out that male tech workers in Germany earn almost 15,000\u20ac more per year than their female counterparts. Women, BIPOC, and LGBTQIA workers also face various kinds of discrimination at work, barriers to opaque promotions to vague performance reviews.  However, it can be difficult to advocate for oneself or challenge these incidents when one is uncertain of the laws surrounding pay, benefits, and other issues in the workplace. This uncertainty is exacerbated by the current climate in tech. Companies have imposed hiring freeze and have laid off hundreds, if not thousands, of workers. \r\n\r\nThis workshop for PyLadies members with Berlin Tech Workers Coalition, grassroots organization that empowers tech workers to build collective power and get involved in campaigns that make a positive impact on our society, aims to empower workers in tech of their rights in the workplace. The workshop will discuss what to expect from contracts and how to navigate termination agreements, two common situations where workers can \"sign away\" their rights.", "recording_license": "", "do_not_record": false, "persons": [], "links": [], "attachments": [], "answers": []}], "A1": [{"id": 26267, "guid": "f390b943-457a-5cbf-85c7-018ba08a4ffd", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26267-bayesian-marketing-science-solving-marketing-s-3-biggest-problems", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/AXMS87/", "title": "Bayesian Marketing Science: Solving Marketing's 3 Biggest Problems", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "In this talk I will present two new open-source packages that make up a powerful and state-of-the-art marketing analytics toolbox. Specifically, PyMC-Marketing is a new library built on top of the popular Bayesian modeling library PyMC. PyMC-Marketing allows robust estimation of customer acquisition costs (via media mix modeling) as well as customer lifetime value. \r\nIn addition, I will show how we can estimate the effectiveness of marketing campaigns using a new Bayesian causal inference package called CausalPy. The talk will be applied with a real-world case-study and many code examples. Special emphasis will be placed on the interplay between these tools and how they can be combined together to make optimal marketing budget decisions in complex scenarios.", "description": "Marketing data science attempts to answer three main questions:\r\n1. How much does it cost to acquire a customer on a given channel?\r\n2. How much do I earn from an acquired customer over their lifetime?\r\n3. What is the causal impact of my marketing campaigns?\r\n\r\nWhile seemingly straight-forward, robust estimation of these quantities on noisy, non-stationary and highly structured data is quite tricky. Moreover, while these questions are intimately related, they are often answered separately. \r\n\r\nIn this talk I will present two new open-source packages that make up a powerful and state-of-the-art marketing analytics toolbox. Specifically, PyMC-Marketing is a new library built on top of the popular Bayesian modeling library PyMC. PyMC-Marketing allows robust estimation of customer acquisition costs (via media mix modeling) as well as customer lifetime value. \r\nIn addition, I will show how we can estimate the effectiveness of marketing campaigns using a new Bayesian causal inference package called CausalPy. The talk will be applied with a real-world case-study and many code examples. Special emphasis will be placed on the interplay between these tools and how they can be combined.\r\n\r\nTogether, these tools demonstrated provide a powerful open-source suite to solve today's biggest marketing analytics challenges.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25814, "code": "CZDJYP", "public_name": "Dr. Thomas Wiecki", "biography": "CEO and founder of PyMC Labs - the Bayesian consultancy that solves your most challenging data science problems. Co-author of PyMC, the industry standard Bayesian modeling library for Python.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26289, "guid": "4039819f-50d6-5e01-b98e-2b96b8540717", "logo": "", "date": "2023-04-18T11:05:00+02:00", "start": "11:05", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26289-ble-and-python-how-to-build-a-simple-ble-project-on-linux-with-python", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WLMDZ7/", "title": "BLE and Python: How to build a simple BLE project on Linux with Python", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Bluetooth Low Energy (BLE) is a part of the Bluetooth standard aimed at bringing wireless technology to low-power devices, and it's getting into everything - lightbulbs, robots, personal health and fitness devices, and plenty more. One of the main advantages of BLE is that everybody can integrate those devices into their tools or projects.\r\n\r\nHowever, BLE is not the most developer-friendly protocol and these devices most of the time don't come with good documentation. In addition, there are not a lot of good open-source tools, examples, and tutorials on how to use Python with BLE. Especially if one wants to build both sides of the communication. \r\n\r\nIn this talk, I will introduce the concepts and properties used in BLE interactions and look at how we can use the Linux Bluetooth Stack (Bluez) to communicate with other devices. We will look at a simple example and learn along the way about common pitfalls and debugging options while working with BLE and Python.\r\n\r\nThis talk is for everybody that has a basic understanding of Python and wants to have a deeper understanding of how BLE works and how one could use it in a private project.", "description": "Slides can be found here: https://drive.google.com/file/d/1rDkSKriobmW71ZMYU6pqdx7Yal1eUgXm/view?usp=sharing\r\n\r\nThe problem that this talk is addressing is the difficulty of using Bluetooth Low Energy (BLE) with Python, particularly for those who are new to the protocol. One issue is that BLE is not necessarily beginner-friendly, with a steep learning curve that can be intimidating for those who are just starting out. Additionally, there are not many examples available for creating a BLE server using Python, which makes it difficult for people to learn and understand the process. This is most likely due to the fact that writing a BLE (GATT) server is often only done in professional contexts. Finally, complexity is added as one has to interact with the system Bluetooth stack which makes it more complicated, particularly on Linux where the use of DBus is required. Overall, these challenges can make it difficult for people to effectively use BLE and Python together. \r\n\r\nThe problem of using BLE with Python is relevant to the audience because BLE is a widely-used technology that allows users to add a variety of peripherals to their projects, both personal and professional. Over the years more devices support a configuration or use through BLE. For example, BLE is often used in home automation systems, wearable devices, and Internet of Things (IoT) applications. By understanding how to use BLE with Python, the audience can take advantage of the many possibilities that this technology offers and create innovative projects that leverage the capabilities of many different types of BLE devices. \r\n\r\nIn this talk, I will introduce the different technologies that are involved in using BLE with Python, including BLE itself, Bluez (the Linux Bluetooth stack), and DBus (a software system for inter-process communication). This is followed by a showcase of a simple GATT server example using Python, which will demonstrate how to use these technologies effectively. In addition to this, I will explain a possible development process for creating BLE projects with Python, including debugging tools and common pitfalls to avoid. Finally, I will point the audience toward further resources that they can use to continue learning about BLE and Python and to help them get started with their own projects.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25112, "code": "LCXWWK", "public_name": "Bruno Vollmer", "biography": "I am the CTO of biped.ai, the AI Copilot for blind and visually impaired people that leverage advanced computer vision to guide them. \r\n\r\nDuring my Masters at RWTH Aachen I worked at several Start-Ups as a software engineer. I've gained experience in Computer Vision and Machine Learning as well as general software engineering areas.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26119, "guid": "5a5f3586-c134-56ff-8f68-5d3525d66382", "logo": "", "date": "2023-04-18T11:40:00+02:00", "start": "11:40", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26119-maps-with-django", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/KYLLZA/", "title": "Maps with Django", "subtitle": "", "track": "PyCon: Django", "type": "Talk", "language": "en", "abstract": "Keeping in mind the **Pythonic** principle that _\u201csimple is better than complex\u201d_ we'll see how to create a web **map** with the **Python** based _web framework_ **Django** using its **GeoDjango** module, storing _geographic data_ in your _local database_ on which to run _geospatial queries_.", "description": "A *map* in a website is the best way to make geographic data easily accessible to users because it represents, in a simple way, the information relating to a specific geographical area and is in fact used by many online services.\r\n\r\nImplementing a web *map* can be complex and many adopt the strategy of using external services, but in most cases this strategy turns out to be a major data and cost management problem.\r\n\r\nIn this talk we'll see how to create a web *map* with the **Python** based web framework **Django** using its **GeoDjango** module, storing geographic data in your local database on which to run geospatial queries.\r\n\r\nThrough this intervention you can learn how to add a *map* on your website, starting from a simple *map* based on **Spatialite/SQLite** up to a more complex and interactive *map* based on **PostGIS/PostgreSQL**.", "recording_license": "", "do_not_record": false, "persons": [{"id": 11475, "code": "BGLPFA", "public_name": "Paolo Melchiorre", "biography": "I\u2019m Paolo Melchiorre, a longtime *Python* backend developer who contributes to the *Django* project and gives talks at tech *conferences*.\r\n\r\nI\u2019ve been a *GNU/Linux* user for over 20 years and I use and promote *Free Software*.\r\n\r\nI graduated in Software Engineering and I\u2019m an alumnus of the University of Bologna, Italy.\r\n\r\nI\u2019ve been working in the web for 15 years and now I\u2019m the CTO of 20tab, a pythonic software company, for which I work remotely.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26163, "guid": "b069b50d-9311-524f-8160-d48a63b2fb74", "logo": "", "date": "2023-04-18T14:10:00+02:00", "start": "14:10", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26163-methods-for-text-style-transfer-text-detoxification-case", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/UECWHD/", "title": "Methods for Text Style Transfer: Text Detoxification Case", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Global access to the Internet has enabled the spread of information throughout the world and has offered many new possibilities. On the other hand, alongside the advantages, the exponential and uncontrolled growth of user-generated content on the Internet has also facilitated the spread of toxicity and hate speech. Much work has been done in the direction of offensive speech detection. However, there is another more proactive way to fight toxic speech -- how a suggestion for a user as a detoxified version of the message. In this presentation, we will provide an overview how texts detoxification task can be solved. The proposed approaches can be reused for any text style transfer task for both monolingual and multilingual use-cases.", "description": "Firstly, we will shortly introduce the research direction of NLP for Social Good. Then, we will show the main direction of research in text style transfer field. This field suffers from the lack of parallel data. We will describe our approach for such parallel dataset collection and show that it can be applied for any language. Then, we will show how monolingual, multilingual, and cross-lingual models can be trained for texts detoxification. In the end, we will discuss ethical issues connected with this task and tackling of toxic and hate speech in general. The whole presented work is based on the peer-reviewed papers from ACL and EMNLP conferences.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25740, "code": "937CJZ", "public_name": "Daryna Dementieva", "biography": "I am a postdoctoral researcher at TUM. Currenlty, I am involved into the project of eXplainable AI. In 2022, I obtained my PhD under the supervision of Pr. Alexander Panchenko, Skoltech. My PhD research was connected with such important sociological issues as Fake News Detection and Texts Detoxification. More broadly, I am super interested in the NLP for Social Good research direction. Besides academical experience, I also was involved in several industrial projects in different companies: Visiology, Moscow, Russian Federation; Beiersdorf, Hamburg, Germany. Now, obtained industrial experience helps me a lot in my research.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26358, "guid": "e8f8ca82-1a83-5448-a298-77d2030bba39", "logo": "", "date": "2023-04-18T14:45:00+02:00", "start": "14:45", "duration": "00:45", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26358-accelerating-public-consultations-with-large-language-models-a-case-study-from-the-uk-planning-inspectorate", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/HMGCPL/", "title": "Accelerating Public Consultations with Large Language Models: A Case Study from the UK Planning Inspectorate", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk (long)", "language": "en", "abstract": "Local Planning Authorities (LPAs) in the UK rely on written representations from the community to inform their Local Plans which outline development needs for their area. With an average of 2000 representations per consultation and 4 rounds of consultation per Local Plan, the volume of information can be overwhelming for both LPAs and the Planning Inspectorate tasked with examining the legality and soundness of plans. In this study, we investigate the potential for Large Language Models (LLMs) to streamline representation analysis.\r\n\r\nWe find that LLMs have the potential to significantly reduce the time and effort required to analyse representations, with simulations on historical Local Plans projecting a reduction in processing time by over 30%, and experiments showing classification accuracy of up to 90%. \r\n\r\nIn this presentation, we discuss our experimental process which used a distributed experimentation environment with Jupyter Lab and cloud resources to evaluate the performance of the BERT, RoBERTa, DistilBERT, and XLNet models. We also discuss the design and prototyping of web applications to support the aided processing of representations using Voil\u00e0, FastAPI, and React. Finally, we highlight successes and challenges encountered and suggest areas for future improvement.", "description": "In the United Kingdom, Local Planning Authorities (LPAs) are responsible for creating Local Plans that outline the development needs of their areas, including land allocation, infrastructure requirements, housing needs, and environmental protection measures. This process involves consulting with the local community and interested parties multiple times, which often results in hundreds or thousands of written representations that must be organised and analysed. On average, LPAs receive approx. 2000 written representations per consultation, and each Local Plan requires 4 rounds of consultation. The process of analysing these representations takes approx. 3.5 months per round of consultation to complete.\r\n\r\nThe Planning Inspectorate is tasked with examining Local Plans to ensure they follow national policy and legislation. The Inspectorate examines approx. 60 Local Plans a year, each examination lasting around a year\u2019s time. The volume of information included in each Local Plan significantly outweighs the capacity of the Planning Inspectorate to read and analyse the content in detail. This can lead to important issues being overlooked and potential problems with the review process or legal challenges. Conducting a thorough and meticulous analysis of representations takes a lot of time and effort for both LPAs and the Planning Inspectorate.\r\n\r\nTogether with the Planning Inspectorate, we conducted an AI discovery to explore how Large Language Models (LLMs) can help reduce the time taken to analyze representations, improve resource planning, increase consistency in decision-making, and mitigate the risk of a key issue of material concern being missed.\r\n\r\nWe assessed the performance of competing models and demonstrated their goodness with proof-of-concept apps for both LPAs and the Planning Inspectorate that unify and streamline the aided processing of representations. Our simulations on historical Local Plans resulted in a projected reduction of the time taken to analyze representations by more than 30%, and experiments show that we are able to classify representations to the relevant policy in Local Plans with up to 90% accuracy.\r\n\r\nIn this talk, we share our experimental process based on Python and the experimental results. We delve into how we approached the problem, sourced and cleaned the data, and used a distributed experimentation environment with Jupyter Lab and cloud resources to evaluate the performance of BERT, RoBERTa, DistilBERT, and XLNet models. We also discuss our strategies for dealing with limited training data. Finally, we present the design and prototyping of two web applications using Voil\u00e0, and demonstrate how we iterated on them using FastAPI and React. Throughout the presentation, we highlight the successes and challenges we encountered, and suggest areas for future improvement.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25866, "code": "8S77NV", "public_name": "Michele Dallachiesa", "biography": "Michele is a freelance data scientist based in Munich. He implemented solutions for Contact Center Forecasting, Marketing Attribution, Out-Of-Home Advertising, Natural Language Processing, Forecasting and Classification Models, Robots Autonomous Charging, Urban Traffic Optimisation, and other AI services for the governments of the United Kingdom and Hong Kong, and private clients including Google, NASA, Stanford University, Huawei, Taxfix, Wayfair, Telef\u00f3nica, and others. He holds a Ph.D. in computer science earned for his research with the University of Trento, the IBM T.J. Watson Research Centre, and the Qatar Computing Research Institute on querying, mining, and storing uncertain data, with a particular interest in data series. He co-authored ten papers in top-tier publications on data management, including SIGMOD, VLDB, EDBT, KAIS, and DKE.", "answers": []}, {"id": 25880, "code": "QYFFVQ", "public_name": "Andreas Leed", "biography": "As Head of Data Science at Oxford Global Projects, I have dedicated my career to improving project planning and decision making through the use of data-driven methods. In my role, I lead technical projects involving advanced techniques like natural language processing and machine learning, and am responsible for managing a database of project performance data from over 17,000 projects across all industries. This data is used to inform future projects and improve our understanding of project performance. In addition to my work at Oxford Global Projects, I serve as an external examiner for quantitative methods and data science courses at universities in Denmark.\r\n\r\nMy passion is to apply data science to the field of project management and help our clients achieve their objectives. I am constantly seeking new and innovative ways to do so and am excited to continue pushing the boundaries of what is possible. I have a strong track record of success, including leading external risk analysis on some of Europe's largest capital projects, contributing to project appraisal methodology for the UK Department for Transport, and presenting statistical analysis and results to senior management and high-level figures globally. I have also led work on a diverse range of projects, including the feasibility assessment of the first road between settlements in Greenland, the risk modeling of large scale nuclear new builds and decommissioning programs, and the development of an AI-based Early Warning System for the Development Bureau in Hong Kong. My expertise in data-driven project planning and risk analysis, as well as my ability to effectively communicate technical information to diverse audiences, have been key to my achievements in this field.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26057, "guid": "6de90b29-09dd-5666-831f-e9bf6c60b052", "logo": "", "date": "2023-04-18T16:00:00+02:00", "start": "16:00", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26057-mlops-in-practice-our-journey-from-batch-to-real-time-inference", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/QLCNN9/", "title": "MLOps in practice: our journey from batch to real-time inference", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "I will present the challenges we encountered while migrating an ML model from batch to real-time predictions and how we handled them. In particular, I will focus on the design decisions and open-source tools we built to test the code, data and models as part of the CI/CD pipeline and enable us to ship fast with confidence.", "description": "At GetYourGuide we build a marketplace for travel experiences. The ranking of activities on the platform is one of the most essential machine-learning products for the business.\r\n\r\nIn this talk, I will explain how we gradually migrated our ranking from global precomputed scores to a live reranking service. Building such a service with high availability requirements and constant modifications brings challenges. I will dive into the design decisions and open-source tools we built to enable us to test code, data, and models as part of the CI/CD pipeline. It allows us to ship fast with confidence without losing ourselves in cumbersome tests and/or a mocking hell.\r\n\r\nAt the end of the talk, you will have actionable insights you can apply to your Machine Learning products and understand how to introduce good MLOps practices using open-source tools.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25666, "code": "EZMJWT", "public_name": "Theodore Meynard", "biography": "Theodore Meynard is a data scientist at GetYourGuide. He works on our ranking algorithm to help customers to find the best activities to book and locations to explore. He is one of the co-organisers of the Pydata Berlin meetup. When he is not programming, he loves riding his bike looking for the best bakery-patisserie in town.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26440, "guid": "bf9bff8b-96dc-531c-8493-0054e6e8ad17", "logo": "", "date": "2023-04-18T16:35:00+02:00", "start": "16:35", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26440-ask-a-question-an-faq-answering-service-for-when-there-s-little-to-no-data", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/PEQZTC/", "title": "Ask-A-Question: an FAQ-answering service for when there's little to no data", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "Doing data science in international development often means finding the right-sized solution in resource-constrained settings. \r\n\r\nThis talk walks you through how my team helped answer thousands of questions from pregnant folks and new parents on a South African maternal and child health helpline, which model we ended up choosing and why (hint: resource-constraints!), and how we've packaged everything into a service that anyone can start for themselves, \r\n\r\nBy the end of the talk, I hope you'll know how to start your own FAQ-answering service and learn about one example of doing data science in international development.", "description": "Doing data science in international development often means finding the right-sized solution in resource-constrained settings. \r\n\r\nThis talk walks you through how my team helped answer thousands of questions from pregnant folks and new parents on a South African maternal and child health helpline, which model we ended up choosing and why (hint: resource-constraints!), and how we've packaged everything into a service that anyone can start for themselves, \r\n\r\nBy the end of the talk, I hope you'll know how to start your own FAQ-answering service and learn about one example of doing data science in international development.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25926, "code": "LYTFDM", "public_name": "Suzin You", "biography": "Suzin You is a Data Scientist based in New Delhi, India, working at IDinsight, an international development advisory. As do her colleagues, she strives to keep impact as her north star at work.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A03-A04": [{"id": 26333, "guid": "ad2a4451-fbfe-5a17-a3c9-365efd9f9299", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-26333-geospatial-data-processing-with-python-a-comprehensive-tutorial", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/RPMMKZ/", "title": "Geospatial Data Processing with Python: A Comprehensive Tutorial", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Tutorial", "language": "en", "abstract": "In this tutorial, you will learn about the various Python modules for processing geospatial data, including GDAL, Rasterio, Pyproj, Shapely, Folium, Fiona, OSMnx, Libpysal, Geopandas, Pydeck, Whitebox, ESDA, and Leaflet. You will gain hands-on experience working with real-world geospatial data and learn how to perform tasks such as reading and writing spatial data, reprojecting data, performing spatial analyses, and creating interactive maps. This tutorial is suitable for beginners as well as intermediate Python users who want to expand their knowledge in the field of geospatial data processing", "description": "Geospatial data, which refers to data that has a geographic component, is a crucial part of many fields, including geography, geography, urban planning, and environmental science. In this tutorial, you will learn about the various Python modules that are available for working with geospatial data.\r\n\r\nWe will start by introducing the **GDAL** (Geospatial Data Abstraction Library) and **Rasterio** modules, which are used for reading and writing raster data (data stored in a grid of cells, where each cell has a value). You will learn how to read and write common raster formats such as GeoTIFF and ESRI ASCII, as well as how to perform common raster operations such as resampling and reprojecting.\r\n\r\nNext, we will cover the **Pyproj** module, which is used for performing coordinate system transformations. You will learn how to convert between different coordinate systems and how to perform common tasks such as converting latitude and longitude coordinates to UTM (Universal Transverse Mercator) coordinates.\r\n\r\nAfter that, we will introduce the **Shapely** module, which is used for working with geometric objects in Python. You will learn how to create and manipulate points, lines, and polygons, as well as how to perform spatial operations such as intersection and union.\r\n\r\nThen, we will cover the **Folium** module, which is used for creating interactive maps in Python. You will learn how to create simple maps, add markers and popups, and customize the appearance of your maps.\r\n\r\nNext, we will introduce the **Fiona** module, which is used for reading and writing vector data (data stored as individual features, each with its own geometry and attributes). You will learn how to read and write common vector formats such as ESRI Shapefile and GeoJSON, as well as how to access and manipulate the attributes of vector features.\r\n\r\nAfter that, we will cover the **OSMnx** module, which is used for working with OpenStreetMap data in Python. You will learn how to download and manipulate street networks, buildings, and other geospatial data from OpenStreetMap.\r\n\r\nNext, we will introduce the **Libpysal** module, which is used for performing spatial statistics and econometrics in Python. You will learn how to calculate spatial weights, perform spatial autocorrelation tests, and estimate spatial econometric models.\r\n\r\nThen, we will cover the **Geopandas** module, which is used for working with geospatial data in a Pandas DataFrame. You will learn how to load and manipulate vector data, perform spatial joins, and create choropleth maps.\r\n\r\nAfter that, we will introduce the **Pydeck** module, which is used for creating interactive 3D maps in Python. You will learn how to create 3D point clouds, 3D building models, and other 3D geospatial visualizations.\r\n\r\nNext, we will cover the **Whitebox** module, which is a powerful open-source GIS toolkit for performing geospatial data processing and analysis. You will learn how to use Whitebox to perform tasks such as raster reclassification, terrain analysis, and hydrological modeling.\r\n\r\nFinally, we will introduce the **ESDA** (Exploratory Spatial Data Analysis) and **LeafMap** modules, which are used for exploring and visualizing spatial patterns and relationships in data. You will learn how to calculate spatial statistics such as Moran's I and local spatial autocorrelation statistics, and how to create interactive choropleth maps.\r\n\r\nBy the end of this tutorial, you will have a solid understanding of the various Python modules that are available for working with geospatial data and will have hands-on experience applying these tools to real-world data. This tutorial is suitable for beginners as well as intermediate Python users who want to expand their knowledge in the field of geospatial data processing.", "recording_license": "", "do_not_record": false, "persons": [{"id": 14, "code": "DKNKQU", "public_name": "Martin Christen", "biography": "Martin Christen is a professor of Geoinformatics and Computer Graphics at the Institute of Geomatics at the University of Applied Sciences Northwestern Switzerland (FHNW). His main research interests are geospatial Virtual- and Augmented Reality, 3D geoinformation, Deep Learning, and interactive 3D maps. \r\nMartin Christen is very active in the Python community. He teaches various Python-related courses and uses Python in most research projects. He organizes the PyBasel meet up - the local Python User Group Northwestern Switzerland. He also organizes the yearly GeoPython conference. He is a board member of the Python Software Verband e.V.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25880, "guid": "91c56db3-ab69-5b7b-9ece-dc87b9cdb8bd", "logo": "", "date": "2023-04-18T14:05:00+02:00", "start": "14:05", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-25880-let-s-contribute-to-pandas-3-hours-1", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/KUKU9Z/", "title": "Let's contribute to pandas (3 hours) #1", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Tutorial", "language": "en", "abstract": "PyData Berlin are excited to bring you this open source workshop dedicated to contributing to pandas. This tutorial is 3 hours. We will have a break and continue with the same group of people.\r\n\r\npandas is a data wrangling platform for Python widely adopted in the scientific computing community. In this session, you will be guided on how you can make your own contributions to the project, no prior experience contributing required! Not only will this teach you new skills and boost your CV, you'll also likely get a nice adrenaline rush when your contribution is accepted!\r\n\r\nIf you don\u2019t finish your contribution during the event, we hope you will continue to work on it after the tutorial. pandas offers regular new contributor meetings and has a slack space to provide ongoing support for new contributors. For more details, see our contributor community page: http://pandas.pydata.org/docs/dev/development/community.html .", "description": "PyData Berlin are excited to bring you this open source workshop dedicated to contributing to pandas. This tutorial is 3 hours. We will have a break and continue with the same group of people.\r\n\r\npandas is a data wrangling platform for Python widely adopted in the scientific computing community. In this session, you will be guided on how you can make your own contributions to the project, no prior experience contributing required! Not only will this teach you new skills and boost your CV, you'll also likely get a nice adrenaline rush when your contribution is accepted!\r\n\r\nIf you don\u2019t finish your contribution during the event, we hope you will continue to work on it after the workshop. pandas offers regular new contributor meetings and has a slack space to provide ongoing support for new contributors. For more details, see our contributor community page: http://pandas.pydata.org/docs/dev/development/community.html .\r\n\u2753Any other requirements \u2753\r\n1. Bring your own laptop\r\n2. Have Github account: https://github.com\r\n3. Have git installed: https://git-scm.com/book/en/v2/Getting-Started-Installing-Gitf\r\n\u2022 Format for the session:\r\nFirst 15 minutes : an introduction - what you can contribute, how to contribute, and how to set up your development environment or use gitpod;\r\nThe rest : \"office hours\", during which you'll be mentored through setting up a development environment and making a contribution to pandas.\r\n\u2022 Preparation (optional)\r\nFor those who are more keen on using the workshop to work on their contribution to pandas, you may want to start setting up your development environment in advance. This w\u200b\u200bay, by the time you arrive you are ready to get started on picking issues, and starting to contribute.\r\nPlease be aware that it could take longer to set up a development on \u200b\u200ba computer running a Windows operating system compared to MacOS or Unix. We will guide you through the steps, and they are useful to learn for many open source projects.\r\nWe also offer a development environment on gitpod. It can take some minutes to load, but provides you an instant and fresh development environment for each new task directly from your browser, using VScode. Documentation is in the works and will be provided before the workshop.\r\nTo get the most out of the session, it's encouraged (but not required) that you have a look at the contributing guide beforehand: https://pandas.pydata.org/pandas-docs/dev/development/contributing.html. Particularly, the development environment instructions: https://pandas.pydata.org/docs/dev/development/contributing_environment.html\r\n\u2022 Audience level\r\nEveryone is welcome to attend this session! If you've never contributed to open source software before, then you will learn how to, and if you have experience contributing, then you can either help mentor other attendees or you can work on more challenging contributions. It is useful to have some pandas, git, and python and experience. If you don't have much experience with them, you might expect to spend time \"learning by doing\".", "recording_license": "", "do_not_record": false, "persons": [{"id": 15468, "code": "AHQBCP", "public_name": "Noa Tamir", "biography": "Noa have been involved with the R and PyData communities for some time, with a focus on community building and DEI. They are a NumFOCUS member of the Board of Directors and DISC committee, PyLadies Organizer, and chaired the PyData Berlin 2022 conference. In addition, they are a Lead Data Science Coach at neue fische, contributing to pandas, and are currently developing the Contributor Experience Community and Handbook with Inessa Pawson and Melissa Mendon\u00e7a.", "answers": []}, {"id": 25677, "code": "DDZLYH", "public_name": "Patrick Hoefler", "biography": "I am a member of the pandas core team since early 2021. I am a regular contributor of pandas since early 2020. I am currently working at Coiled as a Senior Software Engineer. I hold a Masters degree in Mathematics and I am currently studying towards a Software Engineering degree.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28250, "guid": "456c9701-abc8-5d35-8799-6af5031fd5f9", "logo": "", "date": "2023-04-18T15:45:00+02:00", "start": "15:45", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-28250-let-s-contribute-to-pandas-3-hours-2", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/YWTRSG/", "title": "Let's contribute to pandas (3 hours) #2", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Tutorial", "language": "en", "abstract": "PyData Berlin are excited to bring you this open source workshop dedicated to contributing to pandas. This tutorial is 3 hours. We will have a break and continue with the same group of people.\r\n\r\npandas is a data wrangling platform for Python widely adopted in the scientific computing community. In this session, you will be guided on how you can make your own contributions to the project, no prior experience contributing required! Not only will this teach you new skills and boost your CV, you'll also likely get a nice adrenaline rush when your contribution is accepted!\r\n\r\nIf you don\u2019t finish your contribution during the event, we hope you will continue to work on it after the tutorial. pandas offers regular new contributor meetings and has a slack space to provide ongoing support for new contributors. For more details, see our contributor community page: http://pandas.pydata.org/docs/dev/development/community.html .", "description": "PyData Berlin are excited to bring you this open source workshop dedicated to contributing to pandas. This tutorial is 3 hours. We will have a break and continue with the same group of people.\r\n\r\npandas is a data wrangling platform for Python widely adopted in the scientific computing community. In this session, you will be guided on how you can make your own contributions to the project, no prior experience contributing required! Not only will this teach you new skills and boost your CV, you'll also likely get a nice adrenaline rush when your contribution is accepted!\r\n\r\nIf you don\u2019t finish your contribution during the event, we hope you will continue to work on it after the workshop. pandas offers regular new contributor meetings and has a slack space to provide ongoing support for new contributors. For more details, see our contributor community page: http://pandas.pydata.org/docs/dev/development/community.html .\r\n\u2753Any other requirements \u2753\r\n1. Bring your own laptop\r\n2. Have Github account: https://github.com\r\n3. Have git installed: https://git-scm.com/book/en/v2/Getting-Started-Installing-Gitf\r\n\u2022 Format for the session:\r\nFirst 15 minutes : an introduction - what you can contribute, how to contribute, and how to set up your development environment or use gitpod;\r\nThe rest : \"office hours\", during which you'll be mentored through setting up a development environment and making a contribution to pandas.\r\n\u2022 Preparation (optional)\r\nFor those who are more keen on using the workshop to work on their contribution to pandas, you may want to start setting up your development environment in advance. This w\u200b\u200bay, by the time you arrive you are ready to get started on picking issues, and starting to contribute.\r\nPlease be aware that it could take longer to set up a development on \u200b\u200ba computer running a Windows operating system compared to MacOS or Unix. We will guide you through the steps, and they are useful to learn for many open source projects.\r\nWe also offer a development environment on gitpod. It can take some minutes to load, but provides you an instant and fresh development environment for each new task directly from your browser, using VScode. Documentation is in the works and will be provided before the workshop.\r\nTo get the most out of the session, it's encouraged (but not required) that you have a look at the contributing guide beforehand: https://pandas.pydata.org/pandas-docs/dev/development/contributing.html. Particularly, the development environment instructions: https://pandas.pydata.org/docs/dev/development/contributing_environment.html\r\n\u2022 Audience level\r\nEveryone is welcome to attend this session! If you've never contributed to open source software before, then you will learn how to, and if you have experience contributing, then you can either help mentor other attendees or you can work on more challenging contributions. It is useful to have some pandas, git, and python and experience. If you don't have much experience with them, you might expect to spend time \"learning by doing\".", "recording_license": "", "do_not_record": false, "persons": [{"id": 15468, "code": "AHQBCP", "public_name": "Noa Tamir", "biography": "Noa have been involved with the R and PyData communities for some time, with a focus on community building and DEI. They are a NumFOCUS member of the Board of Directors and DISC committee, PyLadies Organizer, and chaired the PyData Berlin 2022 conference. In addition, they are a Lead Data Science Coach at neue fische, contributing to pandas, and are currently developing the Contributor Experience Community and Handbook with Inessa Pawson and Melissa Mendon\u00e7a.", "answers": []}, {"id": 25677, "code": "DDZLYH", "public_name": "Patrick Hoefler", "biography": "I am a member of the pandas core team since early 2021. I am a regular contributor of pandas since early 2020. I am currently working at Coiled as a Senior Software Engineer. I hold a Masters degree in Mathematics and I am currently studying towards a Software Engineering degree.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A05-A06": [{"id": 26216, "guid": "f0d462c1-8074-5de4-9b46-6d1bac2c0b81", "logo": "", "date": "2023-04-18T10:30:00+02:00", "start": "10:30", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26216-aspect-oriented-programming-diving-deep-into-decorators", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/PVJMWB/", "title": "Aspect-oriented Programming - Diving deep into Decorators", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Tutorial", "language": "en", "abstract": "The aspect-oriented programming paradigm can support the separation of\r\ncross-cutting concerns such as logging, caching, or checking of permissions.\r\nThis can improve code modularity and maintainability.\r\nPython offers decorator to implement re-usable code for cross-cutting task.\r\n\r\nThis tutorial is an in-depth introduction to decorators.\r\nIt covers the usage of decorators and how to implement simple and more advanced\r\ndecorators.\r\nUse cases demonstrate how to work with decorators.\r\nIn addition to showing how functions can use closures to create decorators,\r\nthe tutorial introduces callable class instance as alternative.\r\nClass decorators can solve problems that use be to be tasks for metaclasses.\r\nThe tutorial provides uses cases for class decorators.\r\n\r\nWhile the focus is on best practices and practical applications, the tutorial\r\nalso provides deeper insight into how Python works behind the scene.\r\nAfter the tutorial participants will feel comfortable with functions that take\r\nfunctions and return new functions.", "description": "## Audience\r\n\r\nThis tutorial is for intermediate Python programmers who want to dive deeper.\r\nSolid working knowledge of functions and classes basics is required.\r\n\r\n## Format\r\n\r\nThe tutorial will be hands on.\r\nI will start with a blank Notebook for each topic and develop the content\r\nstep-by-step.\r\nThe participants are encouraged to type along.\r\nMy typing speed is usually appropriate and allows participants to follow.\r\nThe students will receive a comprehensive PDF with all course content as well\r\nPython source code files for all use cases and large code blocks I use.\r\nI will load these files in my Notebook.\r\nThe students can do the same or open the files in their preferred editor or IDE.\r\n\r\nI also explicitly ask for feedback if I am too fast or things are unclear.\r\nI encourage questions at any time.\r\nIn fact, questions and my answers are often an important part of my teaching,\r\nmaking the learning experience much more lively and typically more useful.\r\n\r\nSo the participants will be active throughout the whole tutorial.\r\nThere will be two exercises that each participant has to do on its own\r\n(or in breakout rooms if the tutorials should be remote) during the tutorial.\r\nWe will look at the solutions during the tutorial.\r\nI also supply a solutions PDF after the tutorial.\r\n\r\n\r\n## Outline\r\n\r\n* Examples of using decorators\r\n  * from the standard library\r\n  * from third-party packages\r\n* Closures for decorators \r\n* Write a simple decorator\r\n* Best Practice \r\n* Use case: Caching\r\n* Use case: Logging\r\n* Parameterizing decorators\r\n* Chaining decorators\r\n* Callable instances instead of functions\r\n* Use case: Argument Checking\r\n* Use case: Registration\r\n* Class decorators \r\n* Wrap-up and questions", "recording_license": "", "do_not_record": false, "persons": [{"id": 4, "code": "9KSJ3K", "public_name": "Mike M\u00fcller", "biography": "I've been a Python user since 1999, teaching Python professionally since 2004.\r\nI am also active in the community organizing Python conferences such as\r\nPyCon DE, EuroSciPy, and BarCamps.\r\nI am a PSF Fellow and chair of the German Python Software Verband.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28243, "guid": "f82a336e-50a5-59c5-9478-ade3a342dc81", "logo": "", "date": "2023-04-18T14:05:00+02:00", "start": "14:05", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-28243-data-kata-ensemble-programming-with-pydantic-1", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/VFZ3VT/", "title": "Data Kata: Ensemble programming with Pydantic #1", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Tutorial", "language": "en", "abstract": "Write code as an ensemble to solve a data validation problem with Pydantic. Working together is not just about code - learn how to listen to colleagues, make typos in front of everyone, become a supportive team member, defend your ideas and maybe even accept criticism.", "description": "The How\r\n\r\nWe will play a \"collaborative game\" - write code together to solve a problem. Each small group of 5, an \"ensemble\", will be guided by a facilitator. An ensemble has only one screen and one keyboard, so participants rotate the roles of typing and talking. \r\n\r\nThe goals are to have fun, learn how to use Pydantic, write better code with Test Driven Development, listen to colleagues, make typos in front of everyone, become a supportive team, defend our ideas and sometimes even accept criticism.\r\n\r\nExercise:\r\n\"Read data from a CSV and check data types, range of values, consistency between columns using Pydantic.\"\r\nSee data and starting code in  the [repo](https://github.com/tmylk/data-kata/tree/main/validation/pydantic)\r\n\r\nThis is part 1 out of 2 of our data validation tutorial. Part 2 is doing the same task using a different Python framework - Pandera instead of Pydantic. You can attend both or just one part of this tutorial.\r\n\r\nFormat:\r\n- Ensemble programming with a facilitator. We will all collaborate as one team, switching the person on the keyboard every 5 mins. \r\n- You don't need to have any previous experience with ensemble programming to join.\r\n- You don't need to have any previous experience with data validation to join.\r\n\r\nSchedule:\r\n- Intros - 10 mins\r\n- Ensemble programming - 30 mins\r\n- Interim Retrospective - 10 mins\r\n- Ensemble programming - 30 mins\r\n- Final Retrospective - 10 mins\r\n- Closing\r\n\r\nThings to note:\r\n- We will use gitpod.io as a shared VS Code IDE work environment", "recording_license": "", "do_not_record": false, "persons": [{"id": 24692, "code": "8BJ7S9", "public_name": "Lev Konstantinovskiy", "biography": "Lev Konstantinovskiy is an experienced data science and software engineering team lead. Long time ago he used to maintain a python Natural Language Processing library gensim.", "answers": []}, {"id": 25829, "code": "NEVTJW", "public_name": "Nitsan Avni", "biography": null, "answers": []}, {"id": 25822, "code": "C9BHFV", "public_name": "Gregor Riegler", "biography": "I\u2019m a Software Development Coach and Crafter on a lifelong journey to learn better ways to develop quality software. I like to practice with friends and teach and share what I learned. My goal is to help people find joy in their work and become better at developing software.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28242, "guid": "4bc85970-0742-5adb-b144-1bb3c711fc1d", "logo": "", "date": "2023-04-18T15:45:00+02:00", "start": "15:45", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-28242-data-kata-ensemble-programming-with-pydantic-2", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/DEQM3J/", "title": "Data Kata: Ensemble programming with Pydantic #2", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Tutorial", "language": "en", "abstract": "Write code as an ensemble to solve a data validation problem using Pydantic. Working together is not just about code - learn how to listen to colleagues, make typos in front of everyone, become a supportive team member, defend your ideas and maybe even accept criticism.", "description": "The How\r\n\r\nWe will play a \"collaborative game\" - write code together to solve a problem. Each small group of 5, an \"ensemble\", will be guided by a facilitator. An ensemble has only one screen and one keyboard, so participants rotate the roles of typing and talking. \r\n\r\nThe goals are to have fun, learn how to use Pydantic, write better code with Test Driven Development, listen to colleagues, make typos in front of everyone, become a supportive team, defend our ideas and sometimes even accept criticism.\r\n\r\nExercise:\r\n\"Read data from a CSV and check data types, range of values, consistency between columns using Pandera.\"\r\nSee data and starting code in  the [repo](https://github.com/tmylk/data-kata/tree/main/validation/pydantic)\r\n\r\nThis is part 2 of our data validation tutorial. Part 1 is doing the same task using a different Python framework - Pydantic. You can attend both or just one part of this tutorial.\r\n\r\nFormat:\r\n- Ensemble programming with a facilitator. We will all collaborate as one team, switching the person on the keyboard every 5 mins. \r\n- You don't need to have any previous experience with ensemble programming to join.\r\n- You don't need to have any previous experience with data validation to join.\r\n\r\nSchedule:\r\n- Intros - 10 mins\r\n- Ensemble programming - 30 mins\r\n- Interim Retrospective - 10 mins\r\n- Ensemble programming - 30 mins\r\n- Final Retrospective - 10 mins\r\n- Closing\r\n\r\nThings to note:\r\n- We will use gitpod.io as a shared VS Code IDE work environment", "recording_license": "", "do_not_record": false, "persons": [{"id": 24692, "code": "8BJ7S9", "public_name": "Lev Konstantinovskiy", "biography": "Lev Konstantinovskiy is an experienced data science and software engineering team lead. Long time ago he used to maintain a python Natural Language Processing library gensim.", "answers": []}, {"id": 25822, "code": "C9BHFV", "public_name": "Gregor Riegler", "biography": "I\u2019m a Software Development Coach and Crafter on a lifelong journey to learn better ways to develop quality software. I like to practice with friends and teach and share what I learned. My goal is to help people find joy in their work and become better at developing software.", "answers": []}, {"id": 25829, "code": "NEVTJW", "public_name": "Nitsan Avni", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}]}}, {"index": 3, "date": "2023-04-19", "day_start": "2023-04-19T04:00:00+02:00", "day_end": "2023-04-20T03:59:00+02:00", "rooms": {"Kuppelsaal": [{"id": 28268, "guid": "976c7461-89d4-52ee-a554-dd8845f43546", "logo": "", "date": "2023-04-19T09:10:00+02:00", "start": "09:10", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-28268-keynote-lorem-ipsum-dolor-sit-amet", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/HJ9J7Z/", "title": "Keynote - Lorem ipsum dolor sit amet", "subtitle": "", "track": "Plenary", "type": "Keynote", "language": "en", "abstract": "A life without joy is like software without meaningful test data - it's uncertain and unreliable. The search for the perfect test data is a challenge. Real data should not be too real. Random data should not be too random. This is a randomly real and a really random journey to discover the balance between these two, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "description": "A life without joy is like software without meaningful test data - it's uncertain and unreliable. The search for the perfect test data is a challenge. Real data should not be too real. Random data should not be too random. This is a randomly real and a really random journey to discover the balance between these two, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "recording_license": "", "do_not_record": false, "persons": [{"id": 7, "code": "SAXR3H", "public_name": "Miroslav \u0160ediv\u00fd", "biography": "Miro is using Python to make the sun shine and the wind blow.\r\n\r\nBorn in Czechoslovakia, he has lived in France, Germany, and Austria and therefore understands a bunch of European languages. With 20+ years of Linux experience and coding with Python since version 2.5, Miro has dedicated most of his career building complex systems using open-source software and libraries to forecast, process, and analyze power generation and distribution data.\r\n\r\nMiro is also a big fan of OpenStreetMap and helps fill in the gaps on the world map. As a reviewer of the books \u201cModern Vim\u201d and \u201cFluent Python (2nd ed.)\u201d, he loves sharing his expertise on the best tools for daily tasks. In 2021, Miro was named a Fellow of the Python Software Foundation.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26249, "guid": "95672b07-113f-57e6-a0ca-fac583b3babe", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "00:45", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26249-accelerating-python-code", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/W9HLK3/", "title": "Accelerating Python Code", "subtitle": "", "track": "PyCon: Python Language", "type": "Talk (long)", "language": "en", "abstract": "Python is a beautiful language for fast prototyping and and sketching ideas quickly. People often struggle to get their code into production though for various reasons. Besides of all security and safety concerns that usually are not addressed from the very beginning when playing around with an algorithmic idea, performance concerns are quite frequently a reason for not taking the Python code to the next level.\r\n\r\nWe will look at the \"missing performance\" worries using a simple numerical problem and how to speed the corresponding Python code up to top notch performance.", "description": "We all know how much fun it is to play around with an algorithmic idea in Python. It's very satisfying to see the idea develop, doing what it's supposed to be doing and how simple and elegant the code finally looks like. Python being so feature complete with its standard library and the 3rd party universe of libraries and packages allows development to be very quick. And, we're all very grateful to be able to focus on the problem itself, not on the language specifics, to solve it.\r\n\r\nBut when we're arriving at the point where everything just works, there is this one last step that needs to be mastered: Get it into production to finally let it do what it was supposed to be doing and make life easier for all of us.\r\n\r\nBut at that stage there are those final hurdles, and they usually feel giant, that arise unpleasant questions. Will the algorithm really do what it was supposed to be doing under all circumstances? Will it be safe? What if it fails? Will it actually be fast enough for all the data it needs to process in production? Will it be capable of doing its job in the future, when the amount of work grows?\r\n\r\nWhilst the first worries usually can be addressed well using established software engineering habits and patterns, the performance related issue is often seen as the killer on the way to production use, as Python is still considered to be slow just based on the fact that it is an interpreted language. Quite often code is rewritten after the prototyping phase in other languages considered to be fast, such as C++ for example, for this very reason.\r\n\r\nWe'll look at exactly this point and explore ways to accelerate Python code by simple modifications and using third party libraries to support us. \r\n\r\nTo do that we will look at some code to solve a simple numerical problem - calculating the Mandelbrot Set - as it is well suited for this and quite simple to follow. Yet it generates stunning and beautiful results entertaining us through the course of the presentation.\r\n\r\nThe strategies shown to accelerate the code, based on concepts taken from standard library, PyPy, numpy, numba and dask, however are transferable to other algorithmic problems as well.\r\n\r\nWe will analyse the advantages as well as the drawbacks for each concept to see the overall effect and where else the solution might apply.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25799, "code": "DUSVE9", "public_name": "Jens Nie", "biography": "A physicist who has filled a variety of roles in a leading service company in the oil and gas industry, currently tackling the development of embedded devices based on the Raspberry Pi, LinuX and Python with a Python history going back to version 1.4.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26168, "guid": "4f10d138-6180-5865-ab4b-673d0881d2ef", "logo": "", "date": "2023-04-19T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26168-unlocking-information-creating-synthetic-data-for-open-access-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/J9KRKZ/", "title": "Unlocking Information - Creating Synthetic Data for Open Access.", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "Many good project ideas fail before they even start due to the sensitive personal data required. The good news: a synthetic version of this data does not need protection. Synthetic data copies the actual data's structure and statistical properties without recreating personally identifiable information. The bad news: It is difficult to create synthetic data for open-access use, without recreating the exact copy of actual data.\r\nThis talk will give hands-on insights into synthetic data creation and challenges along its lifecycle. We will learn how to create and evaluate synthetic data for any use case using the open-source package Synthetic Data Vault. We will find answers to why it takes so long to synthesize the huge amount of data dormant in public administration. The talk addresses owners who want to create access to their private data as well as analysts looking to use synthetic data. After this session, listeners will know which steps to take to generate synthetic data for multi-purpose use and its limitations for real-world analyses.", "description": "A vast amount of private data lies dormant in public institutions, hidden from the research community. Synthesizing complex, anonymized data could allow researchers access without disclosing personally identifiable information while keeping information loss minimal. The tools to do this exist, but why is it still difficult to realize synthetic solutions? One challenge is to reach the minimum viable quality to serve as many use cases as possible. Ideally, the synthetic data allows data exploration with equal results as the real data. We will guide you through the challenges of creating synthetic data and shine a light on its lifecycle. We will explore the different levels of quality of generated structured data and discuss their potential. Finally, we will link these issues to the domain of public administration, but the main insights are generally applicable to all kinds of domains. In particular, we will focus on four key questions:\r\n \r\n1. How can we create synthetic data from private data?\r\n2. How can synthetic data creation be integrated into institutions that sit on piles of unused highly private data?\r\n3. Can SOTA methods for synthetic data fulfill all needs of the research community? When is access to the actual, private data needed?\r\n4. Which quality measures are adequate for synthetic data?\r\n \r\nAs we address these questions, we'll use the Synthetic Data Vault to create and evaluate synthetic data. After the talk listeners will have understood the concept of synthetic data and will be able to evaluate synthetic data for a plethora of use cases. As a plus, they will also gain a deeper understanding of why open data access is (not yet) solved by synthetic data.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25742, "code": "KFCSXT", "public_name": "Antonia Scherz", "biography": "Antonia Scherz is machine learning engineer and consultant at PD - Berater der \u00f6ffentlichen Hand in Berlin. At PD she builds proof of concept tools and assists in software development for machine learning applications in public administration. She is passionate about making machine leanring and open software tools widely used by public administration and fascinated by how new tools can be integrated in old structures for the public good.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26290, "guid": "3f75f160-5911-5ec2-90e1-8fb7ed50faaa", "logo": "", "date": "2023-04-19T11:50:00+02:00", "start": "11:50", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26290-modern-typed-python-dive-into-a-mature-ecosystem-from-web-dev-to-machine-learning", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/LCCGTT/", "title": "Modern typed python: dive into a mature ecosystem from web dev to machine learning", "subtitle": "", "track": "PyCon: Python Language", "type": "Talk", "language": "en", "abstract": "Typing is at the center of \u201emodern Python\u201c, and tools (mypy, beartype) and libraries (FastAPI, SQLModel, Pydantic, DocArray) based on it are slowly eating the Python world.\r\n\r\nThis talks explores the benefits of Python type hints, and shows how they are infiltrating the next big domain: Machine Learning", "description": "The talk will focus on\u00a0**modern python**\u00a0and its extensive usage of\u00a0**type hints and static type analysis**. There will be a special focus on\u00a0**DocArray and multi-modal AI applications**.\r\n\r\nThe talk will cover different topics around modern python:\r\n\r\n- The history of Python and type hint. How Python came from being not static typed language to having static-type analysis?\r\n- The state of the modern python ecosystem in 2023:\r\n    - Powerful development tools like mypy and beartype. Parallel with TypeScript\r\n    - Powerful libraries that leverage type hint: Pydantic, FastAPI, SQLModel, Typer, DocArray\r\n- Deep dive on DocArray and the future of AI-based web app:\r\n    - Why modern python is a key to speeding up the development of multi-modal AI applications (stable diffusion, neural search \u2026)?\r\n    - What is DocArray and how does it extend pydantic with multi-modal AI in mind?", "recording_license": "", "do_not_record": false, "persons": [{"id": 25834, "code": "H8FBGG", "public_name": "samsja", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26391, "guid": "7d2b1e96-2152-55de-972d-9eb00580a8bb", "logo": "", "date": "2023-04-19T12:25:00+02:00", "start": "12:25", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26391-code-cleanup-a-data-scientist-s-guide-to-sparkling-code", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MBZJE9/", "title": "Code Cleanup: A Data Scientist's Guide to Sparkling Code", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Does your production code look like it\u2019s been copied from Untitled12.ipynb? Are your engineers complaining about the code but you can\u2019t find the time to work on improving the code base? This talk will go through some of the basics of clean coding and how to best implement them in a data science team.", "description": "Data scientists often have a different background and priorities than software engineers. A lot of the code Data Scientists write never makes it to production, and as a result, the code might not always meet the same standards as production-ready code in a developer team. While it makes sense to have rather lax requirements on code for one-off analyses, this can lead to difficulties in maintaining production code and collaborating on projects with software engineers. Since production code is not (always) the main output of a data science team, it can also be hard to prioritize code quality. \r\n\r\nIn this presentation, we will go over some of the main principles of clean code and talk about practical steps that data science teams can take to improve their code. We will specifically focus on strategies that teams can implement to slowly and steadily improve the existing code base. This talk is aimed at data scientists who may not have a strong background in software engineering, but are interested in improving code quality and collaborating more effectively with software engineering teams.", "recording_license": "", "do_not_record": false, "persons": [{"id": 2081, "code": "LXVTBW", "public_name": "Corrie Bartelheimer", "biography": "Corrie Bartelheimer first became interested in data when studying topological data analysis during her math Masters. After working a few years in Berlin and organizing the Berlin Bayesian meetups for a while, she moved to Brussels, Belgium where she now works as a Data Scientist in the hospitality industry. Her interests include, among others, Bayesian modelling, network analysis, data visualization and best practices for data science teams.\r\nIn her freetime, she enjoys cooking for friends and sampling new Belgium beers.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26486, "guid": "afbe6077-53ef-57a6-83c5-1106a4eccbe2", "logo": "", "date": "2023-04-19T14:00:00+02:00", "start": "14:00", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26486-behind-the-scenes-of-tox-the-journey-of-rewriting-a-python-tool-with-more-than-10-million-monthly-downloads", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/XEVGVJ/", "title": "Behind the Scenes of tox: The Journey of Rewriting a Python Tool with more than 10 Million Monthly Downloads", "subtitle": "", "track": "PyCon: Testing", "type": "Talk", "language": "en", "abstract": "tox is a widely-used tool for automating testing in Python. In this talk, we will go behind the scenes of the creation of tox 4, the latest version of the tool. We will discuss the motivations for the rewrite, the challenges and lessons learned during the development process. We will have a look at the new features and improvements introduced in tox 4. But most importantly, you will get to know the maintainers.", "description": "Do you recall what developer legend Joel Spolsky called the \"single worst strategic mistake\" in \"Things You Should Never Do\"?\r\n\r\nRewriting software from scratch.\r\n\r\nThat is what we did. For the tox test automation tool. A tool, downloaded more than 10 million times a month, both heavily used in the open source community, and in multi-billion dollar companies alike.\r\n\r\nI invite you to join me on the very personal journey of rewriting tox, a journey, which already started in 2019. We will have a look at the initial motivations for the rewrite, the design decisions, the challenges, and the lessons learned.\r\n\r\nWe will reconstruct why it took more than three years, from the idea to the release, and why this was a good thing.\r\n\r\nI will explain what we did to take care that the release would cause the least amount of issues, and why we still received dozens and dozens of bug reports about regressions the days after the release.\r\n\r\nAnd finally, I will answer the question. Was it worth it?", "recording_license": "", "do_not_record": false, "persons": [{"id": 10700, "code": "EARAKA", "public_name": "J\u00fcrgen Gmach", "biography": "I am a software developer with a passion for Python and Linux, developing open source software both at my day job at Canonical, and at night as a maintainer of tox and many other projects.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25927, "guid": "fc09daeb-bbbe-523c-aa60-31b7be18c00e", "logo": "", "date": "2023-04-19T14:35:00+02:00", "start": "14:35", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-25927-giving-and-receiving-great-feedback-through-prs", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/9SENVW/", "title": "Giving and Receiving Great Feedback through PRs", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Do you struggle with PRs? Have you ever had to change code even though you disagreed with the change just to land the PR? Have you ever given feedback that would have improved the code only to get into a comment war? We'll discuss how to give and receive feedback to extract maximum value from it and avoid all the communication problems that come with PRs.", "description": "Do you struggle with PRs? Have you ever had to change code even though you disagreed with the change just to land the PR? Have you ever given feedback that would have improved the code only to get into a comment war? We'll discuss how to give and receive feedback to extract maximum value from it and avoid all the communication problems that come with PRs. We'll start with some thoughts about what PRs are intended to achieve and then first discuss how to give feedback that will be well received and result in improvements to the code followed by how to extract maximum value from feedback you receive without agreeing to suboptimal changes. Finally, we will look at a checklist for giving and receiving feedback you can use as you go through reviews both as an author and reviewer.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25530, "code": "DBECVR", "public_name": "David Andersson", "biography": "I started as a web developer after university with an Australian telco developing websites using Python and JavaScript. After a few years, I switched to product management looking after developer telco products such as an API for sending and receiving SMS where I overhauled the developer portal improving the developer experience. Then I switched back to engineering leadership looking after a team that was creating private and public cloud products where I launched a new private cloud product.\r\n\r\nAfter a few years in the telco industry, I switched to Canonical where I lead a team of developer automating operations using and creating open source tooling.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26404, "guid": "528ba98b-da2c-564b-81ee-0566077ee5c8", "logo": "", "date": "2023-04-19T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "Kuppelsaal", "slug": "pyconde-pydata-berlin-2023-26404-how-to-increase-diversity-in-open-source-communities", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/7SYVML/", "title": "How to increase diversity in open source communities", "subtitle": "", "track": "General: Community, Diversity, Career, Life and everything else", "type": "Talk", "language": "en", "abstract": "Today state of the art technology and scientific research strongly depend on open source libraries. The demographic of the contributors to these libraries is predominantly white and male [1][2][3][4]. This situation creates problems not only for individual contributors outside of this demographic but also for open source projects such as loss of career opportunities and less robust technologies, respectively [1][7]. In recent years there have been a number of various recommendations and initiatives to increase the participation in open source projects of groups who are underrepresented in this domain [1][3][5][6]. While these efforts are valuable and much needed, contributor diversity remains a challenge in open source communities [2][3][7]. This talk highlights the underlying problems and explores how we can overcome them.", "description": "In this talk we\u2019ll first examine the problems encountered by people belonging to marginalised groups in open source as well as by project maintainers with respect to contributing to and increasing the diversity of open source projects, respectively [1][2][3][4][5][6]. Building on this overview, we\u2019ll go over what kind of actions have been taken to increase diversity in open source projects, with special focus on scientific libraries, and the effects they have had [1][6][7]. Lastly, we\u2019ll look at ideas that are currently being tested and next steps. By the end of this talk, the audience will have a good understanding of why contributor diversity is low in open source, the efforts that have been made so far to address this problem, and what can further be done to increase the presence of underrepresented groups in technology in general, and in open source in particular.\r\n\r\nReferences:\r\n[1] https://www.wired.com/2017/06/diversity-open-source-even-worse-tech-overall \r\n[2] https://arxiv.org/pdf/1706.02777.pdf\r\n[3] https://ieeexplore.ieee.org/abstract/document/8870179\r\n[4] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9354402 \r\n[5] https://biancatrink.github.io/files/papers/JISA2021.pdf\r\n[6] https://arxiv.org/pdf/2105.08777.pdf\r\n[7] https://blog.scikit-learn.org/events/sprints-value/", "recording_license": "", "do_not_record": false, "persons": [{"id": 20240, "code": "R9KUCJ", "public_name": "Maren Westermann", "biography": "Dr Maren Westermann works as a machine learning engineer and holds a PhD in environmental science. She is a self taught Pythonista, an active open source contributor, especially to the library scikit-learn, and is a co-organiser of PyLadies Berlin where she hosts monthly open source hack nights.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B09": [{"id": 28576, "guid": "19df9e91-d95d-5d50-8785-1ae188a7ba92", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "00:45", "room": "B09", "slug": "pyconde-pydata-berlin-2023-28576-the-spark-of-big-data-an-introduction-to-apache-spark", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/Q7GS8Y/", "title": "The Spark of Big Data: An Introduction to Apache Spark", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk (long)", "language": "en", "abstract": "Get ready to level up your big data processing skills! Join us for an introductory talk on Apache\r\nSpark, the distributed computing system used by tech giants like Netflix and Amazon. We&#39;ll\r\ncover PySpark DataFrames and how to use them. Whether you&#39;re a Python developer new to\r\nbig data or looking to explore new technologies, this talk is for you. You&#39;ll gain foundational\r\nknowledge about Apache Spark and its capabilities, and learn how to leverage DataFrames and\r\nSQL APIs to efficiently process large amounts of data. Don&#39;t miss out on this opportunity to up\r\nyour big data game!", "description": "Get ready to level up your big data processing skills! Join us for an introductory talk on Apache\r\nSpark, the distributed computing system used by tech giants like Netflix and Amazon. We&#39;ll\r\ncover PySpark DataFrames and how to use them. Whether you&#39;re a Python developer new to\r\nbig data or looking to explore new technologies, this talk is for you. You&#39;ll gain foundational\r\nknowledge about Apache Spark and its capabilities, and learn how to leverage DataFrames and\r\nSQL APIs to efficiently process large amounts of data. Don&#39;t miss out on this opportunity to up\r\nyour big data game!", "recording_license": "", "do_not_record": false, "persons": [{"id": 27710, "code": "FK3UKV", "public_name": "Pasha Finkelshteyn", "biography": "Pasha Finkelshteyn is a developer advocate for data engineering at JetBrains with more than a\r\ndecade of experience in the industry. He has a passion for making big data processing\r\naccessible to all and has spent most of his career working with the JVM. However, Pasha\r\nswitched to Data Engineering where he discovered the power of Python", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26482, "guid": "6640a208-3f6e-5f42-b0f8-9d420556039e", "logo": "", "date": "2023-04-19T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26482-shrinking-gigabyte-sized-scikit-learn-models-for-deployment", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/7EUPC3/", "title": "Shrinking gigabyte sized scikit-learn models for deployment", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "We present an open source library to shrink pickled scikit-learn and lightgbm models. We will provide insights of how pickling ML models work and how to improve the disk representation. With this approach, we can reduce the deployment size of machine learning applications up to 6x.", "description": "At QuantCo, we create value from data using machine learning. To that end, we frequently build gigabyte-sized machine learning models. However, deploying and sharing those models can be challenge because of their size. We built and open-sourced a library to aggressively compress tree-based machine learning models: [slim-trees](https://github.com/pavelzw/slim-trees).\r\n\r\nIn this talk, we share our journey and the ideas that went into the making of slim-trees. We delve into the internals of sklearn\u2019s Tree-based models to understand their memory footprint. Afterwards, we explore different techniques that allow us to reduce model size without sacrificing predictive performance.\r\n\r\nFinally, we present how to include slim-trees in your project and give an outlook on what\u2019s to come.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25949, "code": "3W7GZM", "public_name": "Pavel Zwerschke", "biography": "Pavel is a data engineer at QuantCo who is currently studying Mathematics and Computer Science at KIT.", "answers": []}, {"id": 29230, "code": "3VJGE8", "public_name": "Yasin Tatar", "biography": "Yasin works as a Data Engineer at QuantCo and studies Computer Science at Karlsruhe Institute of Technology (KIT)", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26335, "guid": "6bc15245-a5c6-5c4b-8b1a-216b0df37ac0", "logo": "", "date": "2023-04-19T11:50:00+02:00", "start": "11:50", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26335-what-are-you-yield-from-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/ENQBPJ/", "title": "What are you yield from?", "subtitle": "", "track": "PyCon: Python Language", "type": "Talk", "language": "en", "abstract": "Many developers avoid using generators. For example, many well-known python libraries use lists instead of generators. The generators themselves are slower than normal list loops, but their use in code greatly increases the speed of the application. Let\u2019s discover why.", "description": "Many developers, avoid to use the generators in regular python code:\r\n\r\nIt is hard to debug,\r\nit is not easy to profile,\r\nit is not obviously to refactor.\r\nit requires to use special algorithms.\r\nIn this talk i speak about generator pipelines, one-line-generators, builtin-generators, custom generators with yield and yield from.\r\nI will show how to use generators and why we should use them. Also, we learn about situations where we can\u2019t use generators and how to change our thinking to avoid such situations in the future. I give some hints and examples - how big python frameworks use lists instead of generators and therefore lose performance. At the end we can see how builtin zip function works in other world, where developers always use generators in own code.\r\n\r\nLet see what we can yield from this talk\u2026", "recording_license": "", "do_not_record": false, "persons": [{"id": 17714, "code": "TJSMCP", "public_name": "Maxim Danilov", "biography": "more than 24 years in development start with RISC assembler grows to python/Django/VueJs through C, VB, PHP, Jquery", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25850, "guid": "19829511-afbd-5be3-8937-3374787bf0b5", "logo": "", "date": "2023-04-19T12:25:00+02:00", "start": "12:25", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-25850-streamlit-meets-webassembly-stlite", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/GYEZVW/", "title": "Streamlit meets WebAssembly - stlite", "subtitle": "", "track": "PyCon: Web", "type": "Talk", "language": "en", "abstract": "Streamlit, a pure-Python data app framework, has been ported to Wasm as \"stlite\".\r\nSee its power and convenience with many live examples and explore its internals from a technical perspective.\r\nYou will learn to quickly create interactive in-browser apps using only Python.", "description": "Streamlit lets you create interactive web apps with Python, and its WebAssembly-port \"stlite\" extends its power to in-browser apps.\r\n\"stlite\" offers offline capability, data privacy, scalability, and multi-platform portability while keeping Streamlit\u2019s original features, such as Python productivity and its rich ecosystem.\r\n\r\nIn this talk, after a short intro of Streamlit, we will review stlite in the context of the recent emergence of various Wasm-based Python frameworks such as PyScript, and show you what's possible with stlite. We will also see its internals from a technical aspect which may inspire you with ideas about how to make use of Pyodide and how to transform Python frameworks for the Pyodide/Wasm runtime.", "recording_license": "", "do_not_record": false, "persons": [{"id": 14539, "code": "FLSGCD", "public_name": "Yuichiro Tachibana", "biography": "Yuichiro works as a professional software developer and also loves contributing to OSS projects. As a Pythonista, he has participated in various projects including web development, multimedia streaming, data management, computer vision, and machine learning.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26324, "guid": "d55df135-6b7c-5280-afe6-6398b4df65df", "logo": "", "date": "2023-04-19T14:00:00+02:00", "start": "14:00", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26324-bringing-nlp-to-production-an-end-to-end-story-about-some-multi-language-nlp-services-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/VHNJ37/", "title": "Bringing NLP to Production (an end to end story about some multi-language NLP services)", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "Models in Natural Language Processing are fun to train but can be difficult to deploy. The size of their models, libraries and necessary files can be challenging, especially in a microservice environment. When services should be built as lightweight and slim as possible, large (language) models can lead to a lot of problems. With a recent real-world use case as an example, which runs productively for over a year and in 10 different languages, I will walk you through my experiences with deploying NLP models. What kind of pitfalls, shortcuts, and tricks are possible while bringing an NLP model to production?\r\n\r\nIn this talk, you will learn about different ways and possibilities to deploy NLP services. I will speak briefly about the way leading from data to model and a running service (without going into much detail) before I will focus on the MLOps part in the end. I will take you with me on my past journey of struggles and successes so that you don\u2019t need to take these detours by yourselves.", "description": "Models in Natural Language Processing are fun to train but can be difficult to deploy. The size of their models, libraries and necessary files can be challenging, especially in a microservice environment. When services should be built as lightweight and slim as possible, large (language) models can lead to a lot of problems. \r\nAll the way down from brainstorming the use case, receiving and cleaning the data, training and optimizing the model until service building, deployment, and quality monitoring, lots of important data science related decisions need to be made which in the end will influence the selection of deployment tools and infrastructure. And most often, those architectural decisions are rather long-term so they should be thoughtfully chosen in order to fit into the rest of the architecture.\r\nWith a recent real-world use case as an example, which runs productively for over a year and in 10 different languages, I will walk you through my experiences with deploying NLP models. What kind of pitfalls, shortcuts, and tricks are possible while bringing an NLP model to production? How can different model types and approaches influence architectural decisions? What are the most important questions to evaluate deployment platforms when there are several options to choose from?\r\nIn this talk, you will learn about different ways and possibilities to deploy NLP services. I will speak briefly about the way leading from data to model and a running service (without going into much detail) before I will focus on the MLOps part in the end. I will take you with me on my past journey of struggles and successes so that you don\u2019t need to take these detours by yourselves. To follow this talk, you will need to know the basic concepts of deployment and MLOps, but no deeper knowledge of python or Natural Language Processing.\r\nMy goal is to enable you to ask important questions about deployment and going into production right at the beginning of every NLP project. I want you to be aware of problems that might occur so that working on NLP projects will be fun and not be overshadowed by deployment issues.", "recording_license": "", "do_not_record": false, "persons": [{"id": 15991, "code": "HVSNHD", "public_name": "Larissa Haas", "biography": "I am a Senior Data Scientist working at sovanta AG in Heidelberg. With university degrees in Political Science and Data Science, I combine ethical and business views on NLP projects. My latest projects dealt with combining Machine Learning approaches with SAP technologies. Besides that, I care about AI in Science Fiction, Bullet Journaling, and bringing Roundnet to the Olympic Games.", "answers": []}, {"id": 28287, "code": "DVRPY9", "public_name": "Jonathan  Brandt", "biography": "Hi :)\r\nI'm working as a Data Scientist since one year, mainly on topics of natural language processing and timeseries forecasting.\r\nBefore that I studied physics in Heidelberg.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26432, "guid": "79953a40-38a9-5ab9-a1b7-86a775f7f27e", "logo": "", "date": "2023-04-19T14:35:00+02:00", "start": "14:35", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26432-evosax-jax-based-evolution-strategies", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/WMWZQC/", "title": "evosax: JAX-Based Evolution Strategies", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "Tired of having to handle asynchronous processes for neuroevolution? Do you want to leverage massive vectorization and high-throughput accelerators for evolution strategies (ES)? [evosax](https://github.com/RobertTLange/evosax) allows you to leverage JAX, XLA compilation and auto-vectorization/parallelization to scale ES to your favorite accelerators. In this talk we will get to know the core API and how to solve distributed black-box optimization problems with evolution strategies.", "description": "The deep learning revolution has greatly been accelerated by the 'hardware lottery': Recent advances in modern hardware accelerators and compilers paved the way for large-scale batch gradient optimization. Evolutionary optimization, on the other hand, has mainly relied on CPU-parallelism, e.g. using Dask scheduling and distributed multi-host infrastructure. Here we argue that also modern evolutionary computation can significantly benefit from the massive computational throughput provided by GPUs and TPUs. In order to better harness these resources and to enable the next generation of black-box optimization algorithms, we release [evosax](https://github.com/RobertTLange/evosax): A JAX-based library of evolution strategies which allows researchers to leverage powerful function transformations such as just-in-time compilation, automatic vectorization and hardware parallelization. [evosax](https://github.com/RobertTLange/evosax) implements 30 evolutionary optimization algorithms including finite-difference-based, estimation-of-distribution evolution strategies and various genetic algorithms. Every single algorithm can directly be executed on hardware accelerators and automatically vectorized or parallelized across devices using a single line of code. It is designed in a modular fashion and allows for flexible usage via a simple ask-evaluate-tell API. We thereby hope to facilitate a new wave of scalable evolutionary optimization algorithms.", "recording_license": "", "do_not_record": false, "persons": [{"id": 16381, "code": "V9Q3VG", "public_name": "Robert Lange", "biography": "I am a 3rd year PhD student working on Evolutionary Meta-Learning at the Technical University Berlin. My work is funded by the Science of Intelligence Excellence Cluster and supervised by Henning Sprekeler. Previously, I completed a MSc in Computing at Imperial College London, a Data Science MSc at Universitat Pompeu Fabra and an Economics undergraduate at University of Cologne. I also interned at DeepMind (Discovery team) & Accenture and maintain a set of open source tools.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26515, "guid": "7f78c053-6f23-510c-9e97-b985b5b46899", "logo": "", "date": "2023-04-19T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B09", "slug": "pyconde-pydata-berlin-2023-26515-postmodern-architecture-the-python-powered-modern-data-stack", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/A7B8P8/", "title": "Postmodern Architecture: The Python Powered Modern Data Stack", "subtitle": "", "track": "PyData: Data Handling", "type": "Talk", "language": "en", "abstract": "The Modern Data Stack has brought a lot of new buzzwords into the data engineering lexicon: \"data mesh\", \"data observability\", \"reverse ETL\", \"data lineage\", \"analytics engineering\". In this light-hearted talk we will demystify the evolving revolution that will define the future of data analytics & engineering teams.\r\n\r\nOur journey begins with the PyData Stack: pandas pipelines powering ETL workflows...clean code, tested code, data validation, perfect for in-memory workflows. As demand for self-serve analytics grows, new data sources bring more APIs to model, more code to maintain, DAG workflow orchestration tools, new nuances to capture (\"the tax team defines revenue differently\"), more dashboards, more not-quite-bugs (\"but my number says this...\").\r\n\r\nThis data maturity journey is a well-trodden path with common pitfalls & opportunities. After dashboards comes predictive modelling (\"what will happen\"), prescriptive modelling (\"what should we do?\"), perhaps eventually automated decision making. Getting there is much easier with the advent of the Python Powered Modern Data Stack.\r\n\r\nIn this talk, we will cover the shift from ETL to ELT, the open-source Modern Data Stack tools you should know, with a focus on how dbt's new Python integration is changing how data pipelines are built, run, tested & maintained. By understanding the latest trends & buzzwords, attendees will gain a deeper insight into Python's role at the core of the future of data engineering.", "description": "This light-hearted talk will aim to introduce the audience to the theory and terminology of data pipelines and architectures past, present and future. The \"Modern Data Stack\" set of interoperable tools introduced a shift in how organisations can rapidly construct a data architecture that can combine multiple data sources into a single unified data warehouse with clean analytics-ready tables for plugging BI tools, self-serve analytics dashboards, and ML models into.\r\n\r\nUntil recently, the complexity of data transformation and modelling was limited to what can be done with SQL, leaving the rich ecosystem of Python tooling for complex transformations, geospatial analytics, time series modelling, data validation tools and clean tested CI-enabled codebases mostly uninvited to the Modern Data Stack party. A recent trend has been a number of tools that launched Python integrations in 2022 (most notably by dbt), opening up a world of productivity and fast scalable data processing for the PyData-savvy Pythonista.\r\n\r\nAnother recent trend is an explosion of jargon, with analytics engineers getting into heated debates around whether data observability or metadata-capture should be prioritised within a data mesh architecture. These are all important concepts, especially for organisations operating at a scale where reliable data governance is mission-critical. Not all organisations are operating at that scale, and every organisation large or small is own its own data maturity journey.\r\n\r\nMy goal with this talk is to bring these concepts together, introduce attendees to these recent trends, and provide a framework they can take back into their organisations for accelerating their own data maturity journey using the latest tooling & best practices.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25966, "code": "MM3RDV", "public_name": "John Sandall", "biography": "John Sandall is the CEO and Principal Data Scientist at Coefficient.\r\n\r\nHis experience in data science and software engineering spans multiple industries and applications, and his passion for the power of data extends far beyond his work for Coefficient\u2019s clients. In April 2017 he created SixFifty in order to predict the UK General Election using open data and advanced modelling techniques. Previous experience includes Lead Data Scientist at YPlan, business analytics at Apple, genomics research at Imperial College London, building an ed-tech startup at Knodium, developing strategy & technological infrastructure for international non-profit startup STIR Education, and losing sleep to many hackathons along the way.\r\n\r\nJohn is also a co-organiser of PyData London, co-founded Humble Data in 2019 to promote diversity in data science through a programme of free bootcamps, and in 2020 was a Committee Chair for the PyData Global Conference. He is currently a Fellow of Newspeak House with interests in open data, AI ethics and promoting diversity in tech.", "answers": []}], "links": [], "attachments": [], "answers": []}], "B07-B08": [{"id": 26344, "guid": "f6a74652-6e67-5bb0-a652-bd01296ee940", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "00:45", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26344-monorepos-with-python", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/XDRNQC/", "title": "Monorepos with Python", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk (long)", "language": "en", "abstract": "Working with python is fun.\r\nManaging python packaging, linters, tests, CI, etc. is not as fun.\r\n\r\nEvery maintainer needs to worry about consistent styling, quality, speed of tests, etc as the project grows.\r\n\r\nMonorepos have been successful in other communities - how does it work in Python ?", "description": "As a python project grows (within 2-3 years), you will go down either of these 2 paths:\r\n\r\n - Create a monolith\r\n - Modularize your code into smaller packages\r\n\r\nIn the current world, you will be affected by multiple other libraries you use. And modularity is a requirement for any good project.\r\n\r\nBut managing multiple modular packages becomes tough over time.\r\n\r\n1. How do you ensure coding standards (quality, styling, etc) is consistent across them ?\r\n1. How would we ensure all the pakages work correctly without spending hours and hours of CI time ?\r\n1. How can common logical pieces be modularized further and still be DRY ?\r\n\r\nThese are common issues I have faced by the 2-3 year mark in any active project. And if not solved quickly can easily cause your project to get messy very quickly.\r\n\r\nThis talk aims to discuss these common issues and how a monorepo structure which is widely popular in other communities like NodeJS can also be applied to python.\r\n\r\nWe also discuss how the crux of the issue:\r\n\r\n - Making your code structure machine understandable\r\n - How this structured information can then be used to optimize workloads\r\n - How this structured information can be used to automate tasks\r\n\r\nAnd also go into discussing how **monorepo tools** like pants, bazel, nx, etc. leverage this code structure information to simplify your life as a maintainer", "recording_license": "", "do_not_record": false, "persons": [{"id": 24329, "code": "ACJBLZ", "public_name": "AbdealiLoKo", "biography": "Hi, I'm Abdeali Kothari - a.k.a Ali (if we're talking) or @AbdealiLoKo (if we're typing)  \r\nI graduated from IIT Madras and then worked with American Express, followed by Corridor Platforms where I am architecting a Decisioning platform for analytics in the Financial domain.\r\n\r\nI've dabbled with Robotics, Operating System architectures, Machine Learning, Game Development, and Web Development a lot for a bunch of personal projects.  \r\nAnd worked mainly in Big Data, Machine Learning, and Analytics in the Financial Domain for enterprise-productional use-cases.\r\n\r\nI'm a big fan of code hygiene and clean architecture. With a lot of Code Analytics experience under my belt.  \r\nAnd worked mainly in Python in all the above fields for about 13 years now (Back when the first blogpost telling us to stop using Python 2.x was written :D)\r\n\r\nI'm extremely lazy - and hence an automation freak. And have created great automated test suites and CI/CD pipelines to help me remain lazy.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26161, "guid": "10337de2-f12a-5b5a-aad0-8da5521b2a70", "logo": "", "date": "2023-04-19T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26161-thou-shall-judge-but-with-fairness-methods-to-ensure-an-unbiased-model", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/CBHYXG/", "title": "Thou Shall Judge But With Fairness: Methods to Ensure an Unbiased Model", "subtitle": "", "track": "General: Ethics & Privacy", "type": "Talk", "language": "en", "abstract": "Is your model prejudicial? Is your model deviating from the predictions it ought to have made? Has your model misunderstood the concept? In the world of artificial intelligence and machine learning, the word \"fairness\" is particularly common. It is described as having the quality of being impartial or fair. Fairness in ML is essential for contemporary businesses. It helps build consumer confidence and demonstrates to customers that their issues are important. Additionally, it aids in ensuring adherence to guidelines established by authorities. So guaranteeing that the idea of responsible AI is upheld. In this talk, let's explore how certain sensitive features are influencing the model and introducing bias into it. We'll also look at how we can make it better.", "description": "We cannot escape thinking about fairness through numbers and math. Models are not fair simply because they are mathematical, contrary to popular belief. AI systems are subjected to bias. It may be inherent which is due to historical bias in the training dataset. There may be label bias that occurs when the set of labeled data is not a full representation of the entire universe of existing potential labels. Another potential bias is sampling bias, which occurs when certain people in the intended universe have a higher or lower sampling probability than others. Models learn from such biased datasets which may lead to unfair decisions. As cascading models are developed, this bias continues to spread.\r\n\r\nModel fairness is an alerting concern. Unfair AI systems can create habitual losses for businesses. It may also contribute unfavorable commercial values to the company, creating situations like customer eroding, slandering, and decreasing transparency. As a result, Model fairness is becoming increasingly necessary.\r\nIn the proposed talk, I would gently introduce you to the above concepts and some open source libraries that would help us in accessing ML models' fairness. Lastly, I would be walking you through how to assess the fairness of a model for a law school dataset using Fairlearn, an open source library by Microsoft and the measures that can be taken to mitigate the same. \r\n \r\n\r\n\r\n\r\nMy Talk will Focus On\r\n\r\n1. What are the metrics that need to be considered for assessing the fairness of an ML model?\r\n2. What are the mitigation measures that can be implemented for the same?\r\n3. Python code to gauge the fairness of a model trained on a law school dataset using Fairlearn and steps to mitigate the model.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25737, "code": "3EBWTP", "public_name": "Nandana Sreeraj", "biography": "Nandana is a data scientist at Censius AI. She had completed her bachelors degree from College Of Engineering, Trivandrum. She had previously worked in the e-commerce industry where she had to deal with real world problem statements including product ranking and recommendation systems. She had published a research paper in health care domain in an international journal. Currently, her research interests are aligned to the Explainable AI domain.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 30912, "guid": "fd69565b-8c0d-5e83-b22e-a63b5e913d52", "logo": "", "date": "2023-04-19T11:50:00+02:00", "start": "11:50", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-30912-prompt-engineering-101-beginner-intro-to-langchain-the-shovel-of-our-chatgpt-gold-rush-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MSZG7B/", "title": "Prompt Engineering 101: Beginner intro to LangChain, the shovel of our ChatGPT gold rush.\"", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "A modern AI start-up is a front-end developer plus a prompt engineer\" is a popular joke on Twitter. \r\nThis talk is about LangChain, a Python open-source tool for prompt engineering. You can use it with completely open-source language models or ChatGPT. I will show you how to create a prompt and get an answer from LLM. As an example application, I will show a demo of an intelligent agent using web search and generating Python code to answer questions about this conference.", "description": "There is a gold rush to apply AI to anything nowadays. Anyone can do it, you no longer need to be a Machine Learning Engineer! Just write some prompts for ChatGPT.\r\n\r\nThere is a saying \"During a gold rush - sell shovels\". This talk is about a wonderful tool, LangChain, as easy to use as a good shovel.\r\n\r\nThis talk is about LangChain, a Python open-source tool for prompt engineering. You can use it with completely open-source language models or ChatGPT. \r\n\r\nThe project started 6 months ago and now has 25k Github stars and raised $10 mln. What is all this about?\r\n\r\nThis talk is a gentle introduction. It will show how to:\r\n- create a simple prompt\r\n- get an answer from a Large Language Model of your choice - local or API\r\n- chain requests together to search the web, use Python REPL\r\n- make LLM choose which tools to use for complex questions\r\n- answer questions over a collection of long documents\r\n\r\nAs an example application, we will code an AI agent to answer \"When is the PyCon DE & PyData Berlin 2023 conference? How many days are between that date and today?\" using web search and Python REPL.", "recording_license": "", "do_not_record": false, "persons": [{"id": 24692, "code": "8BJ7S9", "public_name": "Lev Konstantinovskiy", "biography": "Lev Konstantinovskiy is an experienced data science and software engineering team lead. Long time ago he used to maintain a python Natural Language Processing library gensim.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25878, "guid": "0de53cbd-3be4-5196-8c0a-1ea8eb4b87b1", "logo": "", "date": "2023-04-19T12:25:00+02:00", "start": "12:25", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-25878-how-to-connect-your-application-to-the-world-and-avoid-sleepless-nights-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MTXCHH/", "title": "How to connect your application to the world (and avoid sleepless nights)", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Let\u2019s say you are the ruler of a remote island. For it to succeed and thrive you can\u2019t expect it to be isolated from the world. You need to establish trade routes, offer your products to other islands, and import items from them. Doing this will certainly make your economy grow! We\u2019re not going to talk about land masses or commerce, however, you should think of your application as an island that needs to connect to other applications to succeed. Unfortunately, the sea is treacherous and is not always very consistent, similar to the networks you use to connect your application to the world.\r\n\r\nWe will explore some techniques and libraries in the Python ecosystem used to make your life easier while dealing with external services. From asynchronicity, caching, testing, and building abstractions on top of the APIs you consume, you will definitely learn some strategies to build your connected application gracefully, and avoid those pesky 2 AM errors that keep you awake.", "description": "This talk will explore best practices for distributed programming in Python, and how to solve some of the more common issues when dealing with external systems. We will be exploring a few techniques that can help your system be reliable and available, even if your external services aren't.\r\n\r\nOutline:\r\n\r\nAgenda:\r\n- Introduction - 2 min\r\n- The problems around distributed computing - 3 min\r\n- Caching - 5 min\r\n- Asynchronous task queuing - 5 min\r\n- Building API abstractions - 5 min\r\n- Testing - 5 min\r\n- Closing statements - 5 min", "recording_license": "", "do_not_record": false, "persons": [{"id": 24842, "code": "AZX839", "public_name": "Luis Fernando Alvarez", "biography": "Hi! I\u2019m Luis Fernando Alvarez, Engineering Manager at Stack Builders. I\u2019ve been working in the Software development industry for more than 15 years, both as a full-stack developer and tech lead in multiple projects, with a varied number of technologies and languages. I\u2019m passionate about Software Development and helping younger engineers grow. I\u2019m also a multi-instrumentalist musician, videogame lover, and cooking enthusiast.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25654, "guid": "05720fe4-ab95-5f57-b82d-bade90dce226", "logo": "", "date": "2023-04-19T14:00:00+02:00", "start": "14:00", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-25654-you-ve-got-trust-issues-we-ve-got-solutions-differential-privacy", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/G9TATQ/", "title": "You've got trust issues, we've got solutions: Differential Privacy", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "As we are in an era of big data where large groups of information are assimilated and analyzed, for insights into human behavior, data privacy has become a hot topic. Since there is a lot of private information which once leaked can be misused, all data cannot be released for research. This talk aims to discuss Differential Privacy, a cutting-edge technique of cybersecurity that claims to preserve an individual\u2019s privacy, how it is employed to minimize the risks with private data, its applications in various domains, and how Python eases the task of employing it in our models with PyDP.", "description": "Since there is a lot of private information which once leaked can be misused, how should privacy be protected? One might think that simply making personally identifiable fields in the dataset anonymous might be useful, but this can lead to the entire dataset becoming useless and not fit for analysis. And research has proven that by statistically studying both the datasets, private information can easily be re-extracted!\r\n \r\nThe session will start with a brief on the current standards of privacy, and the possible risks of handling customer data. This will lay the foundation for introducing Differential Privacy, a cutting-edge technique of cybersecurity that claims to preserve an individual\u2019s privacy, by manipulating data in such a way as to not render it useless for data analysis. Developers will gain an insight into the concept of Differential Privacy, how it is employed to minimize the risks associated with private data, its practical applications in various domains, and how Python eases the task of employing it in our models with PyDP. As the talk progresses, a walkthrough of a real-life practical example, along with a nifty visualization will acquaint the audience with PyDP, and how differential private results come out to be in approximation to what unfiltered data would have provided.", "recording_license": "", "do_not_record": false, "persons": [{"id": 24435, "code": "M9AJZ9", "public_name": "Sarthika Dhawan", "biography": "Sarthika Dhawan is a Software Engineer at Microsoft, where she has worked with a variety of technologies and teams. She is actively involved in the software development and research community, and has authored and presented a conference paper at IJCAI 2019. She is an ACM-W and AICTE-INAE scholarship recipient and has attended various conferences like GHCI and IJCAI. She has given technical talks, provided mentorships and volunteered as a tutor at NGOs to educate economically less fortunate kids in various disciplines. She has participated in multiple hackathons as she believes that\u2019s an amazing way to keep yourself involved and updated.", "answers": []}, {"id": 24386, "code": "VKQTEE", "public_name": "Vikram Waradpande", "biography": "Vikram is a Computer Science master's student at Columbia University with a focus on Machine Learning. He completed his bachelor's in Computer Science and Mathematics in India from BITS Pilani. Before Columbia, he was a part of the engineering and strategy team at Goldman Sachs, where he built scalable and efficient trading tools. He has also had research experience working at TU Leibniz, Germany in the area of Reinforcement Learning and Parallel Programming. He presented his research at the International Conference on Mining and Learning on Graphs in 2020 in Vienna, Austria. He was a teaching assistant for three courses during his academic career, which involved conducting seminars (NumPy, Pytorch, etc.), organizing technical meetings and organizing research fairs. He has also tutored for the website 'Chegg' for more than two years where he taught Math and Programming to high school and university students.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26242, "guid": "363d85e0-3a4f-5936-804c-a9160c6846de", "logo": "", "date": "2023-04-19T14:35:00+02:00", "start": "14:35", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-26242-introduction-to-async-programming", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/PPXA79/", "title": "Introduction to Async programming", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "Asynchronous programming is a type of parallel programming in which a unit of work is allowed to run separately from the primary application thread. Post execution, it notifies the main thread about the completion or failure of the worker thread. There are numerous benefits to using it, such as improved application performance, enhanced responsiveness, and effective usage of CPU.\r\n\r\nAsynchronicity seems to be a big reason why Node.js is so popular for server-side programming. Most of the code we write, especially in heavy IO applications like websites, depends on external resources. This could be anything from a remote database POST API call. As soon as you ask for any of these resources, your code is waiting around for process completion with nothing to do. With asynchronous programming, you allow your code to handle other tasks while waiting for these other resources to respond.\r\n\r\nIn this session, we are going to talk about asynchronous programming in Python. Its benefits and multiple ways to implement it.", "description": "How Do We Implement Asynchronicity in Python?\r\n1. Multiple Processes: The most obvious way is to use multiple processes. From the terminal, you can start multiple scripts, and then all the scripts are going to run independently or at the same time. The operating system that's underneath will take care of sharing your CPU resources among all those instances. Alternatively you can use the multiprocessing library which supports spawning processes as shown in the example below.\r\n2. Multiple Threads: The next way to run multiple things at once is to use threads. A thread is a line of execution, pretty much like a process, but you can have multiple threads in the context of one process and they all share access to common resources. But because of this, it's difficult to write a threading code. And again, the operating system is doing all the heavy lifting on sharing the CPU, but the global interpreter lock (GIL) allows only one thread to run Python code at a given time even when you have multiple threads running code. So, In CPython, the GIL prevents multi-core concurrency. Basically, you\u2019re running in a single core even though you may have two or four or more.\r\n3. Coroutines using yield: Coroutines are generalizations of subroutines. They are used for cooperative multitasking where a process voluntarily yield (gives away) control periodically or when idle in order to enable multiple applications to be run simultaneously.\r\n4. Asynchronous Programming: The fourth way is asynchronous programming, where the OS is not participating is asyncio. Asyncio is the new concurrency module introduced in Python 3.4. It is designed to use coroutines and futures to simplify asynchronous code and make it almost as readable as synchronous code as there are no callbacks.\r\n5. Using Redis and Redis Queue(RQ): Using asyncio and aiohttp may not always be in an option especially if you are using older versions of python. Also, there will be scenarios when you would want to distribute your tasks across different servers. In that case, we can leverage RQ (Redis Queue). It is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis - a key/value data store.\r\n\r\nA practical definition of Async is that it's a style of concurrent programming in which tasks release the CPU during waiting periods, so that other tasks can use it. In Python, there are several ways to achieve concurrency, based on our requirement, code flow, data manipulation, architecture design, and use cases we can select any of these methods.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25790, "code": "TSAEPG", "public_name": "Dishant Sethi", "biography": "I am a software engineer who loves being a problem solver. I am equipped with experience in web development, Cloud Engineering, and DevOps. I am self-motivated and able to work independently with minimal supervision. My experience in software development from open-source contributions, internships, and personal projects gives me confidence in my ability to be fit for the role.\r\n\r\nI am passionate about supporting the education system and meeting new people. I believe in free and open information/internet access for everyone.\r\n\r\nInterested in opportunities to contribute as:\r\n\u2666 Web Developer\r\n\u2666 System / Cloud Engineer\r\n\u2666 DevOps\r\n\r\nTalk to me about:\r\n\u2666 Web Development Practices\r\n\u2666 Free and Open Source Software (FOSS) Community\r\n\u2666 Starting Software Engineering Journey", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 30911, "guid": "6d0407bd-d38d-5f05-8178-8cb19e2a86be", "logo": "", "date": "2023-04-19T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B07-B08", "slug": "pyconde-pydata-berlin-2023-30911-rethinking-codes-of-conduct", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/AWBLKN/", "title": "Rethinking codes of conduct", "subtitle": "", "track": "General: Community, Diversity, Career, Life and everything else", "type": "Talk", "language": "en", "abstract": "Did you know that the Python Software Foundation Code of Conduct is turning 10 years old in 2023? It was voted in as they felt they were \u201cunbalanced and not seeing the true spectrum of the greater community\u201d. \r\nWhy is that a big thing? Come to my talk and find out!", "description": "Did you know that the Python Software Foundation Code of Conduct is turning 10 years old in 2023? It was voted in as they felt they were \u201cunbalanced and not seeing the true spectrum of the greater community\u201d. And thought that with time they could \u201cadvance towards a more diverse representation.\u201d[1]\r\n\r\nWhy is that a big thing? Codes of conduct are an important part of any community, outlining the values of the community. They establish clear guidelines for acceptable behavior and help to create a safe and inclusive environment in the community. This can prevent discrimination and promote equal opportunities for all members.\r\n\r\nIn this talk, we will explore the role of code of conduct in communities, it\u2019s history in the PSF, and discuss strategies for rethinking what it means to have and enforce a Code of Conduct. We will look at the existing challenges that the Python communities face when implementing codes of conduct and talk about possible solutions. How does it look like when it works well and when it doesn\u2019t?\r\n\r\nAs an essential part of any open source project, reflecting on these guidelines can help to ensure that the projects are successful and sustainable in the long term. Thinking back to Python, which also turns 20 in 2023: \u201cPython got to where it is by being open, and it\u2019ll continue to prosper by remaining open\u201d. It\u2019s important we continue this mission, after all, one of the things many people love about Python is the community.\r\n\r\n[1] https://pyfound.blogspot.com/2013/06/announcing-code-of-conduct-for-use-by.html", "recording_license": "", "do_not_record": false, "persons": [{"id": 1899, "code": "NMACLQ", "public_name": "Tereza Iofciu", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}], "B05-B06": [{"id": 26307, "guid": "a9f0450c-deec-5f2c-8896-fcb9a1a0564d", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "00:45", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26307-advanced-visual-search-engine-with-self-supervised-learning-ssl-representations-and-milvus", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/TCWCVV/", "title": "Advanced Visual Search Engine with Self-Supervised Learning (SSL) Representations and Milvus", "subtitle": "", "track": "PyData: Computer Vision", "type": "Talk (long)", "language": "en", "abstract": "Image retrieval is the process of searching for images in a large database that are similar to one or more query images. A classical approach is to transform the database images and the query images into embeddings via a feature extractor (e.g., a CNN or a ViT), so that they can be compared via a distance metric. Self-supervised learning (SSL) can be used to train a feature extractor without the need for expensive and time-consuming labeled training data. We will use DINO's SSL method to build a feature extractor and Milvus, an open-source vector database built for evolutionary similarity search, to index image representation vectors for efficient retrieval. We will compare the SSL approach with supervised and pre-trained feature extractors.", "description": "[Image Retrieval](https://en.wikipedia.org/wiki/Image_retrieval) consists in searching in a large database for the most similar images to one or more query images. It has many applications in various fields, e.g., to validate whether a person's photo is contained in your database of people's photos; to build a visual recommendation system; or to create a video deduplication system.\r\n\r\nHuge progress in Computer Vision in the deep learning era highlighted [Content-based Image Retrieval](https://en.wikipedia.org/wiki/Content-based_image_retrieval) (CBIR) techniques that use the image contents (features, colors, shapes, etc) rather than metadata (keywords, tags). This gets rid of time-consuming, costly and error-prone human annotations to produce the metadata.\r\n\r\nA classic CBIR approach consists of three steps:\r\n\r\n1. A deep neural network called **the feature extractor** (typically a CNN, or a [ViT](https://arxiv.org/pdf/2010.11929.pdf)) computes a representation of each image of the database in the form of an embedding vector.\r\n2. The same *feature extractor* is used to compute an embedding of a query image.\r\n3. The search is performed by retrieving the **closest** representations in this vector space using a distance metric (cosine, L1, or more complex ones).\r\n\r\nThereafter, two main challenges arise:\r\n\r\n- **Quality of image representations** - the embeddings should capture the visual features that are relevant to your searches/tasks. For instance, if you intend to do face recognition, embeddings should encode eye/hair color, skin texture, nose position, etc. \r\nTraditionally, the feature extractor is trained in a supervised way. Therefore, the relevance of the representations hugely depends on 1) how close is the training dataset from the searched query images 2) the potential visual biases in the annotations (see a [famous example here](https://medium.com/hackernoon/dogs-wolves-data-science-and-why-machines-must-learn-like-humans-do-41c43bc7f982)).\r\n- **Speed of search in the representation space** - comparing each query image to every single image in the searched database in near real-time is challenging and expensive with large datasets.\r\n\r\nIn this talk, we will build a [Visual Search Engine](https://en.wikipedia.org/wiki/Visual_search_engine):\r\n\r\n- We will introduce **[Self-Supervised Learning](https://en.wikipedia.org/wiki/Self-supervised_learning) (SSL)** in the context of computer vision and the [data2vec](https://arxiv.org/pdf/2202.03555.pdf) approach. Labelling data can be a time-consuming and expensive process, especially if it requires specialized knowledge or expertise. SSL does not require labelled training data to learn good representations, hence it allows to lower the cost and time to build a model producing good representations for our visual search engine.\r\n- As a concrete example for this talk, we will use the [DINO](https://arxiv.org/pdf/2104.14294.pdf)'s SSL method to build a feature extractor.\r\n- We will compare the DINO feature extractor with supervised pre-trained feature extractors. We will show the main differences between the obtained representations: SSL ones are generally richer (more visual features are in the representation) whereas supervised learning introduces a natural semantic bias in the representations. In addition, we will present practical tools to understand the visual features encoded in the embeddings (activation maps, grad-cams, self-attention maps for transformers).\r\n- We will present [Milvus](https://milvus.io/), a vector database built for scalable similarity search: it\u2019s an open-source search engine tool (14.5k stars on Github) that is suitable for production use cases as it can be easily scaled and managed. Milvus uses [Approximate Nearest Neighbors (ANN) methods](https://milvus.io/docs/v2.0.x/index.md#Selecting-an-Index-Best-Suited-for-Your-Scenario) to build vector indexes that improve retrieval efficiency by sacrificing accuracy within an acceptable range.\r\n- We will use the Milvus Python API to index the image representation vectors: as a result, the images the most similar to a query image can be retrieved in a split second, even for datasets containing millions of vectors. \r\n\r\nBy the end of the session, participants will have learned how to build a Visual Search Engine using Milvus with pre-trained self-supervised and supervised models.", "recording_license": "", "do_not_record": false, "persons": [{"id": 14858, "code": "9LGUSS", "public_name": "Antoine Toubhans", "biography": "Python developer and Data-Scientist, I am Head of Science at Sicara since 2018.\r\n\r\nI am also the organizer of the [Paris Computer Vision Meetup](https://www.meetup.com/Meetup-Computer-Vision-Paris/).\r\n\r\nSpeaker at PyConUS22 and PyDataBerlin22.", "answers": []}, {"id": 25892, "code": "B893GM", "public_name": "No\u00e9 Achache", "biography": "No\u00e9 is a lead data scientist at Sicara, and worked on various AI and data engineering related projects.\r\nSpeaker at the [Paris Computer Vision Meetup](https://www.meetup.com/Meetup-Computer-Vision-Paris/).", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26326, "guid": "8e51bbb5-4391-5bac-9ebe-130eeb961fe7", "logo": "", "date": "2023-04-19T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26326-teaching-neural-networks-a-sense-of-geometry", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/3TH9UC/", "title": "Teaching Neural Networks a Sense of Geometry", "subtitle": "", "track": "PyData: Deep Learning", "type": "Talk", "language": "en", "abstract": "By taking neural networks back to the school bench and teaching them some elements of geometry and topology we can build algorithms that can reason about the shape of data. Surprisingly these methods can be useful not only for computer vision \u2013 to model input data such as images or point clouds through global, robust properties \u2013 but in a wide range of applications, such as evaluating and improving the learning of embeddings, or the distribution of samples originating from generative models. This is the promise of the emerging field of Topological Data Analysis (TDA) which we will introduce and review recent works at its intersection with machine learning. TDA can be seen as being part of the increasingly popular movement of Geometric Deep Learning which encourages us to go beyond seeing data only as vectors in Euclidean spaces and instead consider machine learning algorithms that encode other geometric priors. In the past couple of years TDA has started to take a step out of the academic bubble, to a large extent thanks to powerful Python libraries written as extensions to scikit-learn or PyTorch.", "description": "Researchers have hypothesised that a sense of geometry is something that sets the intelligence of humans apart from that of other animals. This intriguing fact motivates why geometric reasoning can be an interesting direction for AI.\r\nHow can we incorporate geometric concepts into deep learning? We can tap in to the mathematical fields of geometry and topology and see how methods in these fields can be adapted to be used in the context of data analysis and machine learning. This is the aim of Topological Data Analysis.\r\nStarting from hierarchical clustering, which many data scientists are familiar with, we gently introduce a method used in TDA, where we look at clustering of a data set at different thresholds and form a topological summary which represents the creation and destruction of clusters (which is an example of a topological feature) at different thresholds.\r\nWe then look at a few examples where these methods can be useful:\r\n- In neuroscience we can use these methods to model neuronal or glia trees, capturing properties of important branching structures and incorporating the invariances that these objects have.\r\n- In image segmentation we would like to teach a neural network to take the shape of the segmentation masks into consideration, where some of the classical loss functions can't account for these kind of global properties.\r\n- For dimensionality reduction, we can argue that minimising a reconstruction loss is not enough, instead we would like to somehow make sure that the shape of the original dataset and its dimensionality-reduced version are similar.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25853, "code": "UGR7MX", "public_name": "Jens Agerberg", "biography": "Jens is pursuing a PhD in Machine Learning and Topological Data Analysis at KTH Royal Institute of Technology in Stockholm, Sweden, while also working as a data scientist at Ericsson.\r\nHe believes that an important property that sets humans apart from other animals is that we have a sense of geometry and topology. Teaching computers a sense of geometrical recognition and reasoning is thus a promising direction if we want to develop more powerful AIs.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26419, "guid": "ad349b86-a5b5-53b5-9be5-622e9e1fbe48", "logo": "", "date": "2023-04-19T11:50:00+02:00", "start": "11:50", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26419-the-future-of-the-jupyter-notebook-interface", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/VXPFFP/", "title": "The future of the Jupyter Notebook interface", "subtitle": "", "track": "PyData: Jupyter", "type": "Talk", "language": "en", "abstract": "Jupyter Notebooks have been a widely popular tool for data science in recent years due to their ability to combine code, text, and visualizations in a single document. \r\n\r\nDespite its popularity, the core functionality and user experience of the Classic Jupyter Notebook interface has remained largely unchanged over the past years.\r\n\r\nLately the Jupyter Notebook project decided to base its next major version 7 on JupyterLab components and extensions, which means many JupyterLab features are also available to Jupyter Notebook users. \r\n\r\nIn this presentation, we will demo the new features coming in Jupyter Notebook version 7 and how they are relevant to existing users of the Classic Notebook.", "description": "Jupyter Notebook 7 is based on the JupyterLab codebase, but provides an equivalent user experience to the current (version 6) application. Notebook 7 keeps the document-centric user experience at its core, and brings many new features that were not previously available:\r\n\r\n- Debugger\r\n- Real-time collaboration\r\n- Theming and dark mode\r\n- Internationalization\r\n- Improved Web Content Accessibility Guidelines (WCAG) compliance\r\n- Support for many JupyterLab extensions, including Jupyter LSP (Language Server Protocol) for enhanced code completions\r\n- Performance improvements\r\n\r\nThis talk will be about demoing the new features coming to Notebook 7, and how uses of the Classic Notebook interface should approach.\r\n\r\nWe will also cover other aspects mentioned in the related Jupyter Enhancement Proposal, such as support for popular extensions and future developments: https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html", "recording_license": "", "do_not_record": false, "persons": [{"id": 2028, "code": "KA8ZHH", "public_name": "Jeremy Tuloup", "biography": "Jeremy Tuloup is a Technical Director at QuantStack and a Jupyter Distinguished Contributor. Maintainer and contributor of JupyterLab, JupyterLite, Jupyter Notebook, Voil\u00e0 Dashboards, and many projects within the Jupyter ecosystem.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 28625, "guid": "d219e20f-0410-5901-baea-a0b339ff9eb2", "logo": "", "date": "2023-04-19T12:25:00+02:00", "start": "12:25", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-28625-dynamic-pricing-at-flix", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/33XCUR/", "title": "Dynamic pricing at Flix", "subtitle": "", "track": "Sponsor", "type": "Sponsored Talk", "language": "en", "abstract": "In the talk we give a brief overview of how we use Dynamic Pricing to tune the prices for rides based on demand, time of purchase, unexpected events strike etc., and other criteria to fulfil our business requirements.", "description": "Dynamic pricing is more challenging in Flixbus compared to other travel companies as we do not discriminate prices based on various categories such as business, economy classes, which are often used in trains and airlines. In the talk, we describe the challenges faced and discuss how we designed innovative solutions to solve these challenges. \r\n\r\nThe main topic I want to present is how we implemented a real time pipeline to calculate the prices based on current demand. At the same time, how it\u2019s so reactive to changes for example, booking, route changes, etc. I will also present some of the efficient data structures we use to apply the changes very fast and efficient.", "recording_license": "", "do_not_record": false, "persons": [{"id": 16000, "code": "N39PEY", "public_name": "Amit Verma", "biography": "My name is Amit Verma, I have been working for Flixbus as Senior Data Engineer. I designed the dynamic pricing architecture which is currently being used in approximately 80% of market share. Before joining Flixbus, I worked in Cliqz: a Germany based search engine that was focused on user data privacy. Currently, this is used in brave search.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26490, "guid": "671ee696-ad7c-529a-8937-aded4da15599", "logo": "", "date": "2023-04-19T14:00:00+02:00", "start": "14:00", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26490-apache-arrow-connecting-and-accelerating-dataframe-libraries-across-the-pydata-ecosystem", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/H7ZCWK/", "title": "Apache Arrow: connecting and accelerating dataframe libraries across the PyData ecosystem", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "Apache Arrow is a multi-language toolbox for accelerated data interchange and in-memory processing, and is becoming the de facto standard for tabular data. This talk will give an overview of the recent developments both in Apache Arrow itself as how it is being adopted in the PyData ecosystem (and beyond) and can improve your day-to-day data analytics workflows.", "description": "The Apache Arrow (https://arrow.apache.org/) project specifies a standardized language-independent columnar memory format for tabular data. It enables shared computational libraries, zero-copy shared memory, efficient (inter-process) communication without serialization overhead, etc. Nowadays, Apache Arrow is supported by many programming languages and projects, and is becoming the de facto standard for tabular data.\r\n\r\nBut what does that mean in practice? There is a growing set of tools in the Python bindings, PyArrow, and a growing number of projects that use (Py)Arrow to accelerate data interchange and actual data processing. This talk will give an overview of the recent developments both in Apache Arrow itself as how it is being adopted in the PyData ecosystem (and beyond) and can improve your day-to-day data analytics workflows.", "recording_license": "", "do_not_record": false, "persons": [{"id": 75, "code": "7VUXWM", "public_name": "Joris Van den Bossche", "biography": "I am a core contributor to Pandas and Apache Arrow, and maintainer of GeoPandas. I did a PhD at Ghent University and VITO in air quality research and worked at the Paris-Saclay Center for Data Science. Currently, I work at Voltron Data, contributing to Apache Arrow, and am a freelance teacher of python (pandas) at Ghent University.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26337, "guid": "0e829ee6-4ebb-523e-9a1b-f6d924ff9378", "logo": "", "date": "2023-04-19T14:35:00+02:00", "start": "14:35", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-26337-the-beauty-of-zarr", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/JY3R3Z/", "title": "The Beauty of Zarr", "subtitle": "", "track": "PyData: PyData & Scientific Libraries Stack", "type": "Talk", "language": "en", "abstract": "In this talk, I\u2019d be talking about [Zarr](https://zarr.dev/), an open-source data format for storing chunked, compressed N-dimensional arrays. This talk presents a systematic approach to understanding and implementing Zarr by showing how it works, the need for using it, and a hands-on session at the end. Zarr is based on an open [technical specification](https://zarr.readthedocs.io/en/stable/spec/v2.html), making implementations across several languages possible. I\u2019d mainly talk about [Zarr\u2019s Python](https://github.com/zarr-developers/zarr-python) implementation and show how it beautifully interoperates with the existing libraries in the PyData stack.", "description": "[Zarr](https://zarr.dev/) is a data format for storing chunked, compressed N-dimensional arrays. Zarr is based on open-source [technical specification](https://zarr.readthedocs.io/en/stable/spec/v2.html) and has [implementations](https://github.com/zarr-developers/zarr_implementations) in several languages, with [Zarr-Python](https://github.com/zarr-developers/zarr-python) being the most used. Zarr is [NumFOCUS\u2019s sponsored project](https://numfocus.org/sponsored-projects) and is under their umbrella.\r\n\r\n### Outline:\r\n\r\n\r\nFirst, I\u2019d be talking about:\r\n\r\n### What\u2019s, Why\u2019s, and How\u2019s of Zarr (15 mins.)\r\n\r\n- How does Zarr work?\r\n    - Talking about the motivation and functionality of Zarr\r\n- What\u2019s the need for using Zarr?\r\n    - When, where and why to use it?\r\n- Pluggable compressors and file-storage\r\n    - Talking about several compressors and file-storage systems available in Zarr\r\n- Managing(selection, resizing, writing, reading) chunked arrays using Zarr functions\r\n    - Using inbuilt functions to manage compressed chunks\r\n- How is Zarr different when compared to other storage formats?\r\n    - Talking briefly about technical specification, which allows Zarr to have implementations in several languages\r\n    - Pros and cons when compared to other storage formats\r\n- Zarr community\r\n    - What is the Zarr community, and how do we do things?\r\n\r\n\r\nThen, I\u2019d be doing a hands-on session, which would cover the following:\r\n\r\n### Hands-on (10 mins.)\r\n\r\n- Creating and using Zarr arrays\r\n    - Using inbuilt functions to create Zarr arrays and reading and writing data to it\r\n- Looking under the hood\r\n    - Use store functions to explain how your Zarr data is stored\r\n- Consolidating metadata\r\n    - Consolidating the metadata for an entire group into a single object\r\n- Writing and reading from Cloud object storage\r\n    - Using S3/GCS/Azure to create Zarr arrays and write data to it\r\n- Showing how Zarr interoperates with the PyData stack\r\n    - How Zarr interoperates with the PyData stack(NumPy, Dask and Xarray) and how you can write data to your Zarr chunks at incredibly high speed in parallel using Dask\r\n\r\n\r\nI\u2019d be closing the talk by: \r\n\r\n### Conclusion(5 mins.)\r\n\r\n- Key takeaway\r\n- How can you contribute to Zarr?\r\n- QnA\r\n\r\nThis talk aims to address the audience who works with large amounts of data and are in search of a data format which is transparent, easy to use and friendly to the environment. Zarr is also reasonably used in bioimaging, geospatial and research communities. So, Zarr is your one-stop solution if you\u2019re from a community or an organisation dealing with high-volume data. Also, anyone curious and wants to learn about Zarr and how to use it is most welcome.\r\n\r\nThe tone of the talk is set to be informative, along with a hands-on session. Also, I\u2019m happy to adjust the style according to the audience in the room.\r\n\r\nIntermediate knowledge of Python and NumPy arrays is required for the attendees to attend this talk.\r\n\r\n### After this talk, you\u2019d learn:\r\n\r\n- Basic use cases for Zarr and how to use it\r\n- Understand the basics of data storage in Zarr\r\n- Understand the basics of compressors and file-storage systems in Zarr\r\n- Take a better and more informed decision on what data format to use for your data", "recording_license": "", "do_not_record": false, "persons": [{"id": 15587, "code": "A7ACFE", "public_name": "Sanket Verma", "biography": "Sanket is a data scientist based out of New Delhi, India. He likes to build data science tools and products and has worked with startups, government and organisations. He loves building community and bringing everyone together and is Chair of PyData Delhi and PyData Global.  Currently, he's taking care of the community and OSS at Zarr as their Community Manager.\r\nWhen he\u2019s not working, he likes to play the violin and computer games and sometimes thinks of saving the world!", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25433, "guid": "db6815ad-e89b-566a-a98a-36e5f3c42aa5", "logo": "", "date": "2023-04-19T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "B05-B06", "slug": "pyconde-pydata-berlin-2023-25433-fear-the-mutants-love-the-mutants-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/AQAJDH/", "title": "Fear the mutants. Love the mutants.", "subtitle": "", "track": "PyCon: Testing", "type": "Talk", "language": "en", "abstract": "Developers often use code coverage as a target, which makes it a bad measure of test quality.\r\n\r\nMutation testing changes the game: create mutant versions of your code that break your tests, and you'll quickly start to write better tests!\r\n\r\nCome and learn to use it as part of your CI/CD process. I promise, you'll never look at penguins the same way again!", "description": "Code coverage (the percentage of your code tested by your tests) is a great metric. However, coverage doesn\u2019t tell you how good your tests are at picking up changes to your codebase - if your tests aren\u2019t well-designed, changes can pass your unit tests but break production.\r\n\r\nMutation testing is a great (and massively underrated) way to quantify how much you can trust your tests. Mutation tests work by changing your code in subtle ways, then applying your unit tests to these new, \"mutant\" versions of your code. If your tests fail, great! If they pass\u2026 that\u2019s a change that might cause a bug in production.\r\n\r\nIn this talk, I\u2019ll show you how to get started with mutation testing and how to integrate it into your CI/CD pipeline. After the session, you\u2019ll be ready to use mutation testing with wild abandon. Soon, catching mutant code will be a routine part of your release engineering process, and you\u2019ll never look at penguins the same way again!", "recording_license": "", "do_not_record": false, "persons": [{"id": 24521, "code": "JQ3MRZ", "public_name": "Max Kahan", "biography": "I'm a Python Developer Advocate and Software Engineer at Vonage (ex-IBM). I'm interested in communications APIs, machine learning, open-source, developer experience and dancing! My training is in Physics, and now I use my problem-solving skills daily, working on open-source projects and finding ways to make developers\u2019 lives better.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A1": [{"id": 26452, "guid": "0ec81cbe-f848-545f-9006-15354d99dd4e", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "00:45", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26452-why-gpu-clusters-don-t-need-to-go-brrr-leverage-compound-sparsity-to-achieve-the-fastest-inference-performance-on-cpus", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/7NW7JC/", "title": "Why GPU Clusters Don't Need to Go Brrr? Leverage Compound Sparsity to Achieve the Fastest Inference Performance on CPUs", "subtitle": "", "track": "PyData: Deep Learning", "type": "Talk (long)", "language": "en", "abstract": "Forget specialized hardware. Get GPU-class performance on your commodity CPUs with compound sparsity and sparsity-aware inference execution.\r\nThis talk will demonstrate the power of compound sparsity for model compression and inference speedup for NLP and CV domains, with a special focus on the recently popular Large Language Models. The combination of structured + unstructured pruning (to 90%+ sparsity), quantization, and knowledge distillation can be used to create models that run an order of magnitude faster than their dense counterparts, without a noticeable drop in accuracy. The session participants will learn the theory behind compound sparsity, state-of-the-art techniques, and how to apply it in practice using the Neural Magic platform.", "description": "By intelligently applying SOTA compound sparsity techniques, we can remove 95%+ of the weights and reduce the remaining 5% to 8-bit precision on modern models such as BERT, while maintaining 99%+ of their baseline accuracy. In this talk, we\u2019ll be covering how we can build up to this extreme sparsity and how to harness it to achieve an order of magnitude speedup for CPU inference.\r\n\r\nThis talk will focus on the success story of utilizing sparsity to run fast inference of modern neural networks on CPUs. We will focus on the popular Large Language Models with the goal of learning how the recent state-of-the-art in model compression can help to dramatically lower the computational budget when it comes to model inference.\r\n\r\nToday\u2019s ML hardware acceleration is headed towards chips that apply a petaflop of compute to a cell phone-size memory. Our brains, on the other hand, are biologically the equivalent of applying a cell phone of compute to a petabyte of memory. In this sense, the direction being taken by hardware designers is the opposite of that proven by nature. Why? Simply because we don\u2019t know the algorithms nature uses.\r\nGPUs bring data in and out quickly, but have little locality of reference because of their small caches. They are geared towards applying a lot of compute to little data, not little compute to a lot of data. The networks are designed to run on them full layer after full layer in order to saturate their computational pipeline. \r\nCPUs, on the other hand, have large, much faster caches than GPUs, and have an abundance of memory (terabytes). A typical CPU server can have memory equivalent to tens or even hundreds of GPUs. CPUs are perfect for a brain-like ML world in which parts of an extremely large network are executed piecemeal, as needed.\r\n\r\nThis is the problem Neural Magic set out to solve and the perspective which led to the creation of DeepSparse, a custom computational engine designed to mimic, on commodity hardware, the way brains compute. It uses neural network sparsity combined with the locality of communication by utilizing the CPU\u2019s large fast caches and its very large memory.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25939, "code": "KTKLN8", "public_name": "Damian Bogunowicz", "biography": "Engineer, roboticist, software developer, and problem solver. Previous experience in autonomous driving (Argo AI), AI in industrial robotics (Arrival), and building machines that build machines (Tesla). Currently working in Neural Magic, focusing on the sparse future of AI computation.\r\nWorks towards unlocking creative and economic potential with intelligent robotics while avoiding the uprising of sentient machines.\"", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26302, "guid": "6cdfd8f8-1e36-52fa-9b47-b609de372bb0", "logo": "", "date": "2023-04-19T10:50:00+02:00", "start": "10:50", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26302-haystack-for-climate-q-a", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/SVXFP8/", "title": "Haystack for climate Q/A", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Talk", "language": "en", "abstract": "How can NLP and Haystack help answer sustainability questions and fight climate change? In this talk we walkthrough our experience using Haystack to build Question Answering Models for the climate change and sustainability domain. We discuss how we did it, some of the challenges we faced, and what we learnt along the way!", "description": "Haystack is a framework that enables you to build powerful and production-ready pipelines for different search use cases. You can use the State-of-the-Art NLP models in Haystack to provide unique search experiences and allow your users to query in natural language.  It is built on a modular fashion so that you can combine the best technology from other open-source projects like Transformers, Elasticsearch etc.\r\nWe use the Haystack pipeline to build Question Answering systems to answer domain specific question for climate change and sustainability topics.\r\nWe would like to talk about the challenges we faced, how we do it and how using haystack can in help companies do quicker POCs and eventually take it to production.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25837, "code": "KMSCZT", "public_name": "Vibha Vikram Rao", "biography": "Hi,\r\nI am Vibha Vikram Rao.\r\nI currently work as a (Senior)ML engineer at Climate Tech Startup --Briink based out of Berlin.\r\nThe reason I got into NLP was with the hope that someday every child would have access to an amazing tool which  would be able to read books and would be able to explain it to the child even if they don't have access to good teachers.\r\nSo I started my NLP journey with text Summarization systems.\r\nI have a total of around 4 years of experience in Applied NLP.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26431, "guid": "ca28a351-5075-5127-8162-30f9c5d97131", "logo": "", "date": "2023-04-19T11:50:00+02:00", "start": "11:50", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26431-grokking-anchors-uncovering-what-a-machine-learning-model-relies-on", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/QUAXG3/", "title": "Grokking Anchors: Uncovering What a Machine-Learning Model Relies On", "subtitle": "", "track": "PyData: Machine Learning & Stats", "type": "Talk", "language": "en", "abstract": "Assessing the robustness of models is an essential step in developing machine-learning systems. To determine if a model is sound, it often helps to know which and how many input features its output hinges on. This talk introduces the fundamentals of \u201canchor\u201d explanations that aim to provide that information.", "description": "Many data scientists are familiar with algorithms like Integrated Gradients, SHAP, or LIME that determine the importance of input features. But that\u2019s not always the information we need to determine whether a model\u2019s output is sound. Is there a specific feature value that will make or break the decision? Does the outcome solely depend on artifacts in an image? These questions require a different explanation method.\r\n\r\nFirst introduced in 2018, \u201canchors\u201d are a model-agnostic method to uncover what parts of the input a machine-learning model's output hinges on. Their computation is based on a search-based approach that can be applied to different modalities such as image, text, and tabular data.\r\n\r\nIn this talk, to truly grok the concept of anchor explanations, we will implement a basic anchor algorithm from scratch. Starting with nothing but a text document and a machine learning model, we will create a sampling, encoding, and search component and finally compute an anchor.\r\n\r\nNo knowledge of machine learning is required to follow this talk. Aside from familiarity with the basics of `numpy` arrays, all you need is your curiosity.", "recording_license": "", "do_not_record": false, "persons": [{"id": 21175, "code": "H8CCWU", "public_name": "KIlian Kluge", "biography": "My journey into Python started in a physics research lab, where I discovered the merits of loose coupling and adherence to standards the hard way. I like automated testing, concise documentation, and hunting complex bugs.\r\n\r\nI completed a PhD on the design of human-AI interactions and now work to use Explainable AI to open up new areas of application for AI systems.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26271, "guid": "699eed32-b3bd-5578-b98c-6e9cf5d96d7b", "logo": "", "date": "2023-04-19T12:25:00+02:00", "start": "12:25", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26271-maximizing-efficiency-and-scalability-in-open-source-mlops-a-step-by-step-approach", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/CTKC7B/", "title": "Maximizing Efficiency and Scalability in Open-Source MLOps: A Step-by-Step Approach", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "This talk presents a novel approach to MLOps that combines the benefits of open-source technologies with the power and cost-effectiveness of cloud computing platforms. By using tools such as Terraform, MLflow, and Feast, we demonstrate how to build a scalable and maintainable ML system on the cloud that is accessible to ML Engineers and Data Scientists. Our approach leverages cloud managed services for the entire ML lifecycle, reducing the complexity and overhead of maintenance and eliminating the vendor lock-in and additional costs associated with managed MLOps SaaS services. This innovative approach to MLOps allows organizations to take full advantage of the potential of machine learning while minimizing cost and complexity.", "description": "Building a machine learning (ML) system on a cloud platform can be a challenging and time-consuming task, especially when it comes to selecting the right tools and technologies. In this talk, we will present a comprehensive solution for building scalable and maintainable ML systems on the cloud using open source technologies like MLFlow, Feast, and Terraform. \r\n\r\nMLFlow is a powerful open source platform that simplifies the end-to-end ML lifecycle, including experimentation, reproducibility, and deployment. It allows you to track and compare different runs of your ML models and deploy them to various environments, such as production or staging, with ease. Feast is an innovative open source feature store that enables you to store and serve features for training, serving, and evaluating ML models. It integrates seamlessly with MLFlow, enabling you to track feature versions and dependencies, and deploy feature sets to different environments. Terraform is a widely-used open source infrastructure as code (IaC) tool that enables you to define and manage your cloud resources in a declarative manner. It allows you to automate the provisioning and management of your ML infrastructure, such as compute clusters, databases, and message brokers, saving you time and effort. \r\n\r\nIn this talk, we will demonstrate how these open source technologies can be used together to build an ML system on the cloud and discuss the benefits and trade-offs of using them. We will also share best practices and lessons learned from our own experiences building ML systems on the cloud, providing valuable insights and guidance for attendees looking to do the same.", "recording_license": "", "do_not_record": false, "persons": [{"id": 15804, "code": "RHCRMB", "public_name": "Paul Elvers", "biography": "Dr. Paul Elvers is Head of AI/Data Science at Datadrivers, an IT Consulting Company in Hamburg. He graduated in Systematic Musicology & worked as a Research Fellow at the Max-Planck-Institute for empirical Aesthetics before transitioning into Data Science. He has worked for companies like Tchibo and Smartclip, before joining Datadrivers in 2021.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26379, "guid": "38d0b4c8-f086-5022-b7ec-c7a0f7b3edd4", "logo": "", "date": "2023-04-19T14:00:00+02:00", "start": "14:00", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26379-machine-learning-lifecycle-for-nlp-classification-in-e-commerce", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/8VYHKG/", "title": "Machine Learning Lifecycle for NLP Classification in E-Commerce", "subtitle": "", "track": "PyCon: DevOps & MLOps", "type": "Talk", "language": "en", "abstract": "Running machine learning models in a production environment brings its own challenges. In this talk we would like to present our solution of a machine learning lifecycle for the text-based cataloging classification system from idealo.de. We will share lessons learned and talk about our experiences during the lifecycle migration from a hosted cluster to a cloud solution within the last 3 years. In addition, we will outline how we embedded our ML components as part of the overall idealo.de processing architecture.", "description": "idealo.de offers a price comparison service for millions of products from a wide variety of categories. The automated classification of the offers is carried out using both traditional and deep learning-based approaches. Our machine learning components are part of a fully automated life cycle and process up to 500 million offers daily at peak times. \r\n\r\nIn addition to the enormous amount of data that we process, we particularly face the challenges of being online 24/7 while adapting to an ever-changing catalog structure. This requires a high level of reliability from our inference service and continuous automated retraining and model deployment. \r\n\r\nIn this talk we would like to share and present our view on MLOps: \r\n- How we integrate our CI/CD and continuous training pipelines with Github and AWS Sagemaker \r\n- How we migrate the lifecycle from a hosted cluster (running Kubernetes, Argo Workflows and ArgoCD) to the cloud (running AWS Sagemaker and Datalake). \r\n- How we monitor our models as well as data and performance indicators up to date and alert in case of disruptions \r\n- How we embed the classifiers in an event-driven heterogeneous software architecture (based on Kotlin and Python). \r\n\r\nAnd share lessons learned on: \r\n- How we keep reliability high while deploying, updating, and scaling our classification inference services \r\n- How we meet a valid compromise between performance and cost requirements.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25757, "code": "3CDBQQ", "public_name": "Gunar Maiwald", "biography": "Gunar Maiwald has a background in Computer Science. For the last 3 years he worked as an ML engineer at idealo.de. His professional programming path led him from Perl via TypeScript to Python.", "answers": []}, {"id": 27140, "code": "UXME8R", "public_name": "Tobias Senst", "biography": "Tobias Senst is a Senior Machine Learning Engineer at idealo internet GmbH. Tobias Senst received his PhD in 2019 from the Technische Universit\u00e4t Berlin under the supervision of Prof. Thomas Sikora. He has more than 10 years of experience in Computer Vision and Video Analytics research. \r\n\r\nAt idealo, he switched from the world of images and videos to Natural Language Processing and is responsible for the operation and development of machine learning models in a productive environment.\r\n\r\n* LinkedIn: https://linkedin.com/in/tobias-senst-08090b192\r\n* Github: https://github.com/tsenst", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26351, "guid": "680d7b05-9064-5fc5-abb2-c5c17601e886", "logo": "", "date": "2023-04-19T14:35:00+02:00", "start": "14:35", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26351-cloud-infrastructure-from-python-code-how-far-could-we-go-", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/RQ3MWN/", "title": "Cloud Infrastructure From Python Code: How Far Could We Go?", "subtitle": "", "track": "General: Infrastructure - Hardware & Cloud", "type": "Talk", "language": "en", "abstract": "Discover how Infrastructure From Code (IfC) can revolutionize Cloud DevOps automation by generating cloud deployment templates directly from Python code. Learn how this technology empowers Python developers to easily deploy and operate cost-effective, secure, reliable, and sustainable cloud software. Join us to explore the strategic potential of IfC.", "description": "## Audience\r\n\r\nThe talk is a call for action towards the whole Python community to take an active part in unlocking full Python potential as a truly cloud-native programming language by adapting its runtime and compiler to work optimally with cloud resources.\r\n\r\n## Why SDK Programming and Infrastructure as Code are not enough anymore?\r\n\r\nDeveloping cloud software using cloud SDK combined with deployment automation using Infrastructure as Code templates has some serious limitations. The both SDK and IaC are at realively low level, require special expertise which takes time to acquire, are disconnected from each other and too often prepared by separate enigineering teams. Applying SDK+IaC to multiple test, staging, and production environments can exacerbate complexity and size issues. As a result, there is a need for a more efficient and automated approach to cloud infrastructure management that integrates tightly with application code.\r\n\r\n## What is Infrastructure From Code?\r\n\r\nInfrastructure from Code (IfC) is a newer and more advanced approach than IaC. It interprets mainstream programming language code and automatically generates the specifications needed to configure a cloud environment. Advanced solutions like ServerlessCloud, Ampt, and Nitric have been proposed for the TypeScript ecosystem. This talk will explore the current state of IfC for Python, its potential, and what needs to be done to make Python a truly cloud-native programming language.\r\n\r\n## Talk Outline\r\n\r\n1. Infrastructure from Python Code (PyIfC) Mission\r\n2. The Challenges of SDK programming combined with Infrastructure as Code (IaC)\r\n3. The PyIfC Approach: How It Works and Its Benefits\r\n4. Sample Code and Demo\r\n5. A Closer Look at PyIfC's Inner Workings\r\n6. Overcoming Deployment Location Optimization and Sustainability Challenges\r\n7. Overview of Existing Solutions Landscape for PyIfC\r\n8. Unleashing the Full Potential of Python ecosystem\r\n9. The Intersection of PyIfC and Domain-Driven Design\r\n10. Advancing PyIfC: What Needs to Be Done\r\n11. Key Takeaways and Next Steps\r\n12. Q&A\r\n\r\n# Tags\r\n\r\nCloud, Deployment, Automation, Serverless, Infrastructure as Code, IaC, Infrastructure From Code, IfC, Python", "recording_license": "", "do_not_record": false, "persons": [{"id": 17799, "code": "N7MTXA", "public_name": "Asher Sterkin", "biography": "Asher Sterkin is a 40-year industry veteran specializing in software architecture and technology. He currently serves as General Manager and Head of Engineering of BlackSwan Technologies\u2019 BST LABS, which is developing the Cloud AI Operating System (www.caios.io), cloud infrastructure that incorporates Infrastructure from Code. Prior to this role, Asher served as a Distinguished Engineer at Cisco.", "answers": []}, {"id": 17782, "code": "PTM7K9", "public_name": "Etzik Bega", "biography": null, "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26129, "guid": "393adc26-69c3-5c1e-8799-b53357cba663", "logo": "", "date": "2023-04-19T15:10:00+02:00", "start": "15:10", "duration": "00:30", "room": "A1", "slug": "pyconde-pydata-berlin-2023-26129-great-security-is-one-question-away", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MTRFT3/", "title": "Great Security Is One Question Away", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Talk", "language": "en", "abstract": "After a decade of writing code, I joined the application security team. During the transition process, I discovered that there are many myths about security, and how difficult it is. Often devs choose to ignore it because they think that writing more secure code would take them ages. It is not true. Security doesn\u2019t have to be scary. From my talk, you will learn the most useful piece from the Application Security theory. It will be practical and not boring at all.", "description": "There are so many myths about security, and how difficult it is. Often devs choose to ignore it because they think that writing more secure code would take them ages. It is not true. Security doesn\u2019t have to be scary. In my talk, I share 5 tips that can almost immediately make a product more secure.\r\n\r\nAfter a decade of writing code, I joined the application security team. During the transition process, I discovered that there are a few pieces of security theory that would have made my life as a developer much more painless if I had known them before.\r\n- Always validate the input\r\n- Do not commit credentials into your repository\r\n- Use scanners to find vulnerabilities\r\n- Learn CIA triad - Confidentiality, Integrity and Availability can be a useful framework to develop a security mindset. This is a simple yet powerful piece of theory. It can be a base of threat modeling of a whole project but can also work on a level of a single user story.\r\n- When in doubt, ask your security team for help", "recording_license": "", "do_not_record": false, "persons": [{"id": 25713, "code": "G7JMYZ", "public_name": "Wiktoria Dalach", "biography": "Wiktoria Dalach is a Senior Software Developer, Security Engineer, a writer and a youtuber. She has been building apps for a decade. She has organized over 30 workshops for Webmuses, a community she co-founded in 2012. She's a RailsGirls mentor. Her interests focus on creativity, art and cybersecurity.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A03-A04": [{"id": 26352, "guid": "d2f2fd4e-6c3a-5d4a-b9c1-a7211421ff35", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-26352-create-interactive-jupyter-websites-with-jupyterlite", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/FZY9VV/", "title": "Create interactive Jupyter websites with JupyterLite", "subtitle": "", "track": "PyData: Jupyter", "type": "Tutorial", "language": "en", "abstract": "Jupyter notebooks are a popular tool for data science and scientific computing, allowing users to mix code, text, and multimedia in a single document. However, sharing Jupyter notebooks can be challenging, as they require installing a specific software environment to be viewed and executed.\r\n\r\nJupyterLite is a Jupyter distribution that runs entirely in the web browser without any server components. A significant benefit of this approach is the ease of deployment. With JupyterLite, the only requirement to provide a live computing environment is a collection of static assets. In this talk, we will show how you can create such static website and deploy it to your users.", "description": "We will cover the basics of JupyterLite, including how to use its command-line interface to generate and customize the appearance and behavior of your Jupyter website. This will be a guided walkthrough with step-by-steps instructions for adding content, extensions and configuration.\r\n\r\nBy the end of this tutorial, you will be able to create your own interactive Jupyter website using JupyterLite.\r\n\r\nOutline:\r\n\r\n- Introduction to Jupyter and JupyterLite\r\n- Examples of JupyterLite used for interactive documentation and educational content (NumPy, Try Jupyter, SymPy)\r\n- Step-by-step demo for creating a Jupyter website\r\n    - Quickstart with the demo repository\r\n    - Adding content: notebooks, files and static assets\r\n    - Adding extensions to the user interface\r\n    - Adding packages to the Python runtime\r\n    - Customization and custom settings\r\n- Deploy JupyterLite as a static website on GitHub Pages, Vercel or your own server\r\n- Conclusion and next steps for learning more about the Jupyter ecosystem\r\n\r\nThe tutorial will be based on resources already publicly available:\r\n\r\n- try JupyterLite in your browser: https://jupyterlite.github.io/demo/\r\n- the JupyterLite documentation: https://jupyterlite.readthedocs.io/en/latest/quickstart/deploy.html\r\n- the JupyterLite repositories: https://github.com/jupyterlite\r\n\r\nAt the end of the tutorial the attendees will have something very concrete to present and a functioning Jupyter website.", "recording_license": "", "do_not_record": false, "persons": [{"id": 2028, "code": "KA8ZHH", "public_name": "Jeremy Tuloup", "biography": "Jeremy Tuloup is a Technical Director at QuantStack and a Jupyter Distinguished Contributor. Maintainer and contributor of JupyterLab, JupyterLite, Jupyter Notebook, Voil\u00e0 Dashboards, and many projects within the Jupyter ecosystem.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 25857, "guid": "ef7d6de5-989a-5de4-98d6-1410b5c0d28a", "logo": "", "date": "2023-04-19T11:40:00+02:00", "start": "11:40", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-25857-most-of-you-don-t-need-spark-large-scale-data-management-on-a-budget-with-python", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/V9HBUU/", "title": "Most of you don't need Spark. Large-scale data management on a budget with Python", "subtitle": "", "track": "PyData: Data Handling", "type": "Tutorial", "language": "en", "abstract": "The Python data ecosystem has matured during the last decade and there are less and less reasons to rely only large batch process executed in a Spark cluster, but with every large ecosystem, putting together the key pieces of technology takes some effort. There are now better storage technologies, streaming execution engines, query planners, and low level compute libraries. And modern hardware is way more powerful than what you'd probably expect.  In this workshop we will explore some global-warming-reducing techniques to build more efficient data transformation pipelines in Python, and a little bit of Rust.", "description": "When one looks at the architecture diagram for the big data ecosystem of most corporations, there's a Spark cluster in the center. Even some of these corporations have adopted Spark as the \"de facto\" platform for ETL. If you have a Spark cluster, it's fine to use it, but maybe there are other ways to extract, transform, and load large volumes of data more efficiently and with less overhead. \r\n\r\nSome of the technologies that we'll cover are:\r\n\r\n* Duckdb. Probably the hottest piece of technology of this decade.\r\n* Polars.\r\n* Datafusion, and a little bit or Rust.\r\n* Microbatching.\r\n* Statistical tests.\r\n* We'll dive a little into what makes Parquet datasets so great.\r\n* Filter pushdown and predicate pushdown.\r\n* Overlapping communications and computation.\r\n\r\nWe'll work on a synthetic use case where we'll try to find find out if an online casino is trying to manipulate the roulette boards. To make things harder, we'll use an old and crappy low-power desktop PC with the equivalent computing power of a modern Raspberry PI to crunch around half a terabyte of data.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25346, "code": "ZNZJDP", "public_name": "Guillem Borrell", "biography": "PhD, MS, Aerospace Engineering. Previously researching on Turbulence Theory and Simulation. Now at BCG X helping clients take the most out of data and AI", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 30171, "guid": "05fa61d3-ce24-59b9-9795-e2cca00bd3c1", "logo": "", "date": "2023-04-19T14:10:00+02:00", "start": "14:10", "duration": "01:30", "room": "A03-A04", "slug": "pyconde-pydata-berlin-2023-30171-contributing-to-an-open-source-content-library-for-nlp", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/MECPWF/", "title": "Contributing to an open-source content library for NLP", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Tutorial", "language": "en", "abstract": "Bricks is an open-source content library for natural language processing, which provides the building blocks to quickly and easily enrich, transform or analyze text data for machine learning projects. For many Pythonistas, contributing to an open-source project seems scary and intimidating. In this tutorial, we offer a hands-on experience in which programmers and data scientists learn how to code their own building blocks and share their creations with the community with ease.", "description": "We will prepare some easy-to-use cases so that attendees with novice machine learning and NLP skills can participate in the session. A basic understanding of Python is required, but everyone who wants to learn more about machine learning, NLP, or open-source contributions is welcome.\r\n\r\nA brick is a modular piece of software that enriches, transforms, or analyzes text data for natural language processing, a sub-domain of machine learning. What sets a brick apart from a simple code snippet is its suitability for multiple execution environments. A brick module can also be executed in a demo playground, allowing users to try out different inputs to see if the brick meets their needs. \r\n\r\nIn this session, we will begin by outlining some ideas for building a brick. After substantiating our ideas, we will make the code usable in different environments, such as the playground for testing inputs. Since SpaCy is commonly used in many NLP projects, we will also build a variant of the code that takes a SpaCy document as input. Add some documentation, and voila! You now have a brick.", "recording_license": "", "do_not_record": false, "persons": [{"id": 28902, "code": "H7YRFB", "public_name": "Leonard P\u00fcttmann", "biography": "Leonard P\u00fcttmann studied economics at the Hochschule D\u00fcsseldorf. During a specialization course there he fell in love with all things ML, especially when it comes to natural language processing. After studying, he joined the company Kern AI as a data scientist and now works as a developer advocate, where he is connecting people to topics like ML and programming.", "answers": []}], "links": [], "attachments": [], "answers": []}], "A05-A06": [{"id": 26268, "guid": "259c3408-b9b7-56a0-a5bd-e8908fdcf2ba", "logo": "", "date": "2023-04-19T10:00:00+02:00", "start": "10:00", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26268-building-hexagonal-python-services", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/EAKYPL/", "title": "Building Hexagonal Python Services", "subtitle": "", "track": "PyCon: Programming & Software Engineering", "type": "Tutorial", "language": "en", "abstract": "The importance of enterprise architecture patterns is all well-known and applicable to varied types of tasks. Thinking about the architecture from the beginning of the journey is crucial to have a maintainable, therefore testable, and flexible code base. In We are going to explore the Ports and Adapters(Hexagonal) pattern by showing a simple web app using Repository, Unit of Work, and Services(Use Cases) patterns tied together with Dependency Injection. All those patterns are quite famous in other languages but they are relatively new for the Python ecosystem, which is a crucial missing part.\r\nAs a web framework, we are going to use FastAPI which can be replaced with any framework in a matter of time because of the abstractions we have added.", "description": "In nearly all web applications and Python tutorials we are starting from installing a web framework, and database server, the next step is to build database models and then use ORM, etc.\r\nBut wait, there is a problem with this classical approach, we lose the core business domain discussions - so-called core domain models just get lost inside some classes and functions. How about changing and reverting our approach? How about first starting by thinking, modeling our business, and core domain, and then testing it properly? Afterward, how about adding an abstraction layer on the database, then adding another abstraction on actual services, and use cases? But wait, how we are going to manage all transactional usage - okay let's add another layer with the Unit of Work pattern to manage our work as units. Sounds cryptic? Here is a step-by-step guide to starting our project:\r\n* We are going to start with domain modeling and adding tests for our domain models\r\n* The database layer will be abstracted using a Repository pattern\r\n* The database transactions will be managed by the Unit of Work pattern\r\n* The business logic actions were encapsulated in the Use Cases\r\n\r\nThe question can arise: where are our web framework and database server?\r\nAnswer: good architecture lets us defer those choices until the end. Because the web framework and the database server are details for our business/core application itself. Web framework will be considered as an entry point for our application and the database layer will be encapsulated using SQLAlchemy ORM, but still, ORM itself is hidden behind Repository and UoW patterns. This allows us to change the ORM library if there will be any need in the future.\r\n\r\nThe most important part here is to understand how we are going to build our application using Ports and Adapters(Hexagonal) pattern and all aforementioned patterns will be divided into Ports(using abstract base classes) and Adapters(the actual implementations), we can think about this as a contract between our actual implementations and abstractions.", "recording_license": "", "do_not_record": false, "persons": [{"id": 25815, "code": "C9GDRS", "public_name": "Shahriyar Rzayev", "biography": "Senior Software Engineer. Moving forward on Clean Code and Clean Architecture. Previous accomplishments include contributing to open source, providing technical direction, and sharing knowledge about Clean Code and Architectural patterns. An empathetic team player and mentor.\r\nAzerbaijan Python Group Leader. Former QA Engineer and Bug Hunter.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26277, "guid": "2d9ee8fb-0638-5ec9-9350-1f5ed61d7f44", "logo": "", "date": "2023-04-19T11:40:00+02:00", "start": "11:40", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26277-workshop-on-privilege-and-ethics-in-data", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/ZYB38R/", "title": "Workshop on Privilege and Ethics in Data", "subtitle": "", "track": "General: Ethics & Privacy", "type": "Tutorial", "language": "en", "abstract": "Data-driven products are becoming more and more ubiquitous. Humans build data-driven products. Humans are intrinsically biased. This bias goes into the data-driven products, confirming and amplifying the original bias. In this tutorial, you will learn how to identify your own -often unperceived- biases and reflect on and discuss the consequences of unchecked biases in Data Products.", "description": "Data-driven products are becoming more and more ubiquitous across industries. Data-driven products are built by humans. Humans are intrinsically biased. This bias goes into the data-driven products, which then amplify the original bias. This has the consequence that the power imbalances in a data-driven world tend to get bigger instead of smaller, most of the time unintentionally, and is particularly prevalent in the tech sector where teams are not diverse.\r\n\r\nOne of the obvious solutions is to get diverse teams, but when considering all the intersections of diversity, achieving full diversity is practically an impossible task. Therefore we see education and awareness as foundational steps to working towards a more equitable data world.\r\n\r\nThis tutorial has two parts. In the first exercise, we will start by revisiting our own privileges, as a tool to better educate ourselves in order to identify our individual - often unperceived - biases. \r\nIn the second part, we will evaluate what happens when these biases happen on a group level and go unchecked into our data products, based on the Data Feminism book and enriched with our own experiences as data professionals.   \r\n\r\nEducation about Privilege and Ethics in the data-driven world can only improve how we see and work with data and better understand how our work with data can affect others.", "recording_license": "", "do_not_record": true, "persons": [{"id": 1899, "code": "NMACLQ", "public_name": "Tereza Iofciu", "biography": null, "answers": []}, {"id": 15984, "code": "WVNMPG", "public_name": "Paula Gonzalez Avalos", "biography": "Paula's love for working with Data brought her to the field of Data Science from her initial Natural Sciences background. Loving Teaching brought her to work as a Data Science Coach.Enjoying public speaking brought her to the PyData community - where she enjoys sharing and learning, and most recently organizing. Now she's working as Head of Data Science at Spiced Academy, although technically she's still on parental leave.", "answers": []}], "links": [], "attachments": [], "answers": []}, {"id": 26494, "guid": "cdc5e6bb-639c-5993-9874-7feeff902cf5", "logo": "", "date": "2023-04-19T14:10:00+02:00", "start": "14:10", "duration": "01:30", "room": "A05-A06", "slug": "pyconde-pydata-berlin-2023-26494-the-battle-of-giants-causality-vs-nlp-from-theory-to-practice", "url": "https://pretalx.com/pyconde-pydata-berlin-2023/talk/GLQH8X/", "title": "The Battle of Giants: Causality vs NLP => From Theory to Practice", "subtitle": "", "track": "PyData: Natural Language Processing", "type": "Tutorial", "language": "en", "abstract": "With an average of 3.2 new papers published on Arxiv every day in 2022, causal inference has exploded in popularity, attracting large amount of talent and interest from top researchers and institutions including industry giants like Amazon or Microsoft. Text data, with its high complexity, posits an exciting challenge for causal inference community. In the workshop, we'll review the latest advances in the field of Causal NLP and implement a causal Transformer model to demonstrate how to translate these developments into a practical solution that can bring real business value. All in Python!", "description": "Join us for a workshop exploring the exciting field of causal inference and its applications in natural language processing (NLP).\r\n\r\nThe workshop is addressed to people who want to enrich their NLP and/or Causal Inference toolkits and enhance their perspective on contemporary machine learning. \r\n\r\nThe workshop will start with an overview of modern causality frameworks. We\u2019ll discuss the most prominent ideas in Causal NLP and present an overview of Causal NLP tasks. Finally, we\u2019ll implement CausalBERT model and demonstrate how it can be used to estimate causal effects in practical contexts.\r\n\r\nThe workshop is open to everyone, yet to fully enjoy the content, it\u2019s recommend that you:\r\n\r\n\u2022\tHave a solid understanding of Python fundamentals (lists, dicts, scientific stack) \r\n\r\n\u2022\tUnderstand the basics of graph theory (nodes, directed and undirected edges)\r\n\r\n\u2022\tHave a good understanding of deep learning basics\r\n\r\n\u2022\tHave a good understanding of NLP concepts like tokens and embeddings\r\n\r\nThe goal of this talk is to give you practical understanding of how to implement Causal NLP methods and inspire you to explore the fast growing world of causality.", "recording_license": "", "do_not_record": false, "persons": [{"id": 16315, "code": "DGHVRT", "public_name": "Aleksander Molak", "biography": "For the last 6 years Alex worked as a machine learning consultant, engineer and researcher in the industry and academia. He helped designing and building machine learning systems for Fortune 100, Fortune 500 and Inc 5000 companies.\r\n\r\nHe\u2019s an international speaker, blogger and author of a book on causality in Python. Interested in causality, NLP, probabilistic modeling, representation learning and graph neural networks.\r\n\r\nLoves traveling with his wife, passionate about vegan food, languages and running.\r\n\r\nWebsite: https://alxndr.io\r\nLinkedIn: https://www.linkedin.com/in/aleksandermolak/", "answers": []}], "links": [], "attachments": [], "answers": []}]}}]}}}